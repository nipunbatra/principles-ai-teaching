---
marp: true
theme: default
paginate: true
backgroundColor: #fff
style: |
  section {
    font-family: 'Segoe UI', 'Arial', sans-serif;
    font-size: 22px;
    padding: 35px;
    color: #333;
  }
  h1 { color: #2E86AB; font-size: 1.7em; margin-bottom: 0.2em; }
  h2 { color: #06A77D; font-size: 1.1em; margin-top: 0; }
  h3 { color: #457B9D; font-size: 1.0em; }
  strong { color: #D62828; }
  code {
    background: #f4f4f4;
    color: #2E86AB;
    padding: 2px 6px;
    border-radius: 4px;
    font-family: 'Consolas', 'Monaco', monospace;
  }
  pre {
    background: #f8f9fa;
    border-radius: 8px;
    padding: 12px;
    font-size: 0.75em;
    line-height: 1.3;
    overflow: hidden;
  }
  .example {
    background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
    border-left: 4px solid #06A77D;
    padding: 10px 12px;
    margin: 8px 0;
    border-radius: 0 6px 6px 0;
  }
  .insight {
    background: #fff3cd;
    border-left: 4px solid #ffc107;
    padding: 10px 12px;
    margin: 8px 0;
    border-radius: 0 6px 6px 0;
  }
  .warning {
    background: #ffebee;
    border-left: 4px solid #D62828;
    padding: 10px 12px;
    margin: 8px 0;
    border-radius: 0 6px 6px 0;
  }
  .realworld {
    background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
    border-left: 4px solid #2196F3;
    padding: 10px 12px;
    margin: 8px 0;
    border-radius: 0 6px 6px 0;
  }
  .columns { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }
  table { font-size: 0.85em; width: 100%; }
  th { background: #2E86AB; color: white; padding: 6px; }
  td { padding: 6px; border-bottom: 1px solid #dee2e6; }
---

# Object Detection Basics
## Deep Learning for Computer Vision

**Nipun Batra** Â· IIT Gandhinagar
*Teaching computers to see and locate objects*

---

# Why Object Detection?

```
Imagine you're driving...

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚   [Car] <-- Your car sees this scene                               â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚         â”‚  [Ped] pedestrian   [Car] car    [Bike] cyclist      â”‚    â”‚
â”‚         â”‚      â†“                â†“              â†“               â”‚    â”‚
â”‚         â”‚   STOP!           SLOW DOWN      CAREFUL!           â”‚    â”‚
â”‚         â”‚                                                      â”‚    â”‚
â”‚         â”‚  Your car needs to know:                             â”‚    â”‚
â”‚         â”‚  â€¢ WHAT is there? (classification)                   â”‚    â”‚
â”‚         â”‚  â€¢ WHERE is it? (localization)                       â”‚    â”‚
â”‚         â”‚  â€¢ HOW MANY? (counting)                              â”‚    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Classification alone says "there's a person somewhere" â€” **not enough!**

---

# What You Will Learn Today

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚   Part 1: The Core Problem                                          â”‚
â”‚           Classification â†’ Detection â†’ Segmentation                 â”‚
â”‚                                                                     â”‚
â”‚   Part 2: Bounding Boxes                                            â”‚
â”‚           How we represent "WHERE is the object?"                   â”‚
â”‚                                                                     â”‚
â”‚   Part 3: IoU (Intersection over Union)                             â”‚
â”‚           How we measure "Is this box correct?"                     â”‚
â”‚                                                                     â”‚
â”‚   Part 4: NMS (Non-Maximum Suppression)                             â”‚
â”‚           How we handle "Too many boxes!"                           â”‚
â”‚                                                                     â”‚
â”‚   Part 5: YOLO Architecture                                         â”‚
â”‚           How real detectors work                                   â”‚
â”‚                                                                     â”‚
â”‚   Part 6: Training & Metrics                                        â”‚
â”‚           How we train and evaluate detectors                       â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Part 1: The Core Problem
## What IS Object Detection?

---

# The Vision Task Hierarchy

```
Level 1: CLASSIFICATION                 "What is in this image?"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                         â”‚
         â”‚   [Dog] [Dog] [Cat]    â”‚  ->  Output: "dog" (or "cat")
         â”‚                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         Just ONE label for the whole image

Level 2: OBJECT DETECTION              "What + Where?"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”  â”Œâ”€â”€â”€â”  â”‚  â†’  Output: 3 boxes with labels
         â”‚  â”‚Dogâ”‚   â”‚Dogâ”‚  â”‚Catâ”‚  â”‚      dog (0.95) @ [10,20,50,80]
         â”‚  â””â”€â”€â”€â”˜   â””â”€â”€â”€â”˜  â””â”€â”€â”€â”˜  â”‚      dog (0.92) @ [100,20,140,85]
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      cat (0.88) @ [200,30,250,90]

Level 3: INSTANCE SEGMENTATION         "Exact shape of each object"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  â–‘â–‘â–‘â–‘     â–ˆâ–ˆâ–ˆâ–ˆ   â–“â–“â–“â–“  â”‚  â†’  Pixel-perfect mask for each
         â”‚  â–‘â–‘â–‘â–‘â–‘    â–ˆâ–ˆâ–ˆâ–ˆ   â–“â–“â–“   â”‚      individual object
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Classification vs Detection: Key Difference

![w:1100 center](diagrams/realistic/classification_vs_detection.png)

**Classification:** ONE label for the whole image
**Detection:** MULTIPLE objects with locations (bounding boxes)

<div class="insight">
Detection = Classification + Localization + Counting (implicitly)
</div>

---

# A More Detailed Comparison

| Aspect | Classification | Detection | Segmentation |
|--------|---------------|-----------|--------------|
| **Output** | Single label | Boxes + labels | Pixel masks |
| **Multiple objects?** | No (or multi-label) | Yes âœ“ | Yes âœ“ |
| **Location?** | No | Box âœ“ | Exact shape âœ“ |
| **Typical use** | "What is this?" | "Where are things?" | "Precise boundaries" |
| **Difficulty** | Easier | Medium | Harder |
| **Speed** | Fastest | Fast | Slower |

<div class="realworld">
<strong>Real applications:</strong>
â€¢ Classification: "Is this X-ray normal?"
â€¢ Detection: "Where are the tumors?"
â€¢ Segmentation: "Exact tumor boundary for surgery planning"
</div>

---

# Real-World Detection Applications

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OBJECT DETECTION IN THE WILD                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  AUTONOMOUS DRIVING                 SMARTPHONE CAMERAS              â”‚
â”‚  â€¢ Pedestrians, cars, signs         â€¢ Face detection for focus     â”‚
â”‚  â€¢ Traffic lights, lanes            â€¢ Scene understanding          â”‚
â”‚  â€¢ Cyclist detection                â€¢ AR filters positioning       â”‚
â”‚                                                                     â”‚
â”‚  RETAIL & INVENTORY                 MEDICAL IMAGING                 â”‚
â”‚  â€¢ Shelf stock monitoring           â€¢ Tumor detection              â”‚
â”‚  â€¢ Checkout-free stores (Amazon Go) â€¢ Cell counting                â”‚
â”‚  â€¢ Customer flow analysis           â€¢ Organ localization           â”‚
â”‚                                                                     â”‚
â”‚  SECURITY & SURVEILLANCE            MANUFACTURING                   â”‚
â”‚  â€¢ Intrusion detection              â€¢ Defect detection             â”‚
â”‚  â€¢ Crowd monitoring                 â€¢ Quality control              â”‚
â”‚  â€¢ License plate recognition        â€¢ Safety compliance            â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Instance Segmentation: Beyond Boxes

```
DETECTION gives you BOXES:              SEGMENTATION gives you MASKS:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        â”‚              â”‚                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚              â”‚      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         â”‚
â”‚  â”‚             â”‚       â”‚              â”‚    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     â”‚
â”‚  â”‚   Person    â”‚       â”‚              â”‚   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    â”‚
â”‚  â”‚   standing  â”‚       â”‚              â”‚     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        â”‚
â”‚  â”‚             â”‚       â”‚              â”‚      â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ       â”‚
â”‚  â”‚             â”‚       â”‚              â”‚      â–ˆâ–ˆâ–ˆâ–ˆ  â–ˆâ–ˆâ–ˆâ–ˆ       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚              â”‚                       â”‚
â”‚                        â”‚              â”‚                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Box includes background!                Every pixel classified!
```

<div class="insight">
Segmentation is needed for: video editing backgrounds, medical boundaries, precise robotics
</div>

---

# Part 2: Bounding Boxes
## How We Represent "WHERE"

---

# What IS a Bounding Box?

```
A bounding box is a RECTANGLE that tightly contains an object.

                    Image Coordinate System
          (0,0) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º x (width)
            â”‚
            â”‚         (xâ‚, yâ‚) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚            â”‚                  â”‚
            â”‚            â”‚    [Dog]         â”‚  <- The dog lives
            â”‚            â”‚    doggo         â”‚     inside this box
            â”‚            â”‚                  â”‚
            â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ (xâ‚‚, yâ‚‚)
            â”‚
            â–¼
          y (height)

The box is defined by 4 numbers. But WHICH 4?
```

<div class="warning">
Different systems use different conventions â€” this causes many bugs!
</div>

---

# The Three Main Formats

```
Format 1: CORNER FORMAT (xâ‚, yâ‚, xâ‚‚, yâ‚‚)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Top-left and bottom-right corners
â€¢ Used by: PyTorch, many APIs
â€¢ Easy for: drawing, IoU calculation

          (100, 50) â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                 â”‚
                    â”‚                 â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ (300, 200)

          Box = [100, 50, 300, 200]
          Width = 300 - 100 = 200
          Height = 200 - 50 = 150
```

---

# The Three Main Formats (continued)

```
Format 2: CENTER FORMAT (cx, cy, w, h)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Center point + width + height
â€¢ Used by: YOLO, many papers
â€¢ Easy for: predicting from grid cells

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚        â”‚        â”‚
                    â”‚â”€â”€â”€â”€â”€ â— â”€â”€â”€â”€â”€â”‚   â”‚   â— = center (200, 125)
                    â”‚        â”‚        â”‚   w = 200, h = 150
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

          Box = [200, 125, 200, 150]
          xâ‚ = 200 - 200/2 = 100
          yâ‚ = 125 - 150/2 = 50


Format 3: CORNER + SIZE (x, y, w, h)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Top-left corner + width + height
â€¢ Used by: COCO dataset format
â€¢ Easy for: drawing (x,y is start point)

          (100, 50) â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                 â”‚  w=200
                    â”‚                 â”‚  h=150
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

          Box = [100, 50, 200, 150]
```

---

# Format Conversion Cheat Sheet

```python
# Corner (x1,y1,x2,y2) â†’ Center (cx,cy,w,h)
cx = (x1 + x2) / 2
cy = (y1 + y2) / 2
w = x2 - x1
h = y2 - y1

# Center (cx,cy,w,h) â†’ Corner (x1,y1,x2,y2)
x1 = cx - w/2
y1 = cy - h/2
x2 = cx + w/2
y2 = cy + h/2

# Corner+Size (x,y,w,h) â†’ Corner (x1,y1,x2,y2)
x1 = x
y1 = y
x2 = x + w
y2 = y + h
```

<div class="warning">
<strong>Common bug alert!</strong> Always check your dataset's format before training.
COCO uses [x,y,w,h], PASCAL VOC uses [x1,y1,x2,y2], YOLO uses normalized [cx,cy,w,h]
</div>

---

# Absolute vs Normalized Coordinates

```
ABSOLUTE COORDINATES (Pixels):

    Image size: 640 Ã— 480 pixels
    Box: [100, 50, 300, 200]  â† pixel values

    Problem: What if image is resized to 320 Ã— 240?
             Box values become invalid!

NORMALIZED COORDINATES (0 to 1):

    Image size: ANY
    Box: [0.156, 0.104, 0.469, 0.417]  â† proportions

    Conversion:
    x_norm = x_pixel / image_width
    y_norm = y_pixel / image_height

    Same box works for ANY image size!
```

<div class="insight">
YOLO uses normalized center format: [cx/W, cy/H, w/W, h/H] â€” all values between 0 and 1
</div>

---

# Quick Format Quiz

```
Image: 640 Ã— 480 pixels
Object is at: top-left (100, 50), bottom-right (300, 200)

What is the box in each format?

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Format              â”‚ Values                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Corner (xyxy)       â”‚ [100, 50, 300, 200]                       â”‚
â”‚ Corner+Size (xywh)  â”‚ [100, 50, 200, 150]                       â”‚
â”‚ Center (cxcywh)     â”‚ [200, 125, 200, 150]                      â”‚
â”‚ Normalized xyxy     â”‚ [0.156, 0.104, 0.469, 0.417]              â”‚
â”‚ Normalized cxcywh   â”‚ [0.312, 0.260, 0.312, 0.312]              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Always verify which format your framework expects!
```

---

# Part 3: Intersection over Union (IoU)
## How Good Is a Detection?

---

# The Core Question

```
Your model predicted a box. Ground truth is another box.
Is your prediction "correct"?

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Ground Truth      â”‚
    â”‚   (from human)      â”‚
    â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      â”‚              â”‚       â”‚
    â”‚      â”‚   OVERLAP    â”‚       â”‚
    â”‚      â”‚              â”‚       â”‚
    â””â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
           â”‚      Prediction      â”‚
           â”‚      (from model)    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Question: How do we measure if prediction is "close enough"?

Answer: IoU = Intersection / Union
```

---

# IoU: The Formula

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚                    Area of OVERLAP                                  â”‚
â”‚         IoU = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                             â”‚
â”‚                 Area of BOTH boxes                                  â”‚
â”‚                 (counted once)                                      â”‚
â”‚                                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                 â”‚
â”‚       â”‚ A         â”‚                                                 â”‚
â”‚       â”‚     â”Œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”       INTERSECTION = shaded area          â”‚
â”‚       â”‚     â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚       UNION = A + B - Intersection        â”‚
â”‚       â”‚     â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚     â”‚                                           â”‚
â”‚       â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”˜  B  â”‚              Intersection                 â”‚
â”‚             â”‚           â”‚       IoU = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  Union                    â”‚
â”‚                                                                     â”‚
â”‚                      Intersection                                   â”‚
â”‚         IoU = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚                A + B - Intersection                                 â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# IoU: Visual Example

![w:800 center](diagrams/realistic/iou_example.png)

<div class="insight">
IoU is between 0 and 1. Higher is better. Think of it as "percent overlap."
</div>

| IoU Value | Interpretation | Action |
|-----------|---------------|--------|
| 0.0 | No overlap | Completely wrong |
| 0.3 | Poor overlap | Likely wrong |
| 0.5+ | Good overlap | Typically counts as correct |
| 0.75+ | Great overlap | High-quality detection |
| 1.0 | Perfect overlap | Identical boxes |

---

# Computing IoU: Step by Step

```python
def compute_iou(box1, box2):
    """
    Boxes in format: [x1, y1, x2, y2]
    """
    # Step 1: Find intersection rectangle
    x1_inter = max(box1[0], box2[0])  # leftmost right edge
    y1_inter = max(box1[1], box2[1])  # topmost bottom edge
    x2_inter = min(box1[2], box2[2])  # rightmost left edge
    y2_inter = min(box1[3], box2[3])  # bottommost top edge

    # Step 2: Compute intersection area
    inter_width = max(0, x2_inter - x1_inter)
    inter_height = max(0, y2_inter - y1_inter)
    intersection = inter_width * inter_height

    # Step 3: Compute union area
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection

    # Step 4: IoU = intersection / union
    return intersection / union if union > 0 else 0
```

---

# IoU Thresholds in Practice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        IoU THRESHOLD GUIDE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   IoU Value   â”‚   What it means                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   1.0         â”‚   Perfect match (never happens in practice)           â”‚
â”‚   0.9+        â”‚   Excellent â€” boxes nearly identical                  â”‚
â”‚   0.75        â”‚   Very good â€” used in strict evaluation               â”‚
â”‚   0.50        â”‚   Standard threshold â€” "correct" in most benchmarks   â”‚
â”‚   0.25        â”‚   Loose â€” used in some older benchmarks               â”‚
â”‚   0.0         â”‚   No overlap at all â€” completely wrong                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

The RULE in most systems:
    IoU â‰¥ 0.5  â†’  TRUE POSITIVE (TP) âœ“  "You found the object!"
    IoU < 0.5  â†’  FALSE POSITIVE (FP) âœ—  "Wrong detection"
```

---

# Why 0.5 Is the Magic Number

```
Think about it visually:

IoU = 0.5 means: Intersection is 1/3 of each box's area

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚    1/3    â”‚    1/3    â”‚    1/3
    â”‚    A only â”‚  OVERLAP  â”‚    B only
    â”‚           â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Union = 1/3 + 1/3 + 1/3 = 1
    Intersection = 1/3
    IoU = 1/3 Ã· 1 = 0.33? No wait...

Actually for IoU = 0.5:
    If Intersection = I, Union = 2I
    Because: A + B - I = 2I means A + B = 3I
    So each box is ~2I, overlapping by I

    Roughly: boxes overlap by about half their area
```

<div class="insight">
IoU = 0.5 is when the overlap is "significant enough" to say "yes, you found it"
</div>

---

# Different Benchmarks, Different Thresholds

```
PASCAL VOC (older, easier):
    â€¢ Single threshold: IoU â‰¥ 0.5
    â€¢ One number: mAP@0.5
    â€¢ 20 object classes

MS COCO (modern, stricter):
    â€¢ Multiple thresholds: 0.50, 0.55, 0.60, ..., 0.95
    â€¢ Reports: mAP@0.5, mAP@0.75, mAP@[.5:.95]
    â€¢ 80 object classes
    â€¢ Also evaluates different object sizes (small/medium/large)

Why COCO is harder:
    â€¢ Must be accurate at multiple thresholds
    â€¢ Average over 10 thresholds penalizes sloppy boxes
    â€¢ Same model might get 60% on VOC but only 40% on COCO
```

---

# Part 4: Non-Maximum Suppression (NMS)
## Cleaning Up Duplicate Detections

---

# The Problem: Too Many Boxes!

```
What the detector sees:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚
â”‚         â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                                             â”‚
â”‚         â”‚ â”‚ â”Œâ”€â”€â”€â”€â”€â” â”‚ â”‚                                             â”‚
â”‚         â”‚ â”‚ â”‚[Dog]â”‚ â”‚ â”‚  <- Same dog, but 5 different boxes!       â”‚
â”‚         â”‚ â”‚ â””â”€â”€â”€â”€â”€â”˜ â”‚ â”‚    All have high confidence!               â”‚
â”‚         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                                             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Boxes with confidences:
    Box 1: dog (0.95)
    Box 2: dog (0.93)
    Box 3: dog (0.91)
    Box 4: dog (0.87)
    Box 5: dog (0.85)

Problem: We want ONE box per object, not five!
```

---

# Why Does This Happen?

```
Detectors check MANY locations:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚   YOLO divides image into 7Ã—7 = 49 cells                           â”‚
â”‚   Each cell predicts 2 boxes                                        â”‚
â”‚   = 98 boxes per class                                              â”‚
â”‚                                                                     â”‚
â”‚   Modern YOLO has 3 scales Ã— (13Â² + 26Â² + 52Â²) Ã— 3 anchors         â”‚
â”‚   = ~10,000+ candidate boxes!                                       â”‚
â”‚                                                                     â”‚
â”‚   Many of these will fire on the same object:                       â”‚
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                                                â”‚
â”‚   â”‚   â”‚ X â”‚ X â”‚   â”‚   X = cells that detect the dog                â”‚
â”‚   â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤   Each X outputs a box                         â”‚
â”‚   â”‚   â”‚ X â”‚ X â”‚   â”‚   All boxes are similar, all correct           â”‚
â”‚   â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜   But we only want ONE                         â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# NMS: The Intuition

```
Non-Maximum Suppression = "Keep the best, remove the rest"

Like choosing the best photo from burst mode:
    ğŸ“¸ ğŸ“¸ ğŸ“¸ ğŸ“¸ ğŸ“¸ â†’ ğŸ“¸ (keep best one)

Algorithm intuition:
    1. Sort all boxes by confidence (best first)
    2. Pick the best box â†’ definitely keep it
    3. Remove all boxes that overlap too much with it
       (they're probably detecting the same object)
    4. Repeat with remaining boxes

                    â”Œâ”€â”€â”€â”€â”€â”
                    â”‚0.95 â”‚ â† Keep this (highest confidence)
                    â””â”€â”€â”€â”€â”€â”˜
                       â†“
    Remove these if IoU > threshold:
    [0.93, 0.91, 0.87, 0.85] â† All overlapping, all removed
```

---

# NMS: Before and After

![w:900 center](diagrams/realistic/nms_example.png)

**The Algorithm:**
1. **Sort** all boxes by confidence (descending)
2. **Take** the best box, add to "keep" list
3. **Remove** all boxes with IoU > threshold (default 0.5) from queue
4. **Repeat** until queue is empty

Result: Multiple overlapping boxes -> One clean box per object!

---

# NMS: The Code

```python
def nms(boxes, scores, iou_threshold=0.5):
    """
    boxes: List of [x1, y1, x2, y2]
    scores: Confidence for each box
    Returns: Indices of boxes to keep
    """
    # Sort by score (descending)
    order = scores.argsort()[::-1]

    keep = []

    while len(order) > 0:
        # Take the best remaining box
        best_idx = order[0]
        keep.append(best_idx)

        # Compute IoU with all remaining boxes
        remaining = order[1:]
        ious = compute_iou_batch(boxes[best_idx], boxes[remaining])

        # Keep only boxes with low overlap (different objects)
        mask = ious <= iou_threshold
        order = remaining[mask]

    return keep
```

---

# NMS Parameters

```
IoU THRESHOLD controls how aggressive NMS is:

    threshold = 0.3 (aggressive)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Removes boxes even with small overlap
    â€¢ Might accidentally merge two close objects into one
    â€¢ Use when objects are far apart

    threshold = 0.5 (standard)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Good balance for most cases
    â€¢ Default in most frameworks

    threshold = 0.7 (lenient)
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Keeps more boxes
    â€¢ Better when objects overlap (crowded scenes)
    â€¢ Might keep some duplicates

Rule of thumb: Start with 0.5, adjust based on your data
```

---

# NMS Variants

```
STANDARD NMS:
    â€¢ Apply separately per class
    â€¢ Simple, fast, widely used

SOFT-NMS:
    â€¢ Instead of removing, reduce confidence of overlapping boxes
    â€¢ Better for crowded scenes
    â€¢ score = score Ã— (1 - IoU) or score Ã— exp(-IoUÂ²/Ïƒ)

CLASS-AGNOSTIC NMS:
    â€¢ Apply across all classes
    â€¢ Prevents same location detecting "dog" AND "cat"

BATCHED NMS:
    â€¢ Optimized for GPU
    â€¢ Process all classes in parallel

DIoU-NMS:
    â€¢ Uses Distance-IoU instead of IoU
    â€¢ Considers center distance, not just overlap
    â€¢ Better for overlapping objects
```

---

# Part 5: How Detectors Work
## The YOLO Architecture

---

# Two Families of Detectors

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚  TWO-STAGE DETECTORS              ONE-STAGE DETECTORS               â”‚
â”‚  (Accurate but slower)            (Fast, good enough)               â”‚
â”‚                                                                     â”‚
â”‚  Stage 1: "Where might              Single pass:                    â”‚
â”‚            objects be?"             "Here are all objects           â”‚
â”‚  Stage 2: "What are they?"           with their locations"          â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚Imageâ”‚ â†’  â”‚ RPN â”‚ â†’  â”‚Classâ”‚     â”‚Imageâ”‚ â†’  â”‚ All â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜              â”‚
â”‚              â†“                                  â†“                   â”‚
â”‚          ~2000 boxes                      Direct output             â”‚
â”‚              â†“                                  â†“                   â”‚
â”‚          Classify each                    Boxes + Classes           â”‚
â”‚                                                                     â”‚
â”‚  Examples:                          Examples:                       â”‚
â”‚  â€¢ R-CNN, Fast R-CNN               â€¢ YOLO (v1-v8)                   â”‚
â”‚  â€¢ Faster R-CNN                    â€¢ SSD                            â”‚
â”‚  â€¢ Mask R-CNN                      â€¢ RetinaNet                      â”‚
â”‚                                                                     â”‚
â”‚  ~5-10 FPS                         ~30-100+ FPS                     â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# YOLO: You Only Look Once

```
The revolutionary idea (2015):

BEFORE YOLO:                           YOLO:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Run classifier 1000s of times          Run CNN ONCE
at different locations/scales          Output everything directly

â”Œâ”€â”€â”€â”€â”€â”  â†’ Is there object here?       â”Œâ”€â”€â”€â”€â”€â”
â”‚     â”‚  â†’ Is there object here?       â”‚     â”‚  â†’  All boxes +
â”‚ IMG â”‚  â†’ Is there object here?       â”‚ IMG â”‚     All classes +
â”‚     â”‚  â†’ Is there object here?       â”‚     â”‚     All confidences
â””â”€â”€â”€â”€â”€â”˜  â†’ ... (repeat 1000x)          â””â”€â”€â”€â”€â”€â”˜     (ONE forward pass)

Very slow (seconds per image)          Very fast (milliseconds)
```

<div class="insight">
YOLO = "You Only Look Once" â€” the entire detection in a single neural network pass
</div>

---

# YOLO Core Idea: Grid Division

```
YOLO divides the image into an SÃ—S grid (e.g., 7Ã—7):

â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚  â—â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â•‘â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â•‘â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚  â•‘  â”‚[Dog]â”‚     â•‘     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â•‘â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â•‘â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚  â•‘  â”‚     â”‚     â•‘     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚     â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
                    â†‘
        The cell containing the CENTER of the dog
        is "responsible" for detecting that dog
```

---

# What Each Cell Predicts

```
Each grid cell outputs:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚  For EACH BOUNDING BOX (B boxes per cell):                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”‚
â”‚    â€¢ x, y     = center offset (relative to cell)                   â”‚
â”‚    â€¢ w, h     = width, height (relative to image)                  â”‚
â”‚    â€¢ conf     = P(object) Ã— IoU with ground truth                  â”‚
â”‚                                                                     â”‚
â”‚  For the CELL (shared across boxes):                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚    â€¢ P(classâ‚), P(classâ‚‚), ..., P(classC)                          â”‚
â”‚                                                                     â”‚
â”‚  OUTPUT SHAPE:                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚    S Ã— S Ã— (B Ã— 5 + C)                                             â”‚
â”‚                                                                     â”‚
â”‚    Example: 7 Ã— 7 Ã— (2 Ã— 5 + 20) = 7 Ã— 7 Ã— 30                      â”‚
â”‚             = 1470 numbers describing all detections                â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# YOLO Prediction Visualization

```
One cell's prediction (2 boxes, 20 classes):

        Box 1                  Box 2                 Classes
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ x: 0.3        â”‚     â”‚ x: 0.6        â”‚     â”‚ P(person): 0.01  â”‚
    â”‚ y: 0.4        â”‚     â”‚ y: 0.5        â”‚     â”‚ P(car): 0.02     â”‚
    â”‚ w: 0.5        â”‚     â”‚ w: 0.3        â”‚     â”‚ P(dog): 0.92     â”‚
    â”‚ h: 0.7        â”‚     â”‚ h: 0.4        â”‚     â”‚ P(cat): 0.03     â”‚
    â”‚ conf: 0.85    â”‚     â”‚ conf: 0.12    â”‚     â”‚ ...              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“                     â†“
      High conf!            Low conf,                 â†“
      This is the           ignore             Most likely: DOG
      main detection

Final: Box 1 predicts "dog" with confidence 0.85 Ã— 0.92 = 0.78
```

---

# The YOLO Network Architecture (v1)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       YOLO v1 Architecture                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                    â”‚
â”‚  INPUT: 448 Ã— 448 Ã— 3 (RGB image)                                 â”‚
â”‚     â”‚                                                              â”‚
â”‚     â–¼                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  24 CONVOLUTIONAL LAYERS                   â”‚                   â”‚
â”‚  â”‚  (Feature extraction - like image filters) â”‚                   â”‚
â”‚  â”‚                                            â”‚                   â”‚
â”‚  â”‚  Conv â†’ Pool â†’ Conv â†’ Pool â†’ ... â†’ Conv   â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚     â”‚                                                              â”‚
â”‚     â–¼                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  2 FULLY CONNECTED LAYERS                  â”‚                   â”‚
â”‚  â”‚  (Turn features into predictions)          â”‚                   â”‚
â”‚  â”‚                                            â”‚                   â”‚
â”‚  â”‚  4096 â†’ 7 Ã— 7 Ã— 30                        â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚     â”‚                                                              â”‚
â”‚     â–¼                                                              â”‚
â”‚  OUTPUT: 7 Ã— 7 Ã— 30 tensor (all predictions)                      â”‚
â”‚                                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Anchor Boxes: Handling Multiple Objects Per Cell

```
PROBLEM: What if TWO objects have centers in the SAME cell?

    â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
    â”‚     â”‚     â”‚     â”‚
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
    â”‚     â”‚D C â”‚     â”‚  <- Both dog AND cat center here!
    â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤       But cell can only predict
    â”‚     â”‚     â”‚     â”‚       one class in YOLO v1
    â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

SOLUTION: Anchor boxes (YOLO v2+)

    Pre-define box SHAPES (anchors):

    â”Œâ”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”
    â”‚   â”‚   â”‚      â”‚   â”‚ â”‚
    â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
    â””â”€â”€â”€â”˜               â”‚ â”‚
    Tall    Wide        â”‚ â”‚
    anchor  anchor      â””â”€â”˜
                        Very tall

    Each anchor predicts separately!
    Now cell can detect person (tall) AND car (wide)
```

---

# Anchor Boxes: How They Work

```
Each cell has K anchor boxes (e.g., K=5 in YOLOv2, K=9 in YOLOv3)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚  ANCHOR SHAPES (learned from data using K-means clustering):       â”‚
â”‚                                                                     â”‚
â”‚   Anchor 1    Anchor 2    Anchor 3    Anchor 4    Anchor 5        â”‚
â”‚    (small)    (tall)      (wide)      (medium)    (large)         â”‚
â”‚                                                                     â”‚
â”‚    â”Œâ”€â”€â”       â”Œâ”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚    â”‚  â”‚       â”‚  â”‚        â”‚        â”‚  â”‚    â”‚      â”‚        â”‚      â”‚
â”‚    â””â”€â”€â”˜       â”‚  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚      â”‚        â”‚      â”‚
â”‚               â”‚  â”‚                    â””â”€â”€â”€â”€â”˜      â”‚        â”‚      â”‚
â”‚               â””â”€â”€â”˜                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                     â”‚
â”‚  PREDICTION: Instead of predicting raw (x,y,w,h),                  â”‚
â”‚              predict OFFSETS from anchor:                           â”‚
â”‚                                                                     â”‚
â”‚              predicted_w = anchor_w Ã— exp(t_w)                      â”‚
â”‚              predicted_h = anchor_h Ã— exp(t_h)                      â”‚
â”‚                                                                     â”‚
â”‚  This makes learning easier! Network just learns adjustments.       â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# YOLO Evolution: v1 â†’ v8

```
YOLOv1 (2015): Original breakthrough
    â€¢ 7Ã—7 grid, 2 boxes/cell, 45 FPS
    â€¢ Simple but limited

YOLOv2 (2016): Better and faster
    â€¢ Anchor boxes, batch normalization
    â€¢ Multi-scale training

YOLOv3 (2018): Big improvement
    â€¢ 3 scales (13Ã—13, 26Ã—26, 52Ã—52)
    â€¢ Better small object detection
    â€¢ Darknet-53 backbone

YOLOv4 (2020): Bag of tricks
    â€¢ Many augmentations (Mosaic, CutMix)
    â€¢ CSPDarknet backbone

YOLOv5-v8 (2020-2023): Ultralytics versions
    â€¢ PyTorch native
    â€¢ Easy to use API
    â€¢ State-of-the-art speed/accuracy
```

---

# Modern YOLO (v5/v8): Key Features

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     YOLO v5/v8 INNOVATIONS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. MULTI-SCALE DETECTION                                           â”‚
â”‚     â€¢ Small objects: 52Ã—52 grid (detailed)                          â”‚
â”‚     â€¢ Medium objects: 26Ã—26 grid                                    â”‚
â”‚     â€¢ Large objects: 13Ã—13 grid (coarse)                            â”‚
â”‚                                                                     â”‚
â”‚  2. FEATURE PYRAMID NETWORK (FPN)                                   â”‚
â”‚     â€¢ Combines low-level (edges) and high-level (semantics)         â”‚
â”‚     â€¢ Better detection at all sizes                                  â”‚
â”‚                                                                     â”‚
â”‚  3. PATH AGGREGATION NETWORK (PAN)                                  â”‚
â”‚     â€¢ Bottom-up path for better localization                        â”‚
â”‚                                                                     â”‚
â”‚  4. MOSAIC AUGMENTATION                                             â”‚
â”‚     â€¢ Combine 4 images â†’ more objects per batch                     â”‚
â”‚     â€¢ Better generalization                                          â”‚
â”‚                                                                     â”‚
â”‚  5. AUTO-ANCHOR                                                     â”‚
â”‚     â€¢ Automatically compute best anchors for your data              â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# YOLO Model Sizes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    YOLOv8 Model Variants                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model   â”‚  Parameters â”‚   Size   â”‚  mAP (COCO) â”‚   Speed (GPU)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  v8n     â”‚    3.2M     â”‚   6 MB   â”‚    37.3     â”‚   ~1.0 ms         â”‚
â”‚  v8s     â”‚   11.2M     â”‚  22 MB   â”‚    44.9     â”‚   ~1.5 ms         â”‚
â”‚  v8m     â”‚   25.9M     â”‚  52 MB   â”‚    50.2     â”‚   ~2.5 ms         â”‚
â”‚  v8l     â”‚   43.7M     â”‚  87 MB   â”‚    52.9     â”‚   ~4.0 ms         â”‚
â”‚  v8x     â”‚   68.2M     â”‚  136 MB  â”‚    53.9     â”‚   ~6.0 ms         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

n = nano (edge devices, phones)
s = small (good balance)
m = medium
l = large
x = extra large (best accuracy)
```

<div class="insight">
Start with YOLOv8n for prototyping, move to larger models if needed.
</div>

---

# Part 6: Training & Evaluation
## How We Train and Measure Detectors

---

# The Detection Loss Function

```
Detection models optimize MULTIPLE objectives simultaneously:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚  TOTAL LOSS = Î»â‚Â·L_box + Î»â‚‚Â·L_obj + Î»â‚ƒÂ·L_class                     â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ L_box (Localization Loss)                                   â”‚   â”‚
â”‚  â”‚ "Is the box in the right place?"                            â”‚   â”‚
â”‚  â”‚ â€¢ Mean squared error on (x, y, w, h)                        â”‚   â”‚
â”‚  â”‚ â€¢ Or IoU-based loss (GIoU, DIoU, CIoU)                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ L_obj (Objectness Loss)                                     â”‚   â”‚
â”‚  â”‚ "Is there an object here?"                                  â”‚   â”‚
â”‚  â”‚ â€¢ Binary cross-entropy                                       â”‚   â”‚
â”‚  â”‚ â€¢ Confidence should match IoU with ground truth             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ L_class (Classification Loss)                               â”‚   â”‚
â”‚  â”‚ "What class is it?"                                         â”‚   â”‚
â”‚  â”‚ â€¢ Cross-entropy for each class                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Understanding the Box Loss

```
ORIGINAL YOLO (MSE Loss):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
L_box = (x - xÌ‚)Â² + (y - Å·)Â² + (âˆšw - âˆšÅµ)Â² + (âˆšh - âˆšÄ¥)Â²

Why square root of w,h?
    â€¢ Large boxes shouldn't dominate
    â€¢ Error of 10px matters more for 20px box than 200px box
    â€¢ âˆš compresses the range

MODERN YOLO (IoU-based Loss):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
L_box = 1 - IoU(predicted, ground_truth)

Or better variants:
    â€¢ GIoU: Adds penalty for gap between boxes
    â€¢ DIoU: Adds penalty for center distance
    â€¢ CIoU: DIoU + aspect ratio penalty

These directly optimize what we care about: overlap!
```

---

# Precision and Recall: Detection Version

```
For DETECTION, we need to define TP/FP/FN differently:

TRUE POSITIVE (TP):
    â€¢ Detection exists
    â€¢ Matches a ground truth box with IoU â‰¥ threshold
    â€¢ Correct class

FALSE POSITIVE (FP):
    â€¢ Detection exists BUT:
      - Doesn't match any ground truth (IoU too low)
      - OR correct box but wrong class
      - OR duplicate detection (already matched)

FALSE NEGATIVE (FN):
    â€¢ Ground truth box exists
    â€¢ No detection matched it

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚                    TP                                 TP            â”‚
â”‚   Precision = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     Recall = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚                TP + FP                     TP + FN                  â”‚
â”‚                                                                     â”‚
â”‚   "Of my detections,             "Of the actual objects,           â”‚
â”‚    how many are correct?"         how many did I find?"            â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Precision vs Recall Trade-off

```
As we lower the CONFIDENCE THRESHOLD:

High threshold (0.9):              Low threshold (0.3):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Only very confident detections   â€¢ Many detections (even uncertain)
â€¢ Might miss some objects          â€¢ Might include wrong detections
â€¢ HIGH PRECISION, LOW RECALL       â€¢ LOW PRECISION, HIGH RECALL

Example on 10 real dogs:

Threshold=0.9:                     Threshold=0.3:
â€¢ Found: 4 dogs (all correct)      â€¢ Found: 12 "dogs" (9 correct, 3 wrong)
â€¢ Precision = 4/4 = 100%           â€¢ Precision = 9/12 = 75%
â€¢ Recall = 4/10 = 40%              â€¢ Recall = 9/10 = 90%

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    P                                                 â”‚
â”‚    r  â”‚*                                             â”‚
â”‚    e  â”‚ *                                            â”‚
â”‚    c  â”‚  *                                           â”‚
â”‚    i  â”‚   *   â† Precision-Recall Curve               â”‚
â”‚    s  â”‚    **                                        â”‚
â”‚    i  â”‚      ***                                     â”‚
â”‚    o  â”‚         ****                                 â”‚
â”‚    n  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€*******â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Recall    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Average Precision (AP)

```
AP = Area under the Precision-Recall curve

    Precision
    1.0 â”¤â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    0.5 â”¤â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
        â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
    0.0 â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        0.0                           1.0
                    Recall

AP = Area of this curve (shaded region)

Higher AP = better detector (good precision at all recall levels)

In practice, we compute AP by:
1. Sort detections by confidence
2. At each detection, compute precision and recall so far
3. Interpolate and compute area under curve
```

---

# Mean Average Precision (mAP)

```
mAP = Mean of AP across all classes

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚   For each class:                                                   â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚   AP_person = 0.85                                                  â”‚
â”‚   AP_car = 0.92                                                     â”‚
â”‚   AP_dog = 0.78                                                     â”‚
â”‚   AP_cat = 0.71                                                     â”‚
â”‚   ...                                                               â”‚
â”‚   AP_class_80 = 0.65                                                â”‚
â”‚                                                                     â”‚
â”‚                         APâ‚ + APâ‚‚ + ... + APc                       â”‚
â”‚   mAP@0.5 = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”‚
â”‚                              C (number of classes)                   â”‚
â”‚                                                                     â”‚
â”‚   COCO also reports:                                                â”‚
â”‚   â€¢ mAP@0.75 (stricter IoU threshold)                               â”‚
â”‚   â€¢ mAP@[0.5:0.95] (average over 10 thresholds: 0.5, 0.55, ..., 0.95)â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# mAP Calculation Example

```
Let's compute mAP step by step:

Dataset: 5 images with dogs and cats
Ground truth: 3 dogs, 2 cats

Model predictions (sorted by confidence):
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ #  â”‚ Class  â”‚ Conf   â”‚ Matched? â”‚ Status â”‚
â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1  â”‚ dog    â”‚ 0.95   â”‚ Yes      â”‚ TP     â”‚
â”‚ 2  â”‚ dog    â”‚ 0.90   â”‚ Yes      â”‚ TP     â”‚
â”‚ 3  â”‚ cat    â”‚ 0.85   â”‚ Yes      â”‚ TP     â”‚
â”‚ 4  â”‚ dog    â”‚ 0.80   â”‚ No       â”‚ FP     â”‚
â”‚ 5  â”‚ cat    â”‚ 0.75   â”‚ Yes      â”‚ TP     â”‚
â”‚ 6  â”‚ dog    â”‚ 0.70   â”‚ Yes      â”‚ TP     â”‚
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

For dogs: TP=3, FP=1, FN=0 (found all 3)
For cats: TP=2, FP=0, FN=0 (found all 2)

AP_dog = (compute P-R curve area)
AP_cat = (compute P-R curve area)
mAP = (AP_dog + AP_cat) / 2
```

---

# Data Augmentation for Detection

```
IMPORTANT: When transforming the image, ALSO transform the boxes!

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚  HORIZONTAL FLIP:                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚   Original              Flipped                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚   â”‚ â”Œâ”€â”€â”€â”        â”‚     â”‚        â”Œâ”€â”€â”€â” â”‚                            â”‚
â”‚   â”‚ â”‚Dogâ”‚        â”‚  ->  â”‚       â”‚Dogâ”‚ â”‚                            â”‚
â”‚   â”‚ â””â”€â”€â”€â”˜        â”‚     â”‚        â””â”€â”€â”€â”˜ â”‚                            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚   Box: [10,20,60,80]   Box: [W-60, 20, W-10, 80]                   â”‚
â”‚                                                                     â”‚
â”‚  SCALE/RESIZE:                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚   Scale factor = s                                                  â”‚
â”‚   New box: [x*s, y*s, w*s, h*s]                                    â”‚
â”‚                                                                     â”‚
â”‚  CROP:                                                              â”‚
â”‚  â”€â”€â”€â”€â”€                                                              â”‚
â”‚   Remove boxes that fall outside crop                               â”‚
â”‚   Clip boxes that partially overlap                                 â”‚
â”‚   Adjust coordinates relative to crop origin                        â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Mosaic Augmentation (YOLO v4+)

```
Combine 4 images into 1 training sample:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Image 1   â”‚   Image 2   â”‚
â”‚ D1   D2    â”‚     C1      â”‚
â”‚             â”‚   D3        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Image 3   â”‚   Image 4   â”‚
â”‚  C2         â”‚    D4 C3    â”‚
â”‚    C4       â”‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
    Single training image with
    4Ã— the objects!

BENEFITS:
â€¢ More objects per batch
â€¢ Objects in unusual contexts
â€¢ Better generalization
â€¢ Reduces need for large batch sizes
```

---

# Training Tips for Detection

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRACTICAL TRAINING TIPS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. START WITH PRETRAINED WEIGHTS                                   â”‚
â”‚     â€¢ Use COCO pretrained model                                     â”‚
â”‚     â€¢ Fine-tune on your dataset                                     â”‚
â”‚     â€¢ Much faster than training from scratch                        â”‚
â”‚                                                                     â”‚
â”‚  2. BALANCE YOUR DATASET                                            â”‚
â”‚     â€¢ Roughly equal examples per class                              â”‚
â”‚     â€¢ Or use focal loss to handle imbalance                         â”‚
â”‚                                                                     â”‚
â”‚  3. ANCHOR BOX ANALYSIS                                             â”‚
â”‚     â€¢ Run k-means on your box sizes                                 â”‚
â”‚     â€¢ Use auto-anchor feature in YOLOv5/v8                          â”‚
â”‚                                                                     â”‚
â”‚  4. IMAGE SIZE MATTERS                                              â”‚
â”‚     â€¢ Larger images = better small object detection                 â”‚
â”‚     â€¢ But slower training/inference                                  â”‚
â”‚     â€¢ 640Ã—640 is common default                                     â”‚
â”‚                                                                     â”‚
â”‚  5. AUGMENTATION IS KEY                                             â”‚
â”‚     â€¢ Mosaic, MixUp, color jitter                                   â”‚
â”‚     â€¢ More augmentation = better generalization                     â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Common Detection Benchmarks

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    POPULAR DATASETS & BENCHMARKS                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  â”‚                                                  â”‚
â”‚  PASCAL VOC      â”‚  â€¢ 20 classes, ~10K images                      â”‚
â”‚  (2007, 2012)    â”‚  â€¢ Classic benchmark                            â”‚
â”‚                  â”‚  â€¢ mAP@0.5 metric                                â”‚
â”‚                  â”‚                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  â”‚                                                  â”‚
â”‚  MS COCO         â”‚  â€¢ 80 classes, ~120K images                     â”‚
â”‚  (2014-present)  â”‚  â€¢ Modern standard benchmark                     â”‚
â”‚                  â”‚  â€¢ mAP@[0.5:0.95] metric (harder)               â”‚
â”‚                  â”‚  â€¢ Small/medium/large object splits              â”‚
â”‚                  â”‚                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  â”‚                                                  â”‚
â”‚  Open Images     â”‚  â€¢ 600 classes, 1.7M images                     â”‚
â”‚                  â”‚  â€¢ Largest, most diverse                         â”‚
â”‚                  â”‚                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                  â”‚                                                  â”‚
â”‚  Custom          â”‚  â€¢ Your domain-specific data                     â”‚
â”‚                  â”‚  â€¢ Label with Roboflow, CVAT, LabelImg          â”‚
â”‚                  â”‚                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# State-of-the-Art Results (2024)

```
MS COCO leaderboard (mAP@[0.5:0.95]):

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model                        â”‚   mAP    â”‚   Params   â”‚   Speed     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DINO (Transformer-based)     â”‚   63.3   â”‚   218M     â”‚   Slow      â”‚
â”‚  Focal-DINO                   â”‚   58.4   â”‚    -       â”‚   Slow      â”‚
â”‚  Co-DETR                      â”‚   66.0   â”‚   218M     â”‚   Slow      â”‚
â”‚  YOLOv8-X                     â”‚   53.9   â”‚   68M      â”‚   Fast      â”‚
â”‚  YOLOv9-E                     â”‚   55.6   â”‚   60M      â”‚   Fast      â”‚
â”‚  YOLO-World                   â”‚   45.0*  â”‚   46M      â”‚   Fast      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  * Zero-shot (no COCO training)

Key insight: Transformer detectors (DINO, DETR) now beat CNNs in accuracy
but YOLO family still dominates real-time applications.
```

---

# Complete Detection Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    END-TO-END DETECTION FLOW                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  INPUT: Image (640Ã—640Ã—3)                                          â”‚
â”‚     â”‚                                                               â”‚
â”‚     â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  BACKBONE: Extract features                    â”‚                 â”‚
â”‚  â”‚  (Conv layers, ResNet, CSPDarknet, etc.)      â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚     â”‚                                                               â”‚
â”‚     â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  NECK: Combine multi-scale features           â”‚                 â”‚
â”‚  â”‚  (FPN, PAN, BiFPN)                            â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚     â”‚                                                               â”‚
â”‚     â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  HEAD: Predict boxes + classes at each scale  â”‚                 â”‚
â”‚  â”‚  Output: ~8000 candidate boxes                â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚     â”‚                                                               â”‚
â”‚     â–¼                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  POST-PROCESSING:                             â”‚                 â”‚
â”‚  â”‚  1. Filter by confidence (> 0.25)             â”‚                 â”‚
â”‚  â”‚  2. Apply NMS per class                       â”‚                 â”‚
â”‚  â”‚  3. Output final detections                   â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚     â”‚                                                               â”‚
â”‚     â–¼                                                               â”‚
â”‚  OUTPUT: List of [class, confidence, x1, y1, x2, y2]               â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Getting Started: YOLO in 3 Lines

```python
# Install: pip install ultralytics

from ultralytics import YOLO

# Load a pretrained model
model = YOLO('yolov8n.pt')

# Run detection on an image
results = model('your_image.jpg')

# Display results
results[0].show()

# Access detections programmatically
for box in results[0].boxes:
    cls = int(box.cls[0])           # Class index
    conf = float(box.conf[0])       # Confidence
    xyxy = box.xyxy[0].tolist()     # [x1, y1, x2, y2]

    print(f"Class: {model.names[cls]}, Conf: {conf:.2f}")
    print(f"Box: {xyxy}")
```

<div class="realworld">
That's it! You now have a working object detector. Try it on your own photos!
</div>

---

# Training on Custom Data

```python
# Step 1: Prepare your dataset (YOLO format)
# data/
#   images/
#     train/  val/
#   labels/
#     train/  val/
# Each label file: class_id cx cy w h (normalized)

# Step 2: Create data.yaml
"""
path: ./data
train: images/train
val: images/val
names:
  0: cat
  1: dog
  2: bird
"""

# Step 3: Train!
from ultralytics import YOLO

model = YOLO('yolov8n.pt')  # Start from pretrained

results = model.train(
    data='data.yaml',
    epochs=50,
    imgsz=640,
    batch=16
)

# Step 4: Use your trained model
model = YOLO('runs/detect/train/weights/best.pt')
model.predict('test_image.jpg')
```

---

# Summary: Key Takeaways

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          KEY CONCEPTS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. DETECTION = CLASSIFICATION + LOCALIZATION                       â”‚
â”‚     Find WHAT objects and WHERE they are                            â”‚
â”‚                                                                     â”‚
â”‚  2. BOUNDING BOX FORMATS VARY                                       â”‚
â”‚     Always check: corner vs center, absolute vs normalized          â”‚
â”‚                                                                     â”‚
â”‚  3. IoU MEASURES OVERLAP QUALITY                                    â”‚
â”‚     IoU â‰¥ 0.5 is standard threshold for "correct"                  â”‚
â”‚                                                                     â”‚
â”‚  4. NMS REMOVES DUPLICATES                                          â”‚
â”‚     Keep highest confidence, remove overlapping                     â”‚
â”‚                                                                     â”‚
â”‚  5. YOLO IS FAST AND PRACTICAL                                      â”‚
â”‚     One-stage detection, grid-based, real-time capable              â”‚
â”‚                                                                     â”‚
â”‚  6. mAP IS THE GOLD STANDARD METRIC                                 â”‚
â”‚     Average precision across classes and IoU thresholds             â”‚
â”‚                                                                     â”‚
â”‚  7. START WITH PRETRAINED MODELS                                    â”‚
â”‚     Fine-tune on your data for best results                         â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Resources for Learning More

```
PAPERS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ YOLO v1: "You Only Look Once" (Redmon et al., 2015)
â€¢ YOLO v3: "YOLOv3: An Incremental Improvement" (Redmon, 2018)
â€¢ Faster R-CNN: "Faster R-CNN" (Ren et al., 2015)
â€¢ DETR: "End-to-End Object Detection with Transformers" (2020)

CODE & TUTORIALS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Ultralytics YOLOv8: https://github.com/ultralytics/ultralytics
â€¢ Roboflow Blog: https://blog.roboflow.com/
â€¢ PyTorch Detection: https://pytorch.org/vision/stable/models.html

DATASETS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ COCO: https://cocodataset.org/
â€¢ Open Images: https://storage.googleapis.com/openimages/
â€¢ Roboflow Universe: https://universe.roboflow.com/ (100K+ datasets)
```

---

# What's Next?

```
NOW YOU CAN:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ“ Understand how object detection differs from classification
âœ“ Work with bounding boxes in different formats
âœ“ Implement and understand IoU calculation
âœ“ Apply Non-Maximum Suppression
âœ“ Use YOLO for real-time detection
âœ“ Evaluate detectors with mAP

NEXT STEPS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Try detection on your own images/videos
â†’ Fine-tune YOLO on a custom dataset
â†’ Explore instance segmentation (Mask R-CNN, YOLOv8-seg)
â†’ Learn about transformer-based detectors (DETR, DINO)
â†’ Deploy detection models on edge devices
```

---

# Thank You!

**Object Detection opens the door to...**

- Self-driving cars
- Augmented reality
- Medical diagnosis
- Smart retail
- Security systems
- Robotics

And so much more!

**Questions?**

---

# Appendix: Common Pitfalls

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THINGS THAT GO WRONG                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. WRONG BOX FORMAT                                                â”‚
â”‚     â€¢ Model expects YOLO format, you gave COCO format              â”‚
â”‚     â€¢ Solution: Always verify format in documentation               â”‚
â”‚                                                                     â”‚
â”‚  2. FORGETTING TO NORMALIZE                                         â”‚
â”‚     â€¢ Pixel coordinates don't work for different image sizes       â”‚
â”‚     â€¢ Solution: Use normalized coordinates or resize consistently  â”‚
â”‚                                                                     â”‚
â”‚  3. NMS THRESHOLD TOO AGGRESSIVE                                    â”‚
â”‚     â€¢ Two close objects merged into one detection                  â”‚
â”‚     â€¢ Solution: Increase NMS threshold (0.5 â†’ 0.7)                 â”‚
â”‚                                                                     â”‚
â”‚  4. SMALL OBJECTS MISSED                                            â”‚
â”‚     â€¢ Default models optimized for medium/large objects            â”‚
â”‚     â€¢ Solution: Use larger image size, or specialized models       â”‚
â”‚                                                                     â”‚
â”‚  5. CLASS IMBALANCE                                                 â”‚
â”‚     â€¢ 1000 cars, 10 motorcycles â†’ model ignores motorcycles        â”‚
â”‚     â€¢ Solution: Oversample rare classes, use focal loss            â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Appendix: IoU Loss Variants

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    IoU-BASED LOSS FUNCTIONS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Standard IoU Loss:                                                 â”‚
â”‚      L = 1 - IoU                                                    â”‚
â”‚      Problem: No gradient when boxes don't overlap                  â”‚
â”‚                                                                     â”‚
â”‚  GIoU (Generalized IoU):                                            â”‚
â”‚      L = 1 - IoU + |C - Union| / |C|                               â”‚
â”‚      C = smallest enclosing box                                     â”‚
â”‚      Handles non-overlapping boxes!                                 â”‚
â”‚                                                                     â”‚
â”‚  DIoU (Distance IoU):                                               â”‚
â”‚      L = 1 - IoU + dÂ² / cÂ²                                         â”‚
â”‚      d = center distance, c = diagonal of enclosing box            â”‚
â”‚      Faster convergence                                             â”‚
â”‚                                                                     â”‚
â”‚  CIoU (Complete IoU):                                               â”‚
â”‚      L = 1 - IoU + dÂ² / cÂ² + Î±v                                    â”‚
â”‚      Î±, v = aspect ratio penalty terms                              â”‚
â”‚      Best overall performance                                       â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
