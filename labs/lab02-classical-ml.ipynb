{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Classical Machine Learning\n",
        "## Decision Trees, Random Forests, Regression & Clustering\n",
        "\n",
        "**Duration**: ~3 hours\n",
        "\n",
        "### Learning Objectives\n",
        "By the end of this lab, you will be able to:\n",
        "1. Understand how decision trees make predictions (if-then-else logic)\n",
        "2. Train Random Forests and understand ensemble methods\n",
        "3. Apply linear regression for continuous predictions\n",
        "4. Discover clusters in data with K-Means\n",
        "\n",
        "### Prerequisites\n",
        "- Completed Lab 1 (sklearn basics)\n",
        "- Basic Python knowledge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "# !pip install scikit-learn pandas numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard imports for this lab\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Better looking plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: Decision Trees\n",
        "\n",
        "## The \"20 Questions\" Game\n",
        "\n",
        "Have you ever played 20 Questions? You ask yes/no questions to guess what someone is thinking:\n",
        "\n",
        "```\n",
        "Is it alive?\n",
        "â”œâ”€â”€ YES: Is it a mammal?\n",
        "â”‚   â”œâ”€â”€ YES: Is it bigger than a cat?\n",
        "â”‚   â”‚   â”œâ”€â”€ YES: Dog? Elephant?\n",
        "â”‚   â”‚   â””â”€â”€ NO: Cat? Hamster?\n",
        "â”‚   â””â”€â”€ NO: Is it a bird?\n",
        "â”‚       â”œâ”€â”€ YES: Parrot? Eagle?\n",
        "â”‚       â””â”€â”€ NO: Fish? Snake?\n",
        "â””â”€â”€ NO: Is it electronic?\n",
        "    â”œâ”€â”€ YES: Phone? Computer?\n",
        "    â””â”€â”€ NO: Book? Chair?\n",
        "```\n",
        "\n",
        "**This is exactly how a Decision Tree works!**\n",
        "\n",
        "A Decision Tree asks questions about your data features, one at a time, until it can make a prediction.\n",
        "\n",
        "### Why Decision Trees?\n",
        "\n",
        "| Advantage | Description |\n",
        "|-----------|-------------|\n",
        "| **Interpretable** | You can see exactly WHY it made a prediction |\n",
        "| **No scaling needed** | Works with raw feature values |\n",
        "| **Handles both types** | Works with numbers AND categories |\n",
        "| **Fast** | Very quick predictions |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SOLVED EXAMPLE: Building a Decision Tree\n",
        "\n",
        "Let's build a decision tree to classify Iris flowers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Decision Tree on Iris\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the tree\n",
        "# max_depth=3 limits how many questions it can ask\n",
        "tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Check accuracy\n",
        "train_acc = tree.score(X_train, y_train)\n",
        "test_acc = tree.score(X_test, y_test)\n",
        "\n",
        "print(f\"Training Accuracy: {train_acc:.2%}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# VISUALIZE THE TREE - This is the magic of decision trees!\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(tree, \n",
        "          feature_names=iris.feature_names,\n",
        "          class_names=iris.target_names,\n",
        "          filled=True,           # Color by class\n",
        "          rounded=True,          # Rounded boxes\n",
        "          fontsize=12)\n",
        "plt.title(\"Decision Tree for Iris Classification\\n(Each box is a question!)\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reading the Tree:\n",
        "\n",
        "Each box shows:\n",
        "- **Question**: e.g., `petal length <= 2.45`\n",
        "- **gini**: Impurity measure (0 = pure, only one class)\n",
        "- **samples**: How many training samples reached this node\n",
        "- **value**: Count of each class [setosa, versicolor, virginica]\n",
        "- **class**: Majority class at this node\n",
        "\n",
        "**To make a prediction**: Start at the top, answer each question, follow the arrow (left=True, right=False)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's trace a prediction!\n",
        "\n",
        "# Take one test sample\n",
        "sample = X_test[0:1]\n",
        "print(\"Sample features:\")\n",
        "for name, val in zip(iris.feature_names, sample[0]):\n",
        "    print(f\"  {name}: {val:.2f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Following the tree:\")\n",
        "print(f\"  Q1: petal length ({sample[0][2]:.2f}) <= 2.45? {sample[0][2] <= 2.45}\")\n",
        "if sample[0][2] <= 2.45:\n",
        "    print(\"  â†’ Going LEFT â†’ Predict: setosa\")\n",
        "else:\n",
        "    print(f\"  â†’ Going RIGHT\")\n",
        "    print(f\"  Q2: petal width ({sample[0][3]:.2f}) <= 1.75? {sample[0][3] <= 1.75}\")\n",
        "\n",
        "print(\"\\nActual prediction:\", iris.target_names[tree.predict(sample)[0]])\n",
        "print(\"True label:\", iris.target_names[y_test[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance\n",
        "\n",
        "Which features matter most? The tree tells us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Feature Importance\n",
        "\n",
        "importance = tree.feature_importances_\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(10, 5))\n",
        "bars = plt.barh(iris.feature_names, importance, color='steelblue')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Which Features Matter Most?')\n",
        "\n",
        "# Add value labels\n",
        "for bar, imp in zip(bars, importance):\n",
        "    plt.text(imp + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "             f'{imp:.2%}', va='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ” Insight: Petal measurements are much more important than sepal!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Overfitting Problem\n",
        "\n",
        "What happens if we let the tree grow too deep?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Overfitting with Deep Trees\n",
        "\n",
        "# Compare different tree depths\n",
        "depths = [1, 2, 3, 5, 10, None]  # None = no limit\n",
        "results = []\n",
        "\n",
        "for depth in depths:\n",
        "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    tree.fit(X_train, y_train)\n",
        "    \n",
        "    train_acc = tree.score(X_train, y_train)\n",
        "    test_acc = tree.score(X_test, y_test)\n",
        "    \n",
        "    results.append({\n",
        "        'depth': str(depth) if depth else 'None',\n",
        "        'train': train_acc,\n",
        "        'test': test_acc\n",
        "    })\n",
        "\n",
        "# Plot comparison\n",
        "df = pd.DataFrame(results)\n",
        "x = range(len(depths))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(x, df['train'], 'o-', label='Training', linewidth=2, markersize=8)\n",
        "plt.plot(x, df['test'], 's-', label='Test', linewidth=2, markersize=8)\n",
        "plt.xticks(x, df['depth'])\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Decision Tree: Training vs Test Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add annotation\n",
        "plt.annotate('Overfitting!\\n(training >> test)', \n",
        "             xy=(5, df['train'].iloc[-1]), \n",
        "             xytext=(4, 0.85),\n",
        "             arrowprops=dict(arrowstyle='->', color='red'),\n",
        "             fontsize=12, color='red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What is Overfitting?\n",
        "\n",
        "```\n",
        "OVERFITTING:\n",
        "\n",
        "  Training Data          New Data\n",
        "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "  â”‚ â˜… Memorizes â”‚        â”‚ âœ— Fails on  â”‚\n",
        "  â”‚   every     â”‚   â†’    â”‚   new data  â”‚\n",
        "  â”‚   detail!   â”‚        â”‚             â”‚\n",
        "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "  \n",
        "  Training: 100%          Test: 70%\n",
        "  \n",
        "  Like a student who memorizes answers\n",
        "  but doesn't understand concepts!\n",
        "```\n",
        "\n",
        "**Solution**: Limit tree depth, or use Random Forests!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1.1\n",
        "\n",
        "Train a decision tree on the digits dataset. Find the best `max_depth` that balances training and test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Load digits\n",
        "digits = load_digits()\n",
        "X_digits, y_digits = digits.data, digits.target\n",
        "\n",
        "# Split into train/test\n",
        "# ...\n",
        "\n",
        "# Try different depths and find the best one\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Random Forests\n",
        "\n",
        "## The Wisdom of Crowds\n",
        "\n",
        "Imagine you're on a game show:\n",
        "\n",
        "```\n",
        "Option A: Ask ONE expert\n",
        "Option B: Ask 100 random people and take majority vote\n",
        "```\n",
        "\n",
        "Surprisingly, Option B often wins! This is called the **Wisdom of Crowds**.\n",
        "\n",
        "**Random Forest = Many decision trees voting together**\n",
        "\n",
        "```\n",
        "                    RANDOM FOREST\n",
        "                         \n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”\n",
        "    â”‚Tree 1â”‚   â”‚Tree 2â”‚   â”‚Tree 3â”‚   â”‚Tree Nâ”‚\n",
        "    â”‚      â”‚   â”‚      â”‚   â”‚      â”‚   â”‚      â”‚\n",
        "    â””â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”€â”˜\n",
        "       â”‚          â”‚          â”‚          â”‚\n",
        "       â–¼          â–¼          â–¼          â–¼\n",
        "      Cat        Dog        Cat        Cat\n",
        "       â”‚          â”‚          â”‚          â”‚\n",
        "       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                       â”‚\n",
        "                       â–¼\n",
        "                   VOTE: Cat wins!\n",
        "                   (3 votes to 1)\n",
        "```\n",
        "\n",
        "### Why Does This Work?\n",
        "\n",
        "Each tree is slightly different because:\n",
        "1. **Bootstrap sampling**: Each tree trains on a random subset of data\n",
        "2. **Random features**: Each split considers only random subset of features\n",
        "\n",
        "Individual trees might make mistakes, but **different trees make different mistakes**.\n",
        "When they vote, the errors cancel out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Random Forest vs Single Tree\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Single tree\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "\n",
        "# Random Forest (100 trees)\n",
        "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "forest.fit(X_train, y_train)\n",
        "\n",
        "print(\"Comparison:\")\n",
        "print(f\"  Single Tree Test Accuracy: {single_tree.score(X_test, y_test):.2%}\")\n",
        "print(f\"  Random Forest Test Accuracy: {forest.score(X_test, y_test):.2%}\")\n",
        "print(f\"\\n  Number of trees in forest: {len(forest.estimators_)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: How Many Trees Do We Need?\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "# Use a harder dataset\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "n_trees_list = [1, 5, 10, 25, 50, 100, 200]\n",
        "accuracies = []\n",
        "\n",
        "for n_trees in n_trees_list:\n",
        "    forest = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    forest.fit(X_train, y_train)\n",
        "    acc = forest.score(X_test, y_test)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"  {n_trees:3d} trees: {acc:.2%}\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(n_trees_list, accuracies, 'o-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('Random Forest: More Trees = Better (up to a point)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add diminishing returns annotation\n",
        "plt.axhline(y=max(accuracies), color='green', linestyle='--', alpha=0.5)\n",
        "plt.annotate('Diminishing returns', \n",
        "             xy=(150, max(accuracies)-0.01),\n",
        "             fontsize=12, color='green')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Feature Importance from Random Forest\n",
        "\n",
        "# Train on Iris\n",
        "iris = load_iris()\n",
        "forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "forest.fit(iris.data, iris.target)\n",
        "\n",
        "# Get importance (averaged across all trees)\n",
        "importance = forest.feature_importances_\n",
        "\n",
        "# Sort by importance\n",
        "sorted_idx = np.argsort(importance)[::-1]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(len(importance)), importance[sorted_idx], color='forestgreen')\n",
        "plt.xticks(range(len(importance)), [iris.feature_names[i] for i in sorted_idx])\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Importance')\n",
        "plt.title('Feature Importance (averaged across 100 trees)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2.1\n",
        "\n",
        "Train both a Decision Tree and a Random Forest on the digits dataset. Compare their:\n",
        "1. Test accuracy\n",
        "2. Training time\n",
        "\n",
        "Use `%%time` magic to measure time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "import time\n",
        "\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train and time Decision Tree\n",
        "# ...\n",
        "\n",
        "# Train and time Random Forest\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: Regression\n",
        "\n",
        "## From Classification to Regression\n",
        "\n",
        "So far we've predicted **categories** (setosa, versicolor, virginica).\n",
        "\n",
        "But what if we want to predict **numbers**?\n",
        "\n",
        "- House price: $350,000\n",
        "- Temperature tomorrow: 25.3Â°C\n",
        "- Student's exam score: 87.5\n",
        "\n",
        "This is **Regression**!\n",
        "\n",
        "```\n",
        "CLASSIFICATION vs REGRESSION:\n",
        "\n",
        "Classification:          Regression:\n",
        "\"Which category?\"        \"What number?\"\n",
        "\n",
        "   â”Œâ”€â”€â”€â”                    â”‚\n",
        "   â”‚Catâ”‚                    â”‚    â•­â”€â”€â”€â”€â”€â•®\n",
        "   â””â”€â”€â”€â”˜                    â”‚   â•±       â•²\n",
        "   â”Œâ”€â”€â”€â”                    â”‚  â•±   â—â—    â•²â—\n",
        "   â”‚Dogâ”‚                    â”‚ â•± â—â—  â—â—    \n",
        "   â””â”€â”€â”€â”˜                    â”‚â•±â—            \n",
        "   â”Œâ”€â”€â”€â”€â”                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "   â”‚Birdâ”‚                   \n",
        "   â””â”€â”€â”€â”€â”˜                   Continuous output\n",
        "   \n",
        "Discrete output            (any number!)\n",
        "(one of these)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression: The Line of Best Fit\n",
        "\n",
        "The simplest idea: fit a straight line through your data!\n",
        "\n",
        "```\n",
        "y = mx + b\n",
        "  = weight * feature + bias\n",
        "```\n",
        "\n",
        "The goal: find `m` and `b` that minimize the errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Simple Linear Regression\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Generate simple data: y = 2x + 1 + noise\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10  # 100 points from 0 to 10\n",
        "y = 2 * X.squeeze() + 1 + np.random.randn(100) * 2  # y = 2x + 1 + noise\n",
        "\n",
        "# Fit linear regression\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get the line parameters\n",
        "print(f\"True equation: y = 2x + 1\")\n",
        "print(f\"Learned equation: y = {model.coef_[0]:.2f}x + {model.intercept_:.2f}\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, alpha=0.6, label='Data points')\n",
        "plt.plot(X, model.predict(X), color='red', linewidth=2, label='Best fit line')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Linear Regression: Finding the Best Line')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Real Dataset - California Housing\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load data\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "print(\"California Housing Dataset:\")\n",
        "print(f\"  Samples: {X.shape[0]}\")\n",
        "print(f\"  Features: {X.shape[1]}\")\n",
        "print(f\"  Target: Median house value (in $100,000s)\")\n",
        "print(f\"\\nFeatures:\")\n",
        "for i, name in enumerate(housing.feature_names):\n",
        "    print(f\"  {i+1}. {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train linear regression on housing data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features (important for regression!)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Results:\")\n",
        "print(f\"  RMSE: ${rmse * 100000:,.0f}\")  # Convert back to dollars\n",
        "print(f\"  RÂ² Score: {r2:.3f}\")\n",
        "print(f\"\\n  RÂ² means: {r2*100:.1f}% of variance is explained by our features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Visualizing Predictions vs Actual\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, y_pred, alpha=0.3)\n",
        "plt.plot([0, 5], [0, 5], 'r--', linewidth=2, label='Perfect prediction')\n",
        "plt.xlabel('Actual Price ($100,000s)')\n",
        "plt.ylabel('Predicted Price ($100,000s)')\n",
        "plt.title('Prediction vs Actual House Prices')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Points on the red line = perfect predictions\")\n",
        "print(\"Points far from line = large errors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Feature Importance in Linear Regression\n",
        "\n",
        "# Get coefficients (weights)\n",
        "coefs = pd.DataFrame({\n",
        "    'feature': housing.feature_names,\n",
        "    'coefficient': model.coef_\n",
        "}).sort_values('coefficient', key=abs, ascending=True)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['green' if c > 0 else 'red' for c in coefs['coefficient']]\n",
        "plt.barh(coefs['feature'], coefs['coefficient'], color=colors)\n",
        "plt.xlabel('Coefficient (scaled)')\n",
        "plt.title('Feature Impact on House Price\\n(Green = increases price, Red = decreases)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding Regression Metrics\n",
        "\n",
        "| Metric | Formula | Interpretation |\n",
        "|--------|---------|----------------|\n",
        "| **MSE** | Mean of (actual - predicted)Â² | Lower = better |\n",
        "| **RMSE** | âˆšMSE | Same units as target (e.g., $) |\n",
        "| **RÂ²** | 1 - (errors / variance) | 0 to 1, higher = better |\n",
        "\n",
        "```\n",
        "RÂ² Interpretation:\n",
        "  RÂ² = 0.0 â†’ Model is no better than predicting the mean\n",
        "  RÂ² = 0.5 â†’ Model explains 50% of the variance\n",
        "  RÂ² = 1.0 â†’ Perfect predictions!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3.1\n",
        "\n",
        "Compare Linear Regression with Decision Tree Regression on the housing dataset. Which performs better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Hint: DecisionTreeRegressor works just like DecisionTreeClassifier\n",
        "# but predicts continuous values!\n",
        "\n",
        "# Train both models\n",
        "# ...\n",
        "\n",
        "# Compare RÂ² scores\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: Clustering with K-Means\n",
        "\n",
        "## Unsupervised Learning: No Labels!\n",
        "\n",
        "So far, all our data had labels:\n",
        "- Iris: \"setosa\", \"versicolor\", \"virginica\"\n",
        "- Housing: actual prices\n",
        "\n",
        "But what if we have data **without labels**?\n",
        "\n",
        "```\n",
        "SUPERVISED (with labels)      UNSUPERVISED (no labels)\n",
        "                              \n",
        "     â—  Cat                        â—  ???\n",
        "     â–   Dog                        â—  ???\n",
        "     â–²  Bird                       â—  ???\n",
        "                              \n",
        "  \"Learn the pattern\"          \"Find the pattern\"\n",
        "```\n",
        "\n",
        "**Clustering** = Group similar things together (without being told the groups!)\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "- **Customer Segmentation**: Group customers by behavior (big spenders, bargain hunters, etc.)\n",
        "- **Document Clustering**: Group news articles by topic\n",
        "- **Image Compression**: Group similar colors together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Means Algorithm\n",
        "\n",
        "The simplest clustering algorithm. Works in 3 steps:\n",
        "\n",
        "```\n",
        "K-MEANS ALGORITHM:\n",
        "\n",
        "Step 1: Pick K random points as \"centers\"\n",
        "        â˜…        â˜…        â˜…\n",
        "         (3 centers for K=3)\n",
        "\n",
        "Step 2: Assign each point to nearest center\n",
        "        â—â—â—â˜…â—â—   â– â– â˜…â– â–     â–²â–²â˜…â–²â–²\n",
        "        (3 clusters formed)\n",
        "\n",
        "Step 3: Move centers to middle of their cluster\n",
        "        â—â—â˜…â—â—â—   â– â– â˜…â– â– â–    â–²â–²â˜…â–²â–²â–²\n",
        "           â†‘       â†‘        â†‘\n",
        "        (new center positions)\n",
        "\n",
        "Repeat steps 2-3 until centers stop moving!\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: K-Means Visualization\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# Generate sample data with 3 natural clusters\n",
        "X, y_true = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Before clustering - we don't know the groups!\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.6)\n",
        "plt.title('Before Clustering\\n(We just see points)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(X)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
        "            c='red', marker='X', s=200, edgecolors='black', linewidths=2,\n",
        "            label='Cluster Centers')\n",
        "plt.title('After K-Means Clustering\\n(Groups discovered!)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: K-Means Step by Step\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "X, _ = make_blobs(n_samples=150, centers=3, cluster_std=1.5, random_state=42)\n",
        "\n",
        "for i, (ax, max_iter) in enumerate(zip(axes.flat, [1, 2, 3, 5, 10, 100])):\n",
        "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=1, max_iter=max_iter)\n",
        "    clusters = kmeans.fit_predict(X)\n",
        "    \n",
        "    ax.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
        "    ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
        "               c='red', marker='X', s=200, edgecolors='black', linewidths=2)\n",
        "    ax.set_title(f'Iteration {max_iter}')\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "plt.suptitle('K-Means: Watch the Centers Move!', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Big Question: How Many Clusters (K)?\n",
        "\n",
        "K-Means needs you to tell it how many clusters to find.\n",
        "\n",
        "**The Elbow Method**: Try different K values and look for the \"elbow\" in the plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: The Elbow Method\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Try K from 1 to 10\n",
        "inertias = []\n",
        "K_range = range(1, 11)\n",
        "\n",
        "for k in K_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X)\n",
        "    inertias.append(kmeans.inertia_)  # Sum of squared distances to centers\n",
        "\n",
        "# Plot the elbow\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=10)\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('Inertia (sum of squared distances)')\n",
        "plt.title('The Elbow Method: Finding Optimal K')\n",
        "\n",
        "# Mark the elbow\n",
        "plt.annotate('Elbow here!\\n(K=4)', \n",
        "             xy=(4, inertias[3]),\n",
        "             xytext=(6, inertias[3] + 200),\n",
        "             arrowprops=dict(arrowstyle='->', color='red'),\n",
        "             fontsize=12, color='red')\n",
        "\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Why elbow? After K=4, adding more clusters doesn't help much.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Clustering Iris (without using labels!)\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load iris - but pretend we don't have labels!\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y_true = iris.target  # We'll only use this to check our clustering\n",
        "\n",
        "# Cluster with K=3 (we happen to know there are 3 species)\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualize using first 2 principal components\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_2d = pca.fit_transform(X)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Our clusters\n",
        "axes[0].scatter(X_2d[:, 0], X_2d[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n",
        "axes[0].set_title('K-Means Clusters\\n(discovered without labels!)')\n",
        "axes[0].set_xlabel('PC1')\n",
        "axes[0].set_ylabel('PC2')\n",
        "\n",
        "# True labels\n",
        "axes[1].scatter(X_2d[:, 0], X_2d[:, 1], c=y_true, cmap='viridis', alpha=0.6)\n",
        "axes[1].set_title('True Species\\n(the actual labels)')\n",
        "axes[1].set_xlabel('PC1')\n",
        "axes[1].set_ylabel('PC2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# How well did we do?\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "ari = adjusted_rand_score(y_true, clusters)\n",
        "print(f\"\\nHow well do clusters match true species?\")\n",
        "print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
        "print(f\"(1.0 = perfect match, 0.0 = random)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4.1\n",
        "\n",
        "Apply K-Means to the digits dataset. Use the Elbow Method to find the optimal number of clusters.\n",
        "\n",
        "Note: Digits has 10 classes (0-9), but the optimal K might be different!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Apply elbow method (try K from 1 to 20)\n",
        "# ...\n",
        "\n",
        "# Plot the elbow curve\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Challenge Problems\n",
        "\n",
        "Complete these to test your understanding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 1: Model Comparison\n",
        "\n",
        "Compare the performance of:\n",
        "1. Decision Tree\n",
        "2. Random Forest\n",
        "3. Logistic Regression (from Lab 1)\n",
        "\n",
        "on the digits dataset. Create a bar chart comparing their accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 2: Polynomial Regression\n",
        "\n",
        "Linear regression fits a straight line. But what if the relationship is curved?\n",
        "\n",
        "Use `sklearn.preprocessing.PolynomialFeatures` to fit a curved line to this data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate curved data\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y = 0.5 * X.squeeze()**2 - 3*X.squeeze() + 10 + np.random.randn(100) * 3\n",
        "\n",
        "plt.scatter(X, y)\n",
        "plt.title('This is clearly not linear!')\n",
        "plt.show()\n",
        "\n",
        "# YOUR CODE HERE: Fit polynomial regression\n",
        "# Hint: from sklearn.preprocessing import PolynomialFeatures\n",
        "# Transform X using PolynomialFeatures(degree=2), then fit LinearRegression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 3: Customer Segmentation\n",
        "\n",
        "Create synthetic customer data and use K-Means to segment customers into groups.\n",
        "Describe what each cluster represents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic customer data\n",
        "np.random.seed(42)\n",
        "n_customers = 200\n",
        "\n",
        "# Features: [annual_income, spending_score]\n",
        "# Spending score: 1-100 (how much they spend)\n",
        "\n",
        "customers = pd.DataFrame({\n",
        "    'annual_income': np.concatenate([\n",
        "        np.random.normal(30, 10, 50),   # Low income\n",
        "        np.random.normal(60, 10, 100),  # Medium income  \n",
        "        np.random.normal(100, 15, 50)   # High income\n",
        "    ]),\n",
        "    'spending_score': np.concatenate([\n",
        "        np.random.normal(20, 10, 50),   # Low spenders\n",
        "        np.random.normal(50, 15, 100),  # Medium spenders\n",
        "        np.random.normal(80, 10, 50)    # High spenders\n",
        "    ])\n",
        "})\n",
        "\n",
        "# Clip to valid ranges\n",
        "customers['annual_income'] = customers['annual_income'].clip(10, 150)\n",
        "customers['spending_score'] = customers['spending_score'].clip(1, 100)\n",
        "\n",
        "print(customers.head())\n",
        "print(f\"\\n{len(customers)} customers\")\n",
        "\n",
        "# YOUR CODE HERE: \n",
        "# 1. Cluster customers into 3-5 groups\n",
        "# 2. Visualize the clusters\n",
        "# 3. Describe what each cluster represents (e.g., \"Budget shoppers\", \"Premium customers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "## What We Learned\n",
        "\n",
        "| Algorithm | Type | Use Case |\n",
        "|-----------|------|----------|\n",
        "| **Decision Tree** | Classification/Regression | Interpretable predictions, feature importance |\n",
        "| **Random Forest** | Classification/Regression | Better accuracy via ensemble |\n",
        "| **Linear Regression** | Regression | Predict continuous values |\n",
        "| **K-Means** | Clustering | Group similar items (no labels needed) |\n",
        "\n",
        "## Key Code Patterns\n",
        "\n",
        "```python\n",
        "# Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "tree = DecisionTreeClassifier(max_depth=3)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "forest = RandomForestClassifier(n_estimators=100)\n",
        "forest.fit(X_train, y_train)\n",
        "\n",
        "# Linear Regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# K-Means Clustering\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=3)\n",
        "clusters = kmeans.fit_predict(X)\n",
        "```\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "In **Lab 3**, we'll move beyond sklearn to **PyTorch** and build our first neural network!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
