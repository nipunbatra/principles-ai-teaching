{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 5: Deploying Your Language Model (SLM-2)\n",
        "## From Training to Production\n",
        "\n",
        "**Duration**: ~2-3 hours\n",
        "\n",
        "### Learning Objectives\n",
        "By the end of this lab, you will be able to:\n",
        "1. Save and load trained PyTorch models\n",
        "2. Understand inference settings (temperature, top-k, top-p)\n",
        "3. Build a web interface with Gradio\n",
        "4. Deploy your model for others to use!\n",
        "\n",
        "### Prerequisites\n",
        "- Completed Lab 4 (SLM Building)\n",
        "\n",
        "### The Big Picture\n",
        "\n",
        "```\n",
        "Lab 4: TRAINING               Lab 5: DEPLOYMENT\n",
        "\n",
        "┌─────────────┐              ┌─────────────┐\n",
        "│   Dataset   │              │ Saved Model │\n",
        "└──────┬──────┘              └──────┬──────┘\n",
        "       │                            │\n",
        "       ▼                            ▼\n",
        "┌─────────────┐              ┌─────────────┐\n",
        "│  Training   │     ──►      │  Inference  │\n",
        "│    Loop     │   save       │   Engine    │\n",
        "└──────┬──────┘              └──────┬──────┘\n",
        "       │                            │\n",
        "       ▼                            ▼\n",
        "┌─────────────┐              ┌─────────────┐\n",
        "│   Weights   │              │  Gradio UI  │\n",
        "└─────────────┘              └─────────────┘\n",
        "                                    │\n",
        "                                    ▼\n",
        "                             ┌─────────────┐\n",
        "                             │   Users!    │\n",
        "                             └─────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Gradio (for web interface)\n",
        "# !pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: Saving & Loading Models\n",
        "\n",
        "## Why Save Models?\n",
        "\n",
        "Training takes time! We don't want to retrain every time we use the model.\n",
        "\n",
        "**Save once, use forever.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's recreate our model from Lab 4\n",
        "# (Or load it if you saved it!)\n",
        "\n",
        "# Download dataset\n",
        "import urllib.request\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\n",
        "filename = \"names.txt\"\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "with open(filename, 'r') as f:\n",
        "    names = f.read().splitlines()\n",
        "\n",
        "# Build vocabulary\n",
        "chars = sorted(list(set(''.join(names).lower())))\n",
        "chars = ['.'] + chars\n",
        "vocab_size = len(chars)\n",
        "\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "print(f\"Vocabulary: {chars}\")\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model definition (same as Lab 4)\n",
        "\n",
        "class CharLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, context_length):\n",
        "        super().__init__()\n",
        "        self.context_length = context_length\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc1 = nn.Linear(context_length * embed_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        emb = emb.view(emb.size(0), -1)\n",
        "        h = torch.tanh(self.fc1(emb))\n",
        "        logits = self.fc2(h)\n",
        "        return logits\n",
        "\n",
        "# Config\n",
        "context_length = 3\n",
        "embed_dim = 10\n",
        "hidden_dim = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Quick Training (or load saved model)\n",
        "\n",
        "def build_dataset(names, context_length):\n",
        "    X, Y = [], []\n",
        "    for name in names:\n",
        "        name = '.' * context_length + name.lower() + '.'\n",
        "        for i in range(len(name) - context_length):\n",
        "            context = name[i:i+context_length]\n",
        "            target = name[i+context_length]\n",
        "            X.append([char_to_idx[c] for c in context])\n",
        "            Y.append(char_to_idx[target])\n",
        "    return torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "# Build and train\n",
        "X, Y = build_dataset(names, context_length)\n",
        "X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "model = CharLM(vocab_size, embed_dim, hidden_dim, context_length).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Quick training (5 epochs)\n",
        "print(\"Training model...\")\n",
        "for epoch in range(10):\n",
        "    # Random mini-batches\n",
        "    idx = torch.randperm(len(X))[:5000]\n",
        "    X_batch, Y_batch = X[idx], Y[idx]\n",
        "    \n",
        "    logits = model(X_batch)\n",
        "    loss = criterion(logits, Y_batch)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Saving the Model\n",
        "\n",
        "# Method 1: Save state_dict (RECOMMENDED)\n",
        "# Only saves the weights, not the architecture\n",
        "\n",
        "save_path = \"char_lm_weights.pt\"\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'vocab_size': vocab_size,\n",
        "    'embed_dim': embed_dim,\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'context_length': context_length,\n",
        "    'chars': chars,\n",
        "    'char_to_idx': char_to_idx,\n",
        "    'idx_to_char': idx_to_char,\n",
        "}, save_path)\n",
        "\n",
        "print(f\"Model saved to {save_path}\")\n",
        "print(f\"File size: {os.path.getsize(save_path) / 1024:.1f} KB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Loading the Model\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load(save_path, map_location=device)\n",
        "\n",
        "# Recreate model with saved config\n",
        "loaded_model = CharLM(\n",
        "    vocab_size=checkpoint['vocab_size'],\n",
        "    embed_dim=checkpoint['embed_dim'],\n",
        "    hidden_dim=checkpoint['hidden_dim'],\n",
        "    context_length=checkpoint['context_length']\n",
        ").to(device)\n",
        "\n",
        "# Load weights\n",
        "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "loaded_model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Restore vocabulary mappings\n",
        "chars = checkpoint['chars']\n",
        "char_to_idx = checkpoint['char_to_idx']\n",
        "idx_to_char = checkpoint['idx_to_char']\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Context length: {checkpoint['context_length']}\")\n",
        "print(f\"Vocab size: {checkpoint['vocab_size']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Checkpoints\n",
        "\n",
        "For longer training, save periodically:\n",
        "\n",
        "```python\n",
        "for epoch in range(100):\n",
        "    train()\n",
        "    \n",
        "    # Save every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "        }, f'checkpoint_epoch_{epoch+1}.pt')\n",
        "```\n",
        "\n",
        "This lets you:\n",
        "- Resume training if it crashes\n",
        "- Go back to earlier versions if model gets worse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1.1\n",
        "\n",
        "Compare the file sizes of:\n",
        "1. Saving just `model.state_dict()`\n",
        "2. Saving the entire model with `torch.save(model, ...)`\n",
        "\n",
        "Which is smaller? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Save just state_dict\n",
        "torch.save(model.state_dict(), 'just_weights.pt')\n",
        "\n",
        "# Save entire model\n",
        "torch.save(model, 'full_model.pt')\n",
        "\n",
        "# Compare sizes\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Inference & Sampling Strategies\n",
        "\n",
        "## Temperature Revisited\n",
        "\n",
        "Temperature controls how \"creative\" the model is:\n",
        "\n",
        "```\n",
        "TEMPERATURE EFFECT:\n",
        "\n",
        "Original probabilities:  [0.7, 0.2, 0.05, 0.03, 0.02]\n",
        "\n",
        "Low temp (0.5):         [0.9, 0.08, 0.01, 0.005, 0.005]\n",
        "                        (Very confident - always picks 'a')\n",
        "\n",
        "High temp (2.0):        [0.4, 0.25, 0.15, 0.12, 0.08]\n",
        "                        (More uniform - might pick anything)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Temperature Visualization\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fake logits from a model\n",
        "logits = torch.tensor([3.0, 1.5, 0.5, -0.5, -1.0])\n",
        "\n",
        "temperatures = [0.3, 0.5, 1.0, 1.5, 2.0]\n",
        "\n",
        "fig, axes = plt.subplots(1, len(temperatures), figsize=(15, 3))\n",
        "\n",
        "for ax, temp in zip(axes, temperatures):\n",
        "    probs = F.softmax(logits / temp, dim=0).numpy()\n",
        "    ax.bar(range(5), probs, color=['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#592941'])\n",
        "    ax.set_title(f'Temp = {temp}')\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_xticks(range(5))\n",
        "    ax.set_xticklabels(['a', 'b', 'c', 'd', 'e'])\n",
        "\n",
        "plt.suptitle('Effect of Temperature on Probability Distribution', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Top-K Sampling\n",
        "\n",
        "Only consider the K most likely tokens.\n",
        "\n",
        "```\n",
        "TOP-K SAMPLING (k=3):\n",
        "\n",
        "Original:   [0.7, 0.15, 0.08, 0.04, 0.03]  (5 options)\n",
        "After top-k: [0.7, 0.15, 0.08, 0,    0   ]  (only top 3)\n",
        "Renormalized: [0.75, 0.16, 0.09, 0,   0   ]  (sums to 1)\n",
        "```\n",
        "\n",
        "**Why?** Prevents very unlikely tokens from being sampled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Top-K Sampling\n",
        "\n",
        "def top_k_sampling(logits, k):\n",
        "    \"\"\"Keep only top k logits, set rest to -inf.\"\"\"\n",
        "    # Get top k values and indices\n",
        "    top_k_values, top_k_indices = torch.topk(logits, k)\n",
        "    \n",
        "    # Create new logits with only top k\n",
        "    filtered_logits = torch.full_like(logits, float('-inf'))\n",
        "    filtered_logits.scatter_(0, top_k_indices, top_k_values)\n",
        "    \n",
        "    return filtered_logits\n",
        "\n",
        "# Example\n",
        "logits = torch.tensor([3.0, 1.5, 0.5, -0.5, -1.0])\n",
        "chars_example = ['a', 'b', 'c', 'd', 'e']\n",
        "\n",
        "print(\"Original probabilities:\")\n",
        "probs = F.softmax(logits, dim=0)\n",
        "for c, p in zip(chars_example, probs):\n",
        "    print(f\"  '{c}': {p.item():.2%}\")\n",
        "\n",
        "print(\"\\nAfter top-3 filtering:\")\n",
        "filtered = top_k_sampling(logits, k=3)\n",
        "probs = F.softmax(filtered, dim=0)\n",
        "for c, p in zip(chars_example, probs):\n",
        "    print(f\"  '{c}': {p.item():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Top-P (Nucleus) Sampling\n",
        "\n",
        "Keep tokens until cumulative probability reaches P.\n",
        "\n",
        "```\n",
        "TOP-P SAMPLING (p=0.9):\n",
        "\n",
        "Sorted probs:   [0.7, 0.15, 0.08, 0.04, 0.03]\n",
        "Cumulative:     [0.7, 0.85, 0.93, 0.97, 1.0]\n",
        "                       ↑\n",
        "                 Stop here (0.93 > 0.9)\n",
        "                 \n",
        "Keep:           [0.7, 0.15, 0.08]  (top 3, sum=0.93)\n",
        "```\n",
        "\n",
        "**Why?** Adapts to the distribution - uses more tokens when uncertain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Top-P (Nucleus) Sampling\n",
        "\n",
        "def top_p_sampling(logits, p):\n",
        "    \"\"\"Keep tokens until cumulative probability reaches p.\"\"\"\n",
        "    probs = F.softmax(logits, dim=0)\n",
        "    \n",
        "    # Sort probabilities\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=0)\n",
        "    \n",
        "    # Find cutoff\n",
        "    cutoff_idx = (cumulative_probs > p).nonzero()\n",
        "    if len(cutoff_idx) > 0:\n",
        "        cutoff_idx = cutoff_idx[0].item() + 1  # Include the token that crosses threshold\n",
        "    else:\n",
        "        cutoff_idx = len(probs)\n",
        "    \n",
        "    # Keep only tokens before cutoff\n",
        "    filtered_logits = torch.full_like(logits, float('-inf'))\n",
        "    filtered_logits.scatter_(0, sorted_indices[:cutoff_idx], logits[sorted_indices[:cutoff_idx]])\n",
        "    \n",
        "    return filtered_logits\n",
        "\n",
        "# Example\n",
        "logits = torch.tensor([3.0, 1.5, 0.5, -0.5, -1.0])\n",
        "\n",
        "print(\"Original probabilities:\")\n",
        "probs = F.softmax(logits, dim=0)\n",
        "for c, p in zip(chars_example, probs):\n",
        "    print(f\"  '{c}': {p.item():.2%}\")\n",
        "\n",
        "print(\"\\nAfter top-p=0.9 filtering:\")\n",
        "filtered = top_p_sampling(logits, p=0.9)\n",
        "probs = F.softmax(filtered, dim=0)\n",
        "for c, p in zip(chars_example, probs):\n",
        "    print(f\"  '{c}': {p.item():.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Complete Generation Function\n",
        "\n",
        "def generate(model, max_len=20, temperature=1.0, top_k=None, top_p=None):\n",
        "    \"\"\"\n",
        "    Generate a name with various sampling strategies.\n",
        "    \n",
        "    Args:\n",
        "        model: The trained language model\n",
        "        max_len: Maximum name length\n",
        "        temperature: Sampling temperature (higher = more random)\n",
        "        top_k: If set, only sample from top k tokens\n",
        "        top_p: If set, only sample from tokens with cumulative prob <= p\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    context = [char_to_idx['.']] * context_length\n",
        "    name = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            x = torch.tensor([context]).to(device)\n",
        "            logits = model(x)[0]  # Get logits for single sample\n",
        "            \n",
        "            # Apply temperature\n",
        "            logits = logits / temperature\n",
        "            \n",
        "            # Apply top-k if specified\n",
        "            if top_k is not None:\n",
        "                logits = top_k_sampling(logits, top_k)\n",
        "            \n",
        "            # Apply top-p if specified\n",
        "            if top_p is not None:\n",
        "                logits = top_p_sampling(logits, top_p)\n",
        "            \n",
        "            # Sample\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_idx = torch.multinomial(probs, 1).item()\n",
        "            next_char = idx_to_char[next_idx]\n",
        "            \n",
        "            if next_char == '.':\n",
        "                break\n",
        "            \n",
        "            name.append(next_char)\n",
        "            context = context[1:] + [next_idx]\n",
        "    \n",
        "    return ''.join(name).capitalize()\n",
        "\n",
        "# Test different settings\n",
        "print(\"Generation with different settings:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "settings = [\n",
        "    {'temperature': 0.5, 'top_k': None, 'top_p': None},\n",
        "    {'temperature': 1.0, 'top_k': 5, 'top_p': None},\n",
        "    {'temperature': 1.0, 'top_k': None, 'top_p': 0.9},\n",
        "    {'temperature': 1.2, 'top_k': 10, 'top_p': None},\n",
        "]\n",
        "\n",
        "for setting in settings:\n",
        "    print(f\"\\nSettings: {setting}\")\n",
        "    names = [generate(loaded_model, **setting) for _ in range(5)]\n",
        "    print(f\"  Generated: {names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2.1\n",
        "\n",
        "Generate 20 names with each setting:\n",
        "1. `temperature=0.5, top_k=5`\n",
        "2. `temperature=1.0, top_p=0.9`\n",
        "3. `temperature=1.5, top_k=10`\n",
        "\n",
        "Which produces the most realistic names? The most creative?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: Building a Web Interface with Gradio\n",
        "\n",
        "## What is Gradio?\n",
        "\n",
        "Gradio lets you create web interfaces for ML models with just a few lines of Python!\n",
        "\n",
        "```\n",
        "GRADIO WORKFLOW:\n",
        "\n",
        "┌─────────────┐     ┌─────────────┐     ┌─────────────┐\n",
        "│   Python    │     │   Gradio    │     │   Users     │\n",
        "│  Function   │ ──► │   Creates   │ ──► │  Access via │\n",
        "│             │     │   Web UI    │     │   Browser   │\n",
        "└─────────────┘     └─────────────┘     └─────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Simple Gradio Interface\n",
        "\n",
        "def generate_names_gradio(num_names, temperature, use_top_k, top_k_value):\n",
        "    \"\"\"\n",
        "    Generate names with user-specified settings.\n",
        "    Returns a formatted string of generated names.\n",
        "    \"\"\"\n",
        "    names = []\n",
        "    for _ in range(int(num_names)):\n",
        "        if use_top_k:\n",
        "            name = generate(loaded_model, temperature=temperature, top_k=int(top_k_value))\n",
        "        else:\n",
        "            name = generate(loaded_model, temperature=temperature)\n",
        "        names.append(name)\n",
        "    \n",
        "    return \"\\n\".join([f\"• {name}\" for name in names])\n",
        "\n",
        "# Test the function\n",
        "print(generate_names_gradio(5, 0.8, False, 5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Create Gradio Interface\n",
        "\n",
        "# Define the interface\n",
        "demo = gr.Interface(\n",
        "    fn=generate_names_gradio,\n",
        "    inputs=[\n",
        "        gr.Slider(1, 20, value=5, step=1, label=\"Number of Names\"),\n",
        "        gr.Slider(0.1, 2.0, value=0.8, step=0.1, label=\"Temperature\"),\n",
        "        gr.Checkbox(label=\"Use Top-K Sampling\", value=False),\n",
        "        gr.Slider(3, 20, value=5, step=1, label=\"Top-K Value\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Names\", lines=10),\n",
        "    title=\"Name Generator\",\n",
        "    description=\"Generate unique names using a character-level language model!\",\n",
        "    examples=[\n",
        "        [5, 0.5, False, 5],   # Conservative\n",
        "        [10, 1.0, True, 8],   # Balanced\n",
        "        [5, 1.5, False, 5],   # Creative\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch (will open in browser or inline)\n",
        "demo.launch(share=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced Interface with More Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Advanced Gradio Interface\n",
        "\n",
        "def generate_with_prefix(prefix, num_names, temperature, top_p):\n",
        "    \"\"\"\n",
        "    Generate names starting with a given prefix.\n",
        "    \"\"\"\n",
        "    prefix = prefix.lower().strip()\n",
        "    names = []\n",
        "    \n",
        "    for _ in range(int(num_names)):\n",
        "        # Start with the prefix\n",
        "        if len(prefix) >= context_length:\n",
        "            context = [char_to_idx.get(c, 0) for c in prefix[-context_length:]]\n",
        "        else:\n",
        "            padding = [char_to_idx['.']] * (context_length - len(prefix))\n",
        "            context = padding + [char_to_idx.get(c, 0) for c in prefix]\n",
        "        \n",
        "        name = list(prefix)\n",
        "        \n",
        "        loaded_model.eval()\n",
        "        with torch.no_grad():\n",
        "            for _ in range(20 - len(prefix)):\n",
        "                x = torch.tensor([context]).to(device)\n",
        "                logits = loaded_model(x)[0] / temperature\n",
        "                \n",
        "                if top_p < 1.0:\n",
        "                    logits = top_p_sampling(logits, top_p)\n",
        "                \n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                next_idx = torch.multinomial(probs, 1).item()\n",
        "                next_char = idx_to_char[next_idx]\n",
        "                \n",
        "                if next_char == '.':\n",
        "                    break\n",
        "                \n",
        "                name.append(next_char)\n",
        "                context = context[1:] + [next_idx]\n",
        "        \n",
        "        names.append(''.join(name).capitalize())\n",
        "    \n",
        "    return \"\\n\".join([f\"• {name}\" for name in names])\n",
        "\n",
        "# Test\n",
        "print(\"Names starting with 'Al':\")\n",
        "print(generate_with_prefix('al', 5, 0.8, 0.95))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Full Featured Interface\n",
        "\n",
        "with gr.Blocks(title=\"Name Generator Pro\") as demo_advanced:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Name Generator Pro\n",
        "        Generate unique names using a neural language model trained on 32,000+ names!\n",
        "        \"\"\"\n",
        "    )\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            prefix_input = gr.Textbox(\n",
        "                label=\"Starting Letters (optional)\",\n",
        "                placeholder=\"e.g., 'Al' or 'Jo'\",\n",
        "                value=\"\"\n",
        "            )\n",
        "            num_names = gr.Slider(1, 20, value=5, step=1, label=\"Number of Names\")\n",
        "            temperature = gr.Slider(0.3, 1.5, value=0.8, step=0.1, label=\"Creativity (Temperature)\")\n",
        "            top_p = gr.Slider(0.5, 1.0, value=0.95, step=0.05, label=\"Top-P Sampling\")\n",
        "            generate_btn = gr.Button(\"Generate Names\", variant=\"primary\")\n",
        "        \n",
        "        with gr.Column():\n",
        "            output = gr.Textbox(label=\"Generated Names\", lines=12)\n",
        "    \n",
        "    # Event handler\n",
        "    generate_btn.click(\n",
        "        fn=generate_with_prefix,\n",
        "        inputs=[prefix_input, num_names, temperature, top_p],\n",
        "        outputs=output\n",
        "    )\n",
        "    \n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        ### Tips:\n",
        "        - **Low temperature (0.3-0.5)**: More common, safe names\n",
        "        - **High temperature (1.0-1.5)**: More creative, unusual names\n",
        "        - **Top-P**: Lower values = more focused on likely characters\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "demo_advanced.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3.1\n",
        "\n",
        "Add a new feature to the Gradio interface: a \"style\" dropdown that lets users choose between:\n",
        "- \"Classic\" (temperature=0.5)\n",
        "- \"Balanced\" (temperature=0.8)\n",
        "- \"Creative\" (temperature=1.2)\n",
        "- \"Experimental\" (temperature=1.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Hint: Use gr.Dropdown with choices\n",
        "# style = gr.Dropdown(\n",
        "#     choices=[\"Classic\", \"Balanced\", \"Creative\", \"Experimental\"],\n",
        "#     value=\"Balanced\",\n",
        "#     label=\"Style\"\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: Deployment Options\n",
        "\n",
        "## Sharing Your App\n",
        "\n",
        "### Option 1: Gradio Share Link (Temporary)\n",
        "```python\n",
        "demo.launch(share=True)  # Creates temporary public URL\n",
        "```\n",
        "\n",
        "### Option 2: Hugging Face Spaces (Permanent, Free)\n",
        "1. Create account at huggingface.co\n",
        "2. Create new Space (Gradio SDK)\n",
        "3. Upload your code and model\n",
        "\n",
        "### Option 3: Self-hosted\n",
        "- Deploy on your own server\n",
        "- Use Docker for portability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Create files for Hugging Face Spaces\n",
        "\n",
        "# app.py content\n",
        "app_code = '''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import gradio as gr\n",
        "\n",
        "# Model definition\n",
        "class CharLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, context_length):\n",
        "        super().__init__()\n",
        "        self.context_length = context_length\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc1 = nn.Linear(context_length * embed_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        emb = emb.view(emb.size(0), -1)\n",
        "        h = torch.tanh(self.fc1(emb))\n",
        "        return self.fc2(h)\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(\"char_lm_weights.pt\", map_location=\"cpu\")\n",
        "model = CharLM(\n",
        "    vocab_size=checkpoint[\"vocab_size\"],\n",
        "    embed_dim=checkpoint[\"embed_dim\"],\n",
        "    hidden_dim=checkpoint[\"hidden_dim\"],\n",
        "    context_length=checkpoint[\"context_length\"]\n",
        ")\n",
        "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "chars = checkpoint[\"chars\"]\n",
        "char_to_idx = checkpoint[\"char_to_idx\"]\n",
        "idx_to_char = checkpoint[\"idx_to_char\"]\n",
        "context_length = checkpoint[\"context_length\"]\n",
        "\n",
        "def generate(num_names, temperature):\n",
        "    names = []\n",
        "    for _ in range(int(num_names)):\n",
        "        context = [char_to_idx[\".\"]] * context_length\n",
        "        name = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(20):\n",
        "                x = torch.tensor([context])\n",
        "                logits = model(x)[0] / temperature\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                next_idx = torch.multinomial(probs, 1).item()\n",
        "                if idx_to_char[next_idx] == \".\":\n",
        "                    break\n",
        "                name.append(idx_to_char[next_idx])\n",
        "                context = context[1:] + [next_idx]\n",
        "        names.append(\"\".join(name).capitalize())\n",
        "    return \"\\\\n\".join([f\"• {n}\" for n in names])\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=generate,\n",
        "    inputs=[\n",
        "        gr.Slider(1, 20, value=5, step=1, label=\"Number of Names\"),\n",
        "        gr.Slider(0.3, 1.5, value=0.8, step=0.1, label=\"Temperature\"),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Names\", lines=10),\n",
        "    title=\"Name Generator\",\n",
        "    description=\"Generate unique names using AI!\",\n",
        ")\n",
        "\n",
        "demo.launch()\n",
        "'''\n",
        "\n",
        "# requirements.txt content\n",
        "requirements = '''\n",
        "torch\n",
        "gradio\n",
        "'''\n",
        "\n",
        "# Save files\n",
        "with open('app.py', 'w') as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write(requirements)\n",
        "\n",
        "print(\"Created deployment files:\")\n",
        "print(\"  - app.py\")\n",
        "print(\"  - requirements.txt\")\n",
        "print(\"  - char_lm_weights.pt (already exists)\")\n",
        "print(\"\\nUpload these to Hugging Face Spaces to deploy!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Challenge Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 1: Batch Generation\n",
        "\n",
        "Modify the generation function to generate multiple names in parallel (batched), which is faster than generating one at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "def generate_batch(model, num_names, max_len=20, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Generate multiple names in parallel.\n",
        "    \"\"\"\n",
        "    # Hint: Process all names at once using batch dimension\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 2: Model Comparison UI\n",
        "\n",
        "Create a Gradio interface that compares outputs from two different models (e.g., bigram vs neural) side by side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 3: Add Filtering\n",
        "\n",
        "Add options to filter generated names by:\n",
        "- Minimum/maximum length\n",
        "- Must contain certain letters\n",
        "- Must not contain certain letters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "## What We Learned\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **Model Saving** | `torch.save(model.state_dict(), ...)` |\n",
        "| **Model Loading** | `model.load_state_dict(torch.load(...))` |\n",
        "| **Temperature** | Controls randomness in generation |\n",
        "| **Top-K Sampling** | Only sample from K most likely tokens |\n",
        "| **Top-P Sampling** | Sample from tokens with cumulative prob <= P |\n",
        "| **Gradio** | Quick way to build ML web interfaces |\n",
        "\n",
        "## Key Code Patterns\n",
        "\n",
        "```python\n",
        "# Save model\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'config': config,\n",
        "}, 'model.pt')\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load('model.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=my_function,\n",
        "    inputs=[...],\n",
        "    outputs=[...]\n",
        ")\n",
        "demo.launch()\n",
        "```\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "In **Lab 6**, we'll switch gears to **Computer Vision** and learn about **Object Detection** with YOLO!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
