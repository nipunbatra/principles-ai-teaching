{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Introduction to Machine Learning with scikit-learn\n",
    "\n",
    "**Principles of AI** Â· IIT Gandhinagar\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "In this lab, you will learn the fundamentals of machine learning using scikit-learn (sklearn), Python's most popular ML library.\n",
    "\n",
    "**By the end of this lab, you will be able to:**\n",
    "1. Load and explore datasets\n",
    "2. Understand the difference between features and labels\n",
    "3. Split data into training and testing sets\n",
    "4. Train your first classifier (Logistic Regression)\n",
    "5. Evaluate model performance using accuracy and confusion matrices\n",
    "\n",
    "**Duration**: 2-3 hours\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install and import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Understanding Data\n",
    "\n",
    "## 1.1 What is a Dataset?\n",
    "\n",
    "In machine learning, a **dataset** is a collection of examples that we use to train our models.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        DATASET                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚   Each ROW = One example (sample)                           â”‚\n",
    "â”‚   Each COLUMN = One feature (attribute)                     â”‚\n",
    "â”‚   One special column = The LABEL (what we predict)          â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Example**: If we're classifying flowers:\n",
    "- **Features**: petal length, petal width, sepal length, sepal width\n",
    "- **Label**: flower species (setosa, versicolor, virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Loading the Iris Dataset (Solved Example)\n",
    "\n",
    "The **Iris dataset** is one of the most famous datasets in ML. It contains measurements of 150 iris flowers from 3 different species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE: Loading and exploring the Iris dataset\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# What's inside?\n",
    "print(\"Keys in the dataset:\", iris.keys())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Feature names:\", iris.feature_names)\n",
    "print(\"Target names (classes):\", iris.target_names)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Shape of features (X):\", iris.data.shape)  # 150 samples, 4 features\n",
    "print(\"Shape of labels (y):\", iris.target.shape)   # 150 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame for easier viewing\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = [iris.target_names[i] for i in iris.target]\n",
    "\n",
    "print(\"First 10 rows of the dataset:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of each species?\n",
    "print(\"Class distribution:\")\n",
    "df['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Visualizing the Data\n",
    "\n",
    "Visualization helps us understand patterns in the data before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE: Scatter plot of two features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each species with a different color\n",
    "colors = {'setosa': 'red', 'versicolor': 'blue', 'virginica': 'green'}\n",
    "for species in iris.target_names:\n",
    "    subset = df[df['species'] == species]\n",
    "    plt.scatter(subset['sepal length (cm)'], \n",
    "                subset['petal length (cm)'],\n",
    "                c=colors[species], \n",
    "                label=species,\n",
    "                alpha=0.7,\n",
    "                s=50)\n",
    "\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Petal Length (cm)')\n",
    "plt.title('Iris Dataset: Sepal Length vs Petal Length')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ Notice how the species form distinct clusters!\")\n",
    "print(\"   This suggests a classifier should work well on this data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.1: Explore Feature Relationships\n",
    "\n",
    "Create a scatter plot of **petal length** vs **petal width**. Do the species still form distinct clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create a scatter plot similar to the solved example\n",
    "# but use 'petal length (cm)' and 'petal width (cm)'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Pair Plot\n",
    "\n",
    "Use seaborn's `pairplot` to visualize ALL pairwise relationships at once.\n",
    "\n",
    "**Hint**: `sns.pairplot(df, hue='species')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Preparing Data for Machine Learning\n",
    "\n",
    "## 2.1 Why Split Data?\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     THE EXAM ANALOGY                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                             â”‚\n",
    "â”‚   Training Data = Practice problems you study from          â”‚\n",
    "â”‚   Test Data = The actual exam (unseen problems)             â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚   If you memorize practice problems word-for-word,          â”‚\n",
    "â”‚   you might fail on new problems in the exam!               â”‚\n",
    "â”‚                                                             â”‚\n",
    "â”‚   Same with ML: We need UNSEEN data to test true learning.  â”‚\n",
    "â”‚                                                             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Insight**: We NEVER train on test data. The test set simulates \"real world\" data the model hasn't seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train-Test Split (Solved Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE: Splitting data into train and test sets\n",
    "\n",
    "# Get features (X) and labels (y)\n",
    "X = iris.data  # All 4 features\n",
    "y = iris.target  # The species labels (0, 1, or 2)\n",
    "\n",
    "print(\"Before split:\")\n",
    "print(f\"  Total samples: {len(X)}\")\n",
    "\n",
    "# Split: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42,    # For reproducibility\n",
    "    stratify=y          # Keep class proportions same in both sets\n",
    ")\n",
    "\n",
    "print(\"\\nAfter split:\")\n",
    "print(f\"  Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.0f}%)\")\n",
    "print(f\"  Testing samples: {len(X_test)} ({len(X_test)/len(X)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify stratification worked (same class proportions)\n",
    "print(\"Class distribution in training set:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u} ({iris.target_names[u]}): {c} samples\")\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  Class {u} ({iris.target_names[u]}): {c} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1: Different Split Ratios\n",
    "\n",
    "Try splitting with 70-30 ratio instead of 80-20. How many samples are in each set now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Use test_size=0.3 for 70-30 split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feature Scaling\n",
    "\n",
    "**Why scale features?**\n",
    "\n",
    "Some algorithms (like Logistic Regression, KNN) are sensitive to the scale of features.\n",
    "\n",
    "```\n",
    "Example without scaling:\n",
    "  - Feature 1: House size in sq ft (range: 500 - 5000)\n",
    "  - Feature 2: Number of bedrooms (range: 1 - 6)\n",
    "  \n",
    "  The algorithm might think Feature 1 is more important \n",
    "  just because it has bigger numbers!\n",
    "```\n",
    "\n",
    "**StandardScaler** transforms features to have:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE: Feature scaling\n",
    "\n",
    "# Before scaling\n",
    "print(\"Before scaling:\")\n",
    "print(f\"  Feature means: {X_train.mean(axis=0).round(2)}\")\n",
    "print(f\"  Feature stds:  {X_train.std(axis=0).round(2)}\")\n",
    "\n",
    "# Create and fit the scaler on TRAINING data only\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # fit + transform\n",
    "X_test_scaled = scaler.transform(X_test)        # only transform (use training stats)\n",
    "\n",
    "print(\"\\nAfter scaling:\")\n",
    "print(f\"  Feature means: {X_train_scaled.mean(axis=0).round(2)}\")\n",
    "print(f\"  Feature stds:  {X_train_scaled.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: We fit the scaler on training data only! We use the SAME transformation parameters for test data. This prevents \"data leakage\" - using information from the test set during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Training Your First Classifier\n",
    "\n",
    "## 3.1 What is Classification?\n",
    "\n",
    "```\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "   Input            â”‚   Classifier    â”‚         Output\n",
    "   Features    â”€â”€â”€â–º â”‚   (Model)       â”‚  â”€â”€â”€â–º   Class Label\n",
    "   [5.1, 3.5,       â”‚                 â”‚         \"setosa\"\n",
    "    1.4, 0.2]       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "The classifier learns patterns in training data, then predicts classes for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Logistic Regression (Solved Example)\n",
    "\n",
    "Despite its name, **Logistic Regression** is used for **classification**, not regression.\n",
    "\n",
    "It's one of the simplest and most interpretable classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE: Training a Logistic Regression classifier\n",
    "\n",
    "# Step 1: Create the model\n",
    "model = LogisticRegression(max_iter=200)  # max_iter ensures convergence\n",
    "\n",
    "# Step 2: Train the model on training data\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"\\nModel type: {type(model).__name__}\")\n",
    "print(f\"Number of classes: {len(model.classes_)}\")\n",
    "print(f\"Classes: {model.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Make predictions on test data\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Compare predictions with actual labels\n",
    "print(\"First 10 predictions vs actual:\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(10):\n",
    "    pred_name = iris.target_names[y_pred[i]]\n",
    "    actual_name = iris.target_names[y_test[i]]\n",
    "    match = \"âœ“\" if y_pred[i] == y_test[i] else \"âœ—\"\n",
    "    print(f\"  Predicted: {pred_name:12} | Actual: {actual_name:12} {match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression also gives probabilities!\n",
    "y_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "print(\"Probabilities for first 5 samples:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Sample':<8} {'P(setosa)':<12} {'P(versicolor)':<14} {'P(virginica)':<12}\")\n",
    "print(\"-\" * 60)\n",
    "for i in range(5):\n",
    "    print(f\"{i:<8} {y_proba[i][0]:<12.3f} {y_proba[i][1]:<14.3f} {y_proba[i][2]:<12.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.1: Train a K-Nearest Neighbors Classifier\n",
    "\n",
    "KNN is another simple classifier. It predicts based on the majority class of the K nearest training examples.\n",
    "\n",
    "Train a KNN classifier with `n_neighbors=5` and make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Create KNeighborsClassifier with n_neighbors=5\n",
    "# 2. Fit on X_train_scaled and y_train\n",
    "# 3. Predict on X_test_scaled\n",
    "# 4. Print first 10 predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Evaluating Your Model\n",
    "\n",
    "## 4.1 Accuracy\n",
    "\n",
    "**Accuracy** = (Correct predictions) / (Total predictions)\n",
    "\n",
    "It's the simplest metric, but not always the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE: Calculating accuracy\n",
    "\n",
    "# Using sklearn's accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Or calculate manually\n",
    "correct = (y_test == y_pred).sum()\n",
    "total = len(y_test)\n",
    "print(f\"\\nManual calculation: {correct}/{total} = {correct/total:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Confusion Matrix\n",
    "\n",
    "A confusion matrix shows WHERE the model makes mistakes.\n",
    "\n",
    "```\n",
    "                    Predicted\n",
    "               Setosa  Versi.  Virgin.\n",
    "Actual Setosa    10      0       0\n",
    "       Versi.     0      9       1\n",
    "       Virgin.    0      0      10\n",
    "       \n",
    "Diagonal = Correct predictions\n",
    "Off-diagonal = Mistakes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLVED EXAMPLE: Confusion matrix visualization\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot it\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.show()\n",
    "\n",
    "# Interpret the results\n",
    "print(\"\\nğŸ“Š Interpretation:\")\n",
    "print(\"   Diagonal values = Correct predictions\")\n",
    "print(\"   Off-diagonal = Errors (confusion between classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1: Compare Models\n",
    "\n",
    "Calculate the accuracy and create a confusion matrix for your KNN classifier from Question 3.1.\n",
    "\n",
    "Which model performs better: Logistic Regression or KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Calculate accuracy for KNN predictions\n",
    "# 2. Create and plot confusion matrix\n",
    "# 3. Compare with Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 When Accuracy Lies\n",
    "\n",
    "**Warning**: Accuracy can be misleading for imbalanced datasets!\n",
    "\n",
    "```\n",
    "Example: Disease detection with 99% healthy, 1% sick\n",
    "\n",
    "\"Dumb\" model: Always predict \"healthy\"\n",
    "Accuracy = 99% (Looks great!)\n",
    "\n",
    "But it NEVER detects sick patients! That's dangerous.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Accuracy on imbalanced data\n",
    "\n",
    "# Create fake imbalanced data\n",
    "y_imbalanced = np.array([0]*99 + [1]*1)  # 99 class-0, 1 class-1\n",
    "y_dumb_pred = np.zeros(100)  # Always predict class 0\n",
    "\n",
    "print(\"Imbalanced dataset example:\")\n",
    "print(f\"  Class 0: 99 samples\")\n",
    "print(f\"  Class 1: 1 sample\")\n",
    "print(f\"\\n'Always predict 0' accuracy: {accuracy_score(y_imbalanced, y_dumb_pred):.0%}\")\n",
    "print(\"\\nâš ï¸ The model is useless but has 99% accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Working with the Digits Dataset\n",
    "\n",
    "Now let's try a more challenging dataset: handwritten digits (similar to MNIST)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "print(\"Digits dataset info:\")\n",
    "print(f\"  Number of samples: {len(digits.data)}\")\n",
    "print(f\"  Number of features: {digits.data.shape[1]}\")\n",
    "print(f\"  Number of classes: {len(digits.target_names)}\")\n",
    "print(f\"  Classes: {digits.target_names}\")\n",
    "print(f\"\\nEach digit is an 8x8 pixel image (64 features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(digits.images[i], cmap='gray')\n",
    "    ax.set_title(f\"Label: {digits.target[i]}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Sample Handwritten Digits', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1: Complete Pipeline for Digits\n",
    "\n",
    "Build a complete ML pipeline for the digits dataset:\n",
    "1. Split into train/test (80/20)\n",
    "2. Scale the features\n",
    "3. Train a Logistic Regression classifier\n",
    "4. Calculate accuracy\n",
    "5. Create confusion matrix\n",
    "\n",
    "What accuracy do you achieve? Which digits are most often confused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Complete the pipeline step by step\n",
    "\n",
    "# Step 1: Get X and y\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# Step 2: Train-test split\n",
    "\n",
    "\n",
    "# Step 3: Scale features\n",
    "\n",
    "\n",
    "# Step 4: Train model\n",
    "\n",
    "\n",
    "# Step 5: Predict and evaluate\n",
    "\n",
    "\n",
    "# Step 6: Confusion matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2: Visualize Mistakes\n",
    "\n",
    "Find samples where the model made mistakes and visualize them. Can you understand why the model was confused?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Hint: Find indices where y_pred != y_test\n",
    "# Then visualize those digits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Challenge Problems\n",
    "\n",
    "These are optional exercises for those who finish early or want extra practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Hyperparameter Tuning\n",
    "\n",
    "For KNN, try different values of `n_neighbors` (1, 3, 5, 7, 9, 11) on the Iris dataset.\n",
    "Which value gives the best accuracy? Plot accuracy vs n_neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Cross-Validation\n",
    "\n",
    "Instead of a single train-test split, use 5-fold cross-validation.\n",
    "\n",
    "**Hint**: Use `from sklearn.model_selection import cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Load Your Own Dataset\n",
    "\n",
    "Download a CSV dataset from Kaggle or elsewhere and build a classifier for it.\n",
    "\n",
    "Suggestions:\n",
    "- Titanic survival prediction\n",
    "- Heart disease prediction\n",
    "- Mushroom classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "1. **Datasets** have features (X) and labels (y)\n",
    "2. **Train-test split** prevents overfitting and tests generalization\n",
    "3. **Feature scaling** helps many algorithms perform better\n",
    "4. **Logistic Regression** and **KNN** are simple but effective classifiers\n",
    "5. **Accuracy** measures overall correctness but can be misleading\n",
    "6. **Confusion matrices** show which classes are confused\n",
    "\n",
    "## Key Code Summary\n",
    "\n",
    "```python\n",
    "# Load data\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2%}\")\n",
    "```\n",
    "\n",
    "## Next Lab\n",
    "\n",
    "**Lab 2: Decision Trees, Random Forests, Regression & Clustering**\n",
    "\n",
    "We'll explore more algorithms that handle different types of problems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
