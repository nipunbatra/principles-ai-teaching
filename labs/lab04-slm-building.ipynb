{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 4: Building a Small Language Model (SLM-1)\n",
        "## Next-Token Prediction from Scratch\n",
        "\n",
        "**Duration**: ~3 hours\n",
        "\n",
        "### Learning Objectives\n",
        "By the end of this lab, you will be able to:\n",
        "1. Understand character-level language modeling\n",
        "2. Build a bigram model (simplest language model)\n",
        "3. Create embeddings from scratch\n",
        "4. Train a neural language model\n",
        "5. Generate new text!\n",
        "\n",
        "### Prerequisites\n",
        "- Completed Lab 3 (PyTorch basics)\n",
        "\n",
        "### The Big Idea\n",
        "\n",
        "**Language models predict the next token given previous tokens.**\n",
        "\n",
        "```\n",
        "Input: \"The cat sat on the\"\n",
        "Model predicts: \"mat\" (or \"floor\", \"chair\", etc.)\n",
        "```\n",
        "\n",
        "This is **exactly** how ChatGPT, Claude, and other LLMs work - just at massive scale!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: The Autocomplete Game\n",
        "\n",
        "## Your Phone Already Does This!\n",
        "\n",
        "Every time you type on your phone, it suggests the next word:\n",
        "\n",
        "```\n",
        "You type: \"I'll be there in\"\n",
        "Suggestions: [5 minutes] [a bit] [an hour]\n",
        "```\n",
        "\n",
        "**This is next-token prediction!**\n",
        "\n",
        "Let's start even simpler: predict the next **character** instead of word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Interactive Prediction Game\n",
        "\n",
        "# What comes after these?\n",
        "test_cases = [\n",
        "    \"Harr\",      # Harry? Harrison?\n",
        "    \"Michae\",    # Michael!\n",
        "    \"qu\",        # Probably 'e' or 'i'\n",
        "    \"th\",        # 'e' is very common\n",
        "]\n",
        "\n",
        "print(\"Guess the next character!\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "for text in test_cases:\n",
        "    print(f\"  '{text}' + ? = ???\")\n",
        "\n",
        "print(\"\\n(We'll train a model to do this automatically!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Our Dataset: Names\n",
        "\n",
        "We'll use a dataset of ~32,000 names (inspired by Andrej Karpathy's makemore).\n",
        "\n",
        "Why names?\n",
        "- Short and simple\n",
        "- Clear patterns (names follow conventions)\n",
        "- Fun to generate new names!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the names dataset\n",
        "import urllib.request\n",
        "import os\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\n",
        "filename = \"names.txt\"\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    urllib.request.urlretrieve(url, filename)\n",
        "    print(f\"Downloaded {filename}\")\n",
        "else:\n",
        "    print(f\"{filename} already exists\")\n",
        "\n",
        "# Load names\n",
        "with open(filename, 'r') as f:\n",
        "    names = f.read().splitlines()\n",
        "\n",
        "print(f\"\\nLoaded {len(names)} names\")\n",
        "print(f\"\\nFirst 10 names: {names[:10]}\")\n",
        "print(f\"Random names: {np.random.choice(names, 5).tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Explore the Dataset\n",
        "\n",
        "# Length distribution\n",
        "lengths = [len(name) for name in names]\n",
        "\n",
        "print(f\"Shortest name: {min(lengths)} characters\")\n",
        "print(f\"Longest name: {max(lengths)} characters\")\n",
        "print(f\"Average length: {np.mean(lengths):.1f} characters\")\n",
        "\n",
        "# Character frequency\n",
        "all_chars = ''.join(names).lower()\n",
        "char_counts = {}\n",
        "for c in all_chars:\n",
        "    char_counts[c] = char_counts.get(c, 0) + 1\n",
        "\n",
        "# Sort by frequency\n",
        "sorted_chars = sorted(char_counts.items(), key=lambda x: -x[1])\n",
        "\n",
        "print(f\"\\nMost common characters:\")\n",
        "for char, count in sorted_chars[:10]:\n",
        "    print(f\"  '{char}': {count} ({count/len(all_chars)*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Build Vocabulary\n",
        "\n",
        "# Get all unique characters\n",
        "chars = sorted(list(set(''.join(names).lower())))\n",
        "\n",
        "# Add special tokens\n",
        "# '.' represents start/end of name\n",
        "chars = ['.'] + chars\n",
        "\n",
        "vocab_size = len(chars)\n",
        "print(f\"Vocabulary: {chars}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Create character <-> index mappings\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "print(f\"\\nExamples:\")\n",
        "print(f\"  'a' -> {char_to_idx['a']}\")\n",
        "print(f\"  'z' -> {char_to_idx['z']}\")\n",
        "print(f\"  '.' -> {char_to_idx['.']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: The Bigram Model\n",
        "\n",
        "## Simplest Language Model Ever\n",
        "\n",
        "**Bigram**: Look at ONE character, predict the NEXT character.\n",
        "\n",
        "```\n",
        "BIGRAM MODEL:\n",
        "\n",
        "    Previous      →    Next\n",
        "    Character          Character\n",
        "    \n",
        "       'a'        →    'n' (30%)\n",
        "                  →    'l' (15%)\n",
        "                  →    'r' (12%)\n",
        "                  →    ...\n",
        "```\n",
        "\n",
        "We just count how often each pair of characters appears!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Build Bigram Counts\n",
        "\n",
        "# Count all character pairs\n",
        "bigram_counts = torch.zeros((vocab_size, vocab_size), dtype=torch.int32)\n",
        "\n",
        "for name in names:\n",
        "    # Add start/end tokens\n",
        "    name = '.' + name.lower() + '.'\n",
        "    \n",
        "    for ch1, ch2 in zip(name, name[1:]):\n",
        "        idx1 = char_to_idx[ch1]\n",
        "        idx2 = char_to_idx[ch2]\n",
        "        bigram_counts[idx1, idx2] += 1\n",
        "\n",
        "print(f\"Bigram counts shape: {bigram_counts.shape}\")\n",
        "print(f\"Total bigrams counted: {bigram_counts.sum().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Visualize Bigram Counts\n",
        "\n",
        "plt.figure(figsize=(16, 16))\n",
        "plt.imshow(bigram_counts, cmap='Blues')\n",
        "\n",
        "# Add labels\n",
        "for i in range(vocab_size):\n",
        "    for j in range(vocab_size):\n",
        "        count = bigram_counts[i, j].item()\n",
        "        if count > 0:\n",
        "            plt.text(j, i, chars[i] + chars[j], ha='center', va='bottom', fontsize=6)\n",
        "            plt.text(j, i, str(count), ha='center', va='top', fontsize=6, color='gray')\n",
        "\n",
        "plt.xlabel('Second Character')\n",
        "plt.ylabel('First Character')\n",
        "plt.xticks(range(vocab_size), chars)\n",
        "plt.yticks(range(vocab_size), chars)\n",
        "plt.title('Bigram Counts (First → Second Character)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Convert Counts to Probabilities\n",
        "\n",
        "# Normalize rows to get probabilities\n",
        "# P(next | current) = count(current, next) / count(current, *)\n",
        "\n",
        "# Add smoothing to avoid division by zero\n",
        "bigram_probs = (bigram_counts + 1).float()  # Add 1 (Laplace smoothing)\n",
        "bigram_probs = bigram_probs / bigram_probs.sum(dim=1, keepdim=True)\n",
        "\n",
        "print(\"Probability of next character after 'a':\")\n",
        "probs_after_a = bigram_probs[char_to_idx['a']]\n",
        "top_k = probs_after_a.topk(5)\n",
        "\n",
        "for prob, idx in zip(top_k.values, top_k.indices):\n",
        "    print(f\"  'a' → '{chars[idx]}': {prob.item():.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Generate Names with Bigram Model\n",
        "\n",
        "def generate_bigram(max_len=20):\n",
        "    \"\"\"Generate a name using the bigram model.\"\"\"\n",
        "    name = []\n",
        "    current_char = '.'  # Start token\n",
        "    \n",
        "    for _ in range(max_len):\n",
        "        # Get probability distribution\n",
        "        probs = bigram_probs[char_to_idx[current_char]]\n",
        "        \n",
        "        # Sample next character\n",
        "        next_idx = torch.multinomial(probs, 1).item()\n",
        "        next_char = chars[next_idx]\n",
        "        \n",
        "        # Check for end token\n",
        "        if next_char == '.':\n",
        "            break\n",
        "            \n",
        "        name.append(next_char)\n",
        "        current_char = next_char\n",
        "    \n",
        "    return ''.join(name).capitalize()\n",
        "\n",
        "# Generate some names\n",
        "print(\"Names generated by bigram model:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "for i in range(20):\n",
        "    name = generate_bigram()\n",
        "    print(f\"  {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bigram Model: Observations\n",
        "\n",
        "**Pros:**\n",
        "- Simple and fast\n",
        "- Captures basic character patterns\n",
        "- Some names look reasonable!\n",
        "\n",
        "**Cons:**\n",
        "- No memory beyond 1 character\n",
        "- Can't learn longer patterns\n",
        "- Many names are gibberish\n",
        "\n",
        "**Let's use a neural network to do better!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1.1\n",
        "\n",
        "What character is most likely to:\n",
        "1. Start a name (come after '.')?\n",
        "2. End a name (come before '.')?\n",
        "\n",
        "Find these using the bigram probability matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Most likely first character\n",
        "# Hint: Look at row for '.' (start token)\n",
        "\n",
        "# Most likely last character\n",
        "# Hint: Look at column for '.' (end token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: Embeddings\n",
        "\n",
        "## Why Embeddings?\n",
        "\n",
        "**Problem:** How do we represent characters as numbers for a neural network?\n",
        "\n",
        "**Bad idea: One-hot encoding**\n",
        "```\n",
        "'a' → [1, 0, 0, 0, ... 0]  (27 dimensions!)\n",
        "'b' → [0, 1, 0, 0, ... 0]\n",
        "'c' → [0, 0, 1, 0, ... 0]\n",
        "```\n",
        "\n",
        "Problems:\n",
        "- Very sparse (mostly zeros)\n",
        "- All characters equidistant (no similarity)\n",
        "\n",
        "**Better idea: Learned embeddings**\n",
        "```\n",
        "'a' → [0.2, -0.5, 0.1, ...]  (dense, learned)\n",
        "'b' → [0.3, -0.4, 0.2, ...]\n",
        "'c' → [0.25, -0.45, 0.15, ...]  (similar to 'a' and 'b'!)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Creating Embeddings\n",
        "\n",
        "# Embedding layer: maps indices to vectors\n",
        "embed_dim = 2  # Start with 2D for visualization\n",
        "embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "print(f\"Embedding table shape: {embedding.weight.shape}\")\n",
        "print(f\"  {vocab_size} characters × {embed_dim} dimensions\")\n",
        "\n",
        "# Look up some characters\n",
        "idx = torch.tensor([char_to_idx['a'], char_to_idx['b'], char_to_idx['c']])\n",
        "vectors = embedding(idx)\n",
        "\n",
        "print(f\"\\nEmbedding for 'a', 'b', 'c':\")\n",
        "print(vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Visualize 2D Embeddings\n",
        "\n",
        "# Get all embeddings\n",
        "all_idx = torch.arange(vocab_size)\n",
        "all_embeddings = embedding(all_idx).detach().numpy()\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(all_embeddings[:, 0], all_embeddings[:, 1], s=100)\n",
        "\n",
        "# Add labels\n",
        "for i, char in enumerate(chars):\n",
        "    plt.annotate(char, (all_embeddings[i, 0], all_embeddings[i, 1]), \n",
        "                 fontsize=12, ha='center', va='center')\n",
        "\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('Character Embeddings (Random Initialization)\\n(Will learn meaningful positions during training!)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: Neural Language Model\n",
        "\n",
        "## The Architecture\n",
        "\n",
        "```\n",
        "NEURAL LANGUAGE MODEL:\n",
        "\n",
        "Input: \"emm\" (predict 'a')\n",
        "\n",
        "  'e'  'm'  'm'\n",
        "   │    │    │\n",
        "   ▼    ▼    ▼\n",
        "┌─────────────────┐\n",
        "│   Embeddings    │  (lookup vectors)\n",
        "└─────────────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│    Concat       │  (combine vectors)\n",
        "└─────────────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│   Hidden MLP    │  (learn patterns)\n",
        "└─────────────────┘\n",
        "         │\n",
        "         ▼\n",
        "┌─────────────────┐\n",
        "│    Output       │  (probability for each char)\n",
        "└─────────────────┘\n",
        "         │\n",
        "         ▼\n",
        "    P(a) = 0.85\n",
        "    P(b) = 0.02\n",
        "    ...\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Prepare Training Data\n",
        "\n",
        "# Context length: how many previous characters to look at\n",
        "context_length = 3\n",
        "\n",
        "def build_dataset(names, context_length):\n",
        "    \"\"\"Build training data: (context) -> (next_char)\"\"\"\n",
        "    X, Y = [], []\n",
        "    \n",
        "    for name in names:\n",
        "        # Add padding and end token\n",
        "        name = '.' * context_length + name.lower() + '.'\n",
        "        \n",
        "        for i in range(len(name) - context_length):\n",
        "            context = name[i:i+context_length]\n",
        "            target = name[i+context_length]\n",
        "            \n",
        "            # Convert to indices\n",
        "            context_idx = [char_to_idx[c] for c in context]\n",
        "            target_idx = char_to_idx[target]\n",
        "            \n",
        "            X.append(context_idx)\n",
        "            Y.append(target_idx)\n",
        "    \n",
        "    return torch.tensor(X), torch.tensor(Y)\n",
        "\n",
        "# Build dataset\n",
        "X, Y = build_dataset(names, context_length)\n",
        "\n",
        "print(f\"Dataset size: {len(X)} examples\")\n",
        "print(f\"X shape: {X.shape}\")  # (N, context_length)\n",
        "print(f\"Y shape: {Y.shape}\")  # (N,)\n",
        "\n",
        "# Show some examples\n",
        "print(f\"\\nExamples:\")\n",
        "for i in range(5):\n",
        "    context = ''.join([chars[idx] for idx in X[i].tolist()])\n",
        "    target = chars[Y[i].item()]\n",
        "    print(f\"  '{context}' → '{target}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Train/Val/Test Split\n",
        "\n",
        "# Shuffle\n",
        "torch.manual_seed(42)\n",
        "perm = torch.randperm(len(X))\n",
        "X, Y = X[perm], Y[perm]\n",
        "\n",
        "# Split: 80% train, 10% val, 10% test\n",
        "n1 = int(0.8 * len(X))\n",
        "n2 = int(0.9 * len(X))\n",
        "\n",
        "X_train, Y_train = X[:n1], Y[:n1]\n",
        "X_val, Y_val = X[n1:n2], Y[n1:n2]\n",
        "X_test, Y_test = X[n2:], Y[n2:]\n",
        "\n",
        "print(f\"Training: {len(X_train)} examples\")\n",
        "print(f\"Validation: {len(X_val)} examples\")\n",
        "print(f\"Test: {len(X_test)} examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Define the Model\n",
        "\n",
        "class CharLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, context_length):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.context_length = context_length\n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        \n",
        "        # MLP layers\n",
        "        self.fc1 = nn.Linear(context_length * embed_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, context_length)\n",
        "        \n",
        "        # Get embeddings\n",
        "        emb = self.embedding(x)  # (batch, context_length, embed_dim)\n",
        "        \n",
        "        # Flatten embeddings\n",
        "        emb = emb.view(emb.size(0), -1)  # (batch, context_length * embed_dim)\n",
        "        \n",
        "        # MLP\n",
        "        h = torch.tanh(self.fc1(emb))\n",
        "        logits = self.fc2(h)\n",
        "        \n",
        "        return logits\n",
        "\n",
        "# Create model\n",
        "embed_dim = 10\n",
        "hidden_dim = 100\n",
        "\n",
        "model = CharLM(vocab_size, embed_dim, hidden_dim, context_length).to(device)\n",
        "print(model)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nTotal parameters: {n_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Training Loop\n",
        "\n",
        "# Move data to device\n",
        "X_train = X_train.to(device)\n",
        "Y_train = Y_train.to(device)\n",
        "X_val = X_val.to(device)\n",
        "Y_val = Y_val.to(device)\n",
        "\n",
        "# Training settings\n",
        "batch_size = 32\n",
        "learning_rate = 0.01\n",
        "num_epochs = 20\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "print(\"Training...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    \n",
        "    # Mini-batch training\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        # Get batch\n",
        "        X_batch = X_train[i:i+batch_size]\n",
        "        Y_batch = Y_train[i:i+batch_size]\n",
        "        \n",
        "        # Forward pass\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, Y_batch)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "    \n",
        "    avg_train_loss = total_loss / num_batches\n",
        "    train_losses.append(avg_train_loss)\n",
        "    \n",
        "    # Validation loss\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_logits = model(X_val)\n",
        "        val_loss = criterion(val_logits, Y_val).item()\n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1:3d}: Train Loss = {avg_train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Plot Training Curves\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, 'o-', label='Train')\n",
        "plt.plot(val_losses, 's-', label='Validation')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Progress')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Generate Names with Neural Model\n",
        "\n",
        "def generate_neural(model, max_len=20, temperature=1.0):\n",
        "    \"\"\"Generate a name using the neural language model.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    # Start with padding\n",
        "    context = [char_to_idx['.']] * context_length\n",
        "    name = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            # Prepare input\n",
        "            x = torch.tensor([context]).to(device)\n",
        "            \n",
        "            # Get prediction\n",
        "            logits = model(x)\n",
        "            \n",
        "            # Apply temperature\n",
        "            probs = F.softmax(logits / temperature, dim=-1)\n",
        "            \n",
        "            # Sample\n",
        "            next_idx = torch.multinomial(probs, 1).item()\n",
        "            next_char = chars[next_idx]\n",
        "            \n",
        "            # Check for end\n",
        "            if next_char == '.':\n",
        "                break\n",
        "            \n",
        "            name.append(next_char)\n",
        "            \n",
        "            # Update context (sliding window)\n",
        "            context = context[1:] + [next_idx]\n",
        "    \n",
        "    return ''.join(name).capitalize()\n",
        "\n",
        "# Generate names\n",
        "print(\"Names generated by neural model:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "for i in range(20):\n",
        "    name = generate_neural(model, temperature=0.8)\n",
        "    print(f\"  {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Temperature Comparison\n",
        "\n",
        "print(\"Effect of Temperature on Generation:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for temp in [0.5, 0.8, 1.0, 1.5]:\n",
        "    print(f\"\\nTemperature = {temp}:\")\n",
        "    names = [generate_neural(model, temperature=temp) for _ in range(5)]\n",
        "    for name in names:\n",
        "        print(f\"  {name}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Low temp (0.5): More conservative, common names\")\n",
        "print(\"High temp (1.5): More creative/random\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2.1\n",
        "\n",
        "Compare the bigram model vs neural model:\n",
        "1. Generate 50 names from each\n",
        "2. Count how many \"look like real names\" (subjective, just guess)\n",
        "3. Which model produces better results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Generate 50 names from bigram model\n",
        "bigram_names = [generate_bigram() for _ in range(50)]\n",
        "\n",
        "# Generate 50 names from neural model\n",
        "neural_names = [generate_neural(model, temperature=0.8) for _ in range(50)]\n",
        "\n",
        "# Print and compare\n",
        "print(\"Bigram names:\")\n",
        "print(bigram_names[:20])\n",
        "\n",
        "print(\"\\nNeural names:\")\n",
        "print(neural_names[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 5: Visualizing Learned Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Visualize Learned Embeddings\n",
        "\n",
        "# Get trained embeddings\n",
        "embeddings = model.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "# Use PCA to reduce to 2D (if embed_dim > 2)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "if embeddings.shape[1] > 2:\n",
        "    pca = PCA(n_components=2)\n",
        "    embeddings_2d = pca.fit_transform(embeddings)\n",
        "else:\n",
        "    embeddings_2d = embeddings\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Color vowels differently\n",
        "vowels = set('aeiou')\n",
        "colors = ['red' if c in vowels else 'blue' for c in chars]\n",
        "\n",
        "for i, char in enumerate(chars):\n",
        "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], \n",
        "                c=colors[i], s=200, alpha=0.7)\n",
        "    plt.annotate(char, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
        "                 fontsize=14, ha='center', va='center', fontweight='bold')\n",
        "\n",
        "plt.xlabel('Dimension 1')\n",
        "plt.ylabel('Dimension 2')\n",
        "plt.title('Learned Character Embeddings\\n(Red = vowels, Blue = consonants)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nNotice: Similar characters should be closer together!\")\n",
        "print(\"(Vowels may cluster, common consonants may cluster, etc.)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Challenge Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 1: Longer Context\n",
        "\n",
        "Increase the context length from 3 to 5 or 8 characters.\n",
        "Does the model generate better names?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 2: Deeper Network\n",
        "\n",
        "Add more hidden layers to the network.\n",
        "Does it improve the loss?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 3: Train on Different Text\n",
        "\n",
        "Try training on a different dataset, like:\n",
        "- Pokemon names\n",
        "- City names\n",
        "- Made-up words\n",
        "\n",
        "Just create a text file with one item per line!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "## What We Learned\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **Language Model** | Predicts next token given previous tokens |\n",
        "| **Bigram Model** | Simplest LM: only looks at 1 previous character |\n",
        "| **Embeddings** | Learned vector representations for characters |\n",
        "| **Neural LM** | Uses MLP to learn complex patterns |\n",
        "| **Temperature** | Controls randomness in generation |\n",
        "\n",
        "## Key Code Patterns\n",
        "\n",
        "```python\n",
        "# Embedding layer\n",
        "embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "# Neural LM architecture\n",
        "class CharLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, context_length):\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.fc1 = nn.Linear(context_length * embed_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x).flatten(1)\n",
        "        h = torch.tanh(self.fc1(emb))\n",
        "        return self.fc2(h)\n",
        "\n",
        "# Sampling with temperature\n",
        "probs = F.softmax(logits / temperature, dim=-1)\n",
        "next_idx = torch.multinomial(probs, 1)\n",
        "```\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "In **Lab 5**, we'll:\n",
        "- Save our trained model\n",
        "- Build a simple web interface with Gradio\n",
        "- Deploy our name generator!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
