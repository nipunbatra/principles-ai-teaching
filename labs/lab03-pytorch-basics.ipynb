{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 3: PyTorch Basics & Training a Neural Network\n",
        "\n",
        "**Duration**: ~3 hours\n",
        "\n",
        "### Learning Objectives\n",
        "By the end of this lab, you will be able to:\n",
        "1. Understand tensors (PyTorch's building blocks)\n",
        "2. Build a simple neural network from scratch\n",
        "3. Understand the training loop: forward pass, loss, backpropagation\n",
        "4. Train a classifier on MNIST digits\n",
        "\n",
        "### Prerequisites\n",
        "- Completed Labs 1-2\n",
        "- Basic Python knowledge\n",
        "\n",
        "### Why PyTorch?\n",
        "\n",
        "sklearn is great for classical ML, but for **deep learning** we need:\n",
        "- Automatic differentiation (computing gradients)\n",
        "- GPU acceleration\n",
        "- Flexible neural network building\n",
        "\n",
        "PyTorch gives us all this with Pythonic syntax!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch (run once)\n",
        "# For CPU: !pip install torch torchvision\n",
        "# For GPU (Colab): Already installed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check PyTorch version and device\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 1: Tensors - The Building Blocks\n",
        "\n",
        "## What is a Tensor?\n",
        "\n",
        "A tensor is just a **fancy array**. That's it!\n",
        "\n",
        "```\n",
        "TENSOR DIMENSIONS:\n",
        "\n",
        "Scalar (0D):     5\n",
        "                 └── Just a number\n",
        "\n",
        "Vector (1D):     [1, 2, 3, 4, 5]\n",
        "                 └── List of numbers\n",
        "\n",
        "Matrix (2D):     [[1, 2, 3],\n",
        "                  [4, 5, 6]]\n",
        "                 └── Table of numbers\n",
        "\n",
        "3D Tensor:       [[[1, 2], [3, 4]],\n",
        "                  [[5, 6], [7, 8]]]\n",
        "                 └── Cube of numbers (e.g., RGB image)\n",
        "```\n",
        "\n",
        "**Why not just use NumPy arrays?**\n",
        "- Tensors can run on GPU (1000x faster!)\n",
        "- Tensors track gradients for automatic differentiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Creating Tensors\n",
        "\n",
        "# From Python list\n",
        "t1 = torch.tensor([1, 2, 3, 4, 5])\n",
        "print(f\"From list: {t1}\")\n",
        "print(f\"Shape: {t1.shape}\")\n",
        "print(f\"Data type: {t1.dtype}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# From NumPy array\n",
        "np_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "t2 = torch.from_numpy(np_array)\n",
        "print(f\"From numpy:\\n{t2}\")\n",
        "print(f\"Shape: {t2.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Special tensors\n",
        "zeros = torch.zeros(3, 4)  # 3x4 matrix of zeros\n",
        "ones = torch.ones(2, 3)    # 2x3 matrix of ones\n",
        "rand = torch.rand(2, 2)    # 2x2 matrix of random [0,1]\n",
        "randn = torch.randn(2, 2)  # 2x2 matrix of random normal\n",
        "\n",
        "print(f\"Zeros (3x4):\\n{zeros}\")\n",
        "print(f\"\\nRandom [0,1]:\\n{rand}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Tensor Operations\n",
        "\n",
        "a = torch.tensor([1., 2., 3.])\n",
        "b = torch.tensor([4., 5., 6.])\n",
        "\n",
        "# Basic math (element-wise)\n",
        "print(f\"a = {a}\")\n",
        "print(f\"b = {b}\")\n",
        "print(f\"a + b = {a + b}\")\n",
        "print(f\"a * b = {a * b}\")\n",
        "print(f\"a ** 2 = {a ** 2}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Matrix operations\n",
        "A = torch.tensor([[1., 2.], [3., 4.]])\n",
        "B = torch.tensor([[5., 6.], [7., 8.]])\n",
        "\n",
        "print(f\"Matrix A:\\n{A}\")\n",
        "print(f\"\\nMatrix B:\\n{B}\")\n",
        "print(f\"\\nA @ B (matrix multiply):\\n{A @ B}\")\n",
        "print(f\"\\nA.T (transpose):\\n{A.T}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Reshaping Tensors\n",
        "\n",
        "# This is CRUCIAL for neural networks!\n",
        "x = torch.arange(12)  # [0, 1, 2, ..., 11]\n",
        "print(f\"Original: {x}\")\n",
        "print(f\"Shape: {x.shape}\")\n",
        "\n",
        "# Reshape to 3x4\n",
        "x_3x4 = x.reshape(3, 4)\n",
        "print(f\"\\nReshaped to 3x4:\\n{x_3x4}\")\n",
        "\n",
        "# Reshape to 2x2x3\n",
        "x_2x2x3 = x.reshape(2, 2, 3)\n",
        "print(f\"\\nReshaped to 2x2x3:\\n{x_2x2x3}\")\n",
        "\n",
        "# Use -1 to infer dimension\n",
        "x_auto = x.reshape(4, -1)  # -1 means \"figure it out\"\n",
        "print(f\"\\nReshaped to 4x?:\\n{x_auto}\")\n",
        "print(f\"Shape: {x_auto.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Moving Tensors to GPU\n",
        "\n",
        "If you have a GPU, you can make computations **much faster**!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: GPU Operations\n",
        "\n",
        "# Create tensor on CPU\n",
        "x_cpu = torch.rand(1000, 1000)\n",
        "print(f\"Tensor device: {x_cpu.device}\")\n",
        "\n",
        "# Move to GPU (if available)\n",
        "x_gpu = x_cpu.to(device)\n",
        "print(f\"After .to(device): {x_gpu.device}\")\n",
        "\n",
        "# Time comparison (only meaningful on GPU)\n",
        "import time\n",
        "\n",
        "# CPU timing\n",
        "x_cpu = torch.rand(5000, 5000)\n",
        "y_cpu = torch.rand(5000, 5000)\n",
        "\n",
        "start = time.time()\n",
        "z_cpu = x_cpu @ y_cpu\n",
        "cpu_time = time.time() - start\n",
        "print(f\"\\nCPU matrix multiply: {cpu_time:.4f} seconds\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    x_gpu = x_cpu.to(device)\n",
        "    y_gpu = y_cpu.to(device)\n",
        "    \n",
        "    # Warm-up\n",
        "    _ = x_gpu @ y_gpu\n",
        "    torch.cuda.synchronize()\n",
        "    \n",
        "    start = time.time()\n",
        "    z_gpu = x_gpu @ y_gpu\n",
        "    torch.cuda.synchronize()\n",
        "    gpu_time = time.time() - start\n",
        "    print(f\"GPU matrix multiply: {gpu_time:.4f} seconds\")\n",
        "    print(f\"Speedup: {cpu_time / gpu_time:.1f}x faster!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1.1\n",
        "\n",
        "Create a 3x3 tensor filled with the numbers 1-9, then:\n",
        "1. Print the sum of all elements\n",
        "2. Print the mean of each row\n",
        "3. Reshape it to 9x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Create 3x3 tensor with values 1-9\n",
        "# Hint: torch.arange(1, 10).reshape(...)\n",
        "\n",
        "# Sum all elements\n",
        "# Hint: tensor.sum()\n",
        "\n",
        "# Mean of each row\n",
        "# Hint: tensor.mean(dim=1)\n",
        "\n",
        "# Reshape to 9x1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 2: Building Blocks of Neural Networks\n",
        "\n",
        "## The Neuron: A Simple Function\n",
        "\n",
        "```\n",
        "A SINGLE NEURON:\n",
        "\n",
        "  inputs         weights           output\n",
        "    │              │                 │\n",
        "   x1 ─────► w1 ──┐                  │\n",
        "                  │                  │\n",
        "   x2 ─────► w2 ──┼──► Σ + b ──► f ──┴──► y\n",
        "                  │     │        │\n",
        "   x3 ─────► w3 ──┘   bias    activation\n",
        "                              (e.g., ReLU)\n",
        "\n",
        "   y = f(w1*x1 + w2*x2 + w3*x3 + b)\n",
        "```\n",
        "\n",
        "**The Linear Layer**: Many neurons in parallel!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: The Linear Layer\n",
        "\n",
        "# A linear layer: input_size -> output_size\n",
        "linear = nn.Linear(in_features=3, out_features=2)\n",
        "\n",
        "print(\"Linear layer: 3 inputs → 2 outputs\")\n",
        "print(f\"Weight shape: {linear.weight.shape}\")  # 2x3 matrix\n",
        "print(f\"Bias shape: {linear.bias.shape}\")      # 2 values\n",
        "\n",
        "print(f\"\\nWeights:\\n{linear.weight}\")\n",
        "print(f\"\\nBias: {linear.bias}\")\n",
        "\n",
        "# Forward pass\n",
        "x = torch.tensor([1., 2., 3.])  # Input\n",
        "y = linear(x)                     # Output\n",
        "\n",
        "print(f\"\\nInput: {x}\")\n",
        "print(f\"Output: {y}\")\n",
        "\n",
        "# Manual calculation to verify\n",
        "y_manual = x @ linear.weight.T + linear.bias\n",
        "print(f\"Manual: {y_manual}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Activation Functions: Adding Non-linearity\n",
        "\n",
        "Without activations, stacking linear layers is useless (just another linear layer!).\n",
        "\n",
        "Activations add **non-linearity** so networks can learn complex patterns.\n",
        "\n",
        "```\n",
        "COMMON ACTIVATIONS:\n",
        "\n",
        "ReLU:           Sigmoid:        Softmax:\n",
        "    │    ╱         │ ───────       │ \n",
        "    │   ╱          │╱              │   Probabilities!\n",
        "────┼──╱       ────┼────       ────┼──── \n",
        "    │              │               │   Sum to 1.0\n",
        "    │              │               │\n",
        "\n",
        "f(x) = max(0,x)  f(x)=1/(1+e^-x)  f(x)=e^x/Σe^x\n",
        "\n",
        "Use: Hidden      Use: Binary     Use: Multi-class\n",
        "     layers           output          output\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Activation Functions\n",
        "\n",
        "x = torch.linspace(-5, 5, 100)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# ReLU\n",
        "axes[0].plot(x, torch.relu(x))\n",
        "axes[0].set_title('ReLU: max(0, x)')\n",
        "axes[0].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes[0].axvline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Sigmoid\n",
        "axes[1].plot(x, torch.sigmoid(x))\n",
        "axes[1].set_title('Sigmoid: 1/(1+e^-x)')\n",
        "axes[1].axhline(0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Tanh\n",
        "axes[2].plot(x, torch.tanh(x))\n",
        "axes[2].set_title('Tanh')\n",
        "axes[2].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Softmax for Classification\n",
        "\n",
        "# Raw scores from network (called \"logits\")\n",
        "logits = torch.tensor([2.0, 1.0, 0.1])\n",
        "\n",
        "# Convert to probabilities with softmax\n",
        "probs = F.softmax(logits, dim=0)\n",
        "\n",
        "print(f\"Raw scores (logits): {logits}\")\n",
        "print(f\"Probabilities (softmax): {probs}\")\n",
        "print(f\"Sum of probabilities: {probs.sum():.4f}\")\n",
        "\n",
        "# Interpretation\n",
        "classes = ['Cat', 'Dog', 'Bird']\n",
        "for cls, prob in zip(classes, probs):\n",
        "    print(f\"  {cls}: {prob:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 3: Building a Neural Network\n",
        "\n",
        "## The Sequential Model\n",
        "\n",
        "Stack layers like LEGO blocks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Building a Simple Network\n",
        "\n",
        "# Network architecture:\n",
        "# Input (784) -> Hidden (128) -> ReLU -> Output (10)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 128),  # First layer: 784 inputs, 128 outputs\n",
        "    nn.ReLU(),            # Activation\n",
        "    nn.Linear(128, 10)    # Output layer: 128 inputs, 10 classes\n",
        ")\n",
        "\n",
        "print(model)\n",
        "print(f\"\\nNumber of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Forward Pass\n",
        "\n",
        "# Fake input: batch of 5 images, each 28x28 = 784 pixels\n",
        "fake_images = torch.randn(5, 784)\n",
        "\n",
        "# Forward pass\n",
        "output = model(fake_images)\n",
        "\n",
        "print(f\"Input shape: {fake_images.shape}\")  # [5, 784]\n",
        "print(f\"Output shape: {output.shape}\")       # [5, 10]\n",
        "\n",
        "# Get predictions\n",
        "probs = F.softmax(output, dim=1)\n",
        "predictions = output.argmax(dim=1)\n",
        "\n",
        "print(f\"\\nPredictions (class indices): {predictions}\")\n",
        "print(f\"\\nFirst sample probabilities:\")\n",
        "for i, p in enumerate(probs[0]):\n",
        "    print(f\"  Class {i}: {p:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Network Class\n",
        "\n",
        "For more control, define your own network class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Custom Network Class\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Define forward pass\n",
        "        x = self.fc1(x)       # Linear\n",
        "        x = F.relu(x)          # Activation\n",
        "        x = self.fc2(x)       # Linear\n",
        "        return x\n",
        "\n",
        "# Create network\n",
        "net = SimpleNet(input_size=784, hidden_size=128, num_classes=10)\n",
        "print(net)\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(1, 784)\n",
        "test_output = net(test_input)\n",
        "print(f\"\\nOutput shape: {test_output.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 2.1\n",
        "\n",
        "Create a deeper network with:\n",
        "- Input: 784\n",
        "- Hidden 1: 256 + ReLU\n",
        "- Hidden 2: 128 + ReLU\n",
        "- Hidden 3: 64 + ReLU\n",
        "- Output: 10\n",
        "\n",
        "How many parameters does it have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Using nn.Sequential\n",
        "deep_net = nn.Sequential(\n",
        "    # Add your layers here\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "# num_params = sum(p.numel() for p in deep_net.parameters())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: The Training Loop\n",
        "\n",
        "## How Neural Networks Learn\n",
        "\n",
        "```\n",
        "THE TRAINING LOOP:\n",
        "\n",
        "┌─────────────────────────────────────────────────────────────┐\n",
        "│                                                             │\n",
        "│  1. FORWARD PASS                                            │\n",
        "│     Input ──► Network ──► Prediction                        │\n",
        "│                                                             │\n",
        "│  2. COMPUTE LOSS                                            │\n",
        "│     How wrong are we? (prediction vs actual)                │\n",
        "│                                                             │\n",
        "│  3. BACKWARD PASS                                           │\n",
        "│     Compute gradients (which direction to adjust?)          │\n",
        "│                                                             │\n",
        "│  4. UPDATE WEIGHTS                                          │\n",
        "│     weights = weights - learning_rate * gradients           │\n",
        "│                                                             │\n",
        "│  5. REPEAT! (thousands of times)                            │\n",
        "│                                                             │\n",
        "└─────────────────────────────────────────────────────────────┘\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Understanding Gradients\n",
        "\n",
        "# Create a tensor that requires gradients\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "# Compute a function: y = x^2\n",
        "y = x ** 2\n",
        "\n",
        "print(f\"x = {x.item()}\")\n",
        "print(f\"y = x² = {y.item()}\")\n",
        "\n",
        "# Compute gradient: dy/dx = 2x\n",
        "y.backward()\n",
        "\n",
        "print(f\"dy/dx = {x.grad.item()}\")\n",
        "print(f\"(We expect 2*x = 2*2 = 4)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Loss Functions\n",
        "\n",
        "# For classification: Cross Entropy Loss\n",
        "logits = torch.tensor([[2.0, 1.0, 0.1]])  # Raw scores for 3 classes\n",
        "target = torch.tensor([0])                  # True class is 0\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "loss = criterion(logits, target)\n",
        "\n",
        "print(f\"Logits: {logits}\")\n",
        "print(f\"True class: {target.item()}\")\n",
        "print(f\"Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Lower loss = better!\n",
        "# Perfect prediction would give loss ≈ 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the MNIST Dataset\n",
        "\n",
        "MNIST: 70,000 handwritten digit images (28x28 pixels, grayscale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Loading MNIST\n",
        "\n",
        "# Transform: Convert to tensor and normalize\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
        "])\n",
        "\n",
        "# Download and load training data\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data', \n",
        "    train=True, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='./data', \n",
        "    train=False, \n",
        "    download=True, \n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "\n",
        "# Look at one sample\n",
        "image, label = train_dataset[0]\n",
        "print(f\"\\nImage shape: {image.shape}\")  # [1, 28, 28]\n",
        "print(f\"Label: {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Visualize Some Digits\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    image, label = train_dataset[i]\n",
        "    ax.imshow(image.squeeze(), cmap='gray')\n",
        "    ax.set_title(f'Label: {label}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('MNIST Digits', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Create Data Loaders\n",
        "\n",
        "# DataLoader: Loads data in batches for efficient training\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Number of batches (training): {len(train_loader)}\")\n",
        "print(f\"Number of batches (test): {len(test_loader)}\")\n",
        "\n",
        "# Get one batch\n",
        "images, labels = next(iter(train_loader))\n",
        "print(f\"\\nBatch images shape: {images.shape}\")  # [64, 1, 28, 28]\n",
        "print(f\"Batch labels shape: {labels.shape}\")    # [64]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Complete Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Complete Training Pipeline\n",
        "\n",
        "# 1. Define the model\n",
        "class MNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()  # 28x28 -> 784\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Create model and move to device\n",
        "model = MNISTNet().to(device)\n",
        "print(model)\n",
        "\n",
        "# 2. Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(f\"\\nModel on: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Training Function\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # Set to training mode\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for images, labels in train_loader:\n",
        "        # Move to device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()  # Clear old gradients\n",
        "        loss.backward()         # Compute new gradients\n",
        "        optimizer.step()        # Update weights\n",
        "        \n",
        "        # Track stats\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    return total_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "\n",
        "def test(model, test_loader, criterion, device):\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():  # No gradients needed for testing\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    \n",
        "    return total_loss / len(test_loader), 100. * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Train for 5 Epochs\n",
        "\n",
        "num_epochs = 5\n",
        "history = {'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': []}\n",
        "\n",
        "print(\"Training MNIST Classifier...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "    test_loss, test_acc = test(model, test_loader, criterion, device)\n",
        "    \n",
        "    # Save history\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['test_acc'].append(test_acc)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "    print(f\"  Test Loss:  {test_loss:.4f}, Test Acc:  {test_acc:.2f}%\")\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Plot Training History\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Loss\n",
        "axes[0].plot(history['train_loss'], 'o-', label='Train')\n",
        "axes[0].plot(history['test_loss'], 's-', label='Test')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].set_title('Loss over Training')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy\n",
        "axes[1].plot(history['train_acc'], 'o-', label='Train')\n",
        "axes[1].plot(history['test_acc'], 's-', label='Test')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].set_title('Accuracy over Training')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SOLVED EXAMPLE: Visualize Predictions\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Get some test images\n",
        "images, labels = next(iter(test_loader))\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(images)\n",
        "    _, predictions = outputs.max(1)\n",
        "\n",
        "# Move back to CPU for plotting\n",
        "images = images.cpu()\n",
        "labels = labels.cpu()\n",
        "predictions = predictions.cpu()\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(images[i].squeeze(), cmap='gray')\n",
        "    color = 'green' if predictions[i] == labels[i] else 'red'\n",
        "    ax.set_title(f'Pred: {predictions[i].item()}\\nTrue: {labels[i].item()}', color=color)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Model Predictions (Green=Correct, Red=Wrong)', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3.1\n",
        "\n",
        "Experiment with the network architecture. Try:\n",
        "1. Adding more hidden layers\n",
        "2. Changing the number of neurons per layer\n",
        "3. Using dropout for regularization\n",
        "\n",
        "Can you get above 98% test accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "class ImprovedMNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Design your improved network\n",
        "        # Hint: Try nn.Dropout(0.2) between layers\n",
        "        pass\n",
        "        \n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "# Train and evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Challenge Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 1: Fashion MNIST\n",
        "\n",
        "Replace MNIST digits with Fashion MNIST (10 clothing categories).\n",
        "Train a classifier and report your accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Load Fashion MNIST\n",
        "# Hint: datasets.FashionMNIST instead of datasets.MNIST\n",
        "\n",
        "# Fashion MNIST classes:\n",
        "fashion_classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 2: Learning Rate Experiments\n",
        "\n",
        "The learning rate is crucial! Compare:\n",
        "- lr = 0.1 (too high?)\n",
        "- lr = 0.01\n",
        "- lr = 0.001 (default)\n",
        "- lr = 0.0001 (too low?)\n",
        "\n",
        "Plot the training curves for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Challenge 3: Confusion Matrix\n",
        "\n",
        "Create a confusion matrix showing which digits get confused with each other most often."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Hint: Use sklearn.metrics.confusion_matrix\n",
        "# and seaborn.heatmap for visualization\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Summary\n",
        "\n",
        "## What We Learned\n",
        "\n",
        "| Concept | Description |\n",
        "|---------|-------------|\n",
        "| **Tensors** | Multi-dimensional arrays that can run on GPU |\n",
        "| **nn.Linear** | Fully connected layer: y = Wx + b |\n",
        "| **Activations** | Non-linear functions (ReLU, Sigmoid, Softmax) |\n",
        "| **Forward Pass** | Input → Network → Output |\n",
        "| **Loss Function** | Measures how wrong our predictions are |\n",
        "| **Backward Pass** | Computes gradients via backpropagation |\n",
        "| **Optimizer** | Updates weights to minimize loss |\n",
        "\n",
        "## Key Code Patterns\n",
        "\n",
        "```python\n",
        "# Define model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(784, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        outputs = model(images)        # Forward\n",
        "        loss = criterion(outputs, labels)  # Loss\n",
        "        optimizer.zero_grad()           # Clear gradients\n",
        "        loss.backward()                 # Backward\n",
        "        optimizer.step()                # Update weights\n",
        "```\n",
        "\n",
        "## What's Next?\n",
        "\n",
        "In **Lab 4**, we'll use these skills to build a **language model** that predicts the next character!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
