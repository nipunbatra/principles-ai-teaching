# Transformer Block

direction: down

title: {
  label: Transformer Block Architecture
  near: top-center
  shape: text
  style.font-size: 24
  style.bold: true
}

input: {
  label: "Input Embeddings\n[The] [cat] [sat] [on] [the] [mat]"
  shape: rectangle
  style.fill: "#E3F2FD"
  style.stroke: "#2E86AB"
  style.stroke-width: 2
}

pos: {
  label: "+ Positional Encoding\n(so model knows word order)"
  shape: rectangle
  style.fill: "#BBDEFB"
  style.stroke: "#1976D2"
  style.stroke-width: 2
}

attention: {
  label: "SELF-ATTENTION\n\nEvery word attends to\nevery other word"
  shape: rectangle
  style.fill: "#C8E6C9"
  style.stroke: "#388E3C"
  style.stroke-width: 2
}

ffn: {
  label: "FEED-FORWARD NETWORK\n\nProcess each position\nindependently"
  shape: rectangle
  style.fill: "#FFF9C4"
  style.stroke: "#F9A825"
  style.stroke-width: 2
}

repeat: {
  label: "Repeat 12-96 times!"
  shape: rectangle
  style.fill: "#FFECB3"
  style.stroke: "#FF8F00"
  style.stroke-width: 2
  style.font-size: 18
}

input -> pos
pos -> attention
attention -> ffn
ffn -> repeat

note: {
  label: "Each block refines the representations with more context"
  shape: rectangle
  style.fill: "#ECEFF1"
  style.stroke: "#607D8B"
  style.font-size: 14
}

repeat -> note
