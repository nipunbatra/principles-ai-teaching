# How to Build an LLM

direction: down

title: {
  label: The Complete LLM Recipe
  near: top-center
  shape: text
  style.font-size: 24
  style.bold: true
}

step1: {
  label: "1. COLLECT DATA\n\nScrape internet, books, code\n(terabytes of text)"
  shape: rectangle
  style.fill: "#E3F2FD"
  style.stroke: "#2E86AB"
  style.stroke-width: 2
}

step2: {
  label: "2. TOKENIZE\n\nConvert text to token sequences\n(BPE algorithm)"
  shape: rectangle
  style.fill: "#BBDEFB"
  style.stroke: "#1976D2"
  style.stroke-width: 2
}

step3: {
  label: "3. BUILD ARCHITECTURE\n\nTransformer with\nembeddings + attention + FFN"
  shape: rectangle
  style.fill: "#C8E6C9"
  style.stroke: "#388E3C"
  style.stroke-width: 2
}

step4: {
  label: "4. TRAIN (Pre-training)\n\nPredict next token, minimize loss\nWeeks on 1000s of GPUs"
  shape: rectangle
  style.fill: "#A5D6A7"
  style.stroke: "#2E7D32"
  style.stroke-width: 2
}

step5: {
  label: "5. FINE-TUNE\n\nInstruction tuning on\n(instruction, response) pairs"
  shape: rectangle
  style.fill: "#FFF9C4"
  style.stroke: "#F9A825"
  style.stroke-width: 2
}

step6: {
  label: "6. RLHF\n\nHuman feedback to make\nit helpful and safe"
  shape: rectangle
  style.fill: "#FFECB3"
  style.stroke: "#FF8F00"
  style.stroke-width: 2
}

step1 -> step2 -> step3 -> step4 -> step5 -> step6
