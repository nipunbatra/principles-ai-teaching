---
marp: true
theme: default
paginate: true
backgroundColor: #fff
style: |
  section {
    font-family: 'Segoe UI', 'Arial', sans-serif;
    font-size: 23px;
    padding: 35px;
    color: #333;
  }
  h1 { color: #2E86AB; font-size: 1.7em; margin-bottom: 0.2em; }
  h2 { color: #06A77D; font-size: 1.1em; margin-top: 0; }
  h3 { color: #457B9D; font-size: 1.0em; }
  strong { color: #D62828; }
  code {
    background: #f4f4f4;
    color: #2E86AB;
    padding: 2px 6px;
    border-radius: 4px;
    font-family: 'Consolas', 'Monaco', monospace;
  }
  pre {
    background: #f8f9fa;
    border-radius: 8px;
    padding: 12px;
    font-size: 0.78em;
    line-height: 1.3;
    overflow: hidden;
  }
  .example {
    background: linear-gradient(135deg, #e8f5e9 0%, #c8e6c9 100%);
    border-left: 4px solid #06A77D;
    padding: 10px 12px;
    margin: 8px 0;
    border-radius: 0 6px 6px 0;
  }
  .insight {
    background: #fff3cd;
    border-left: 4px solid #ffc107;
    padding: 10px 12px;
    margin: 8px 0;
    border-radius: 0 6px 6px 0;
  }
  .warning {
    background: #ffebee;
    border-left: 4px solid #D62828;
    padding: 10px 12px;
    margin: 8px 0;
    border-radius: 0 6px 6px 0;
  }
  .realworld {
    background: #e3f2fd;
    border-left: 4px solid #2196F3;
    padding: 10px 12px;
    margin: 8px 0;
    border-radius: 0 6px 6px 0;
  }
  .columns { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; }
  .columns3 { display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; }
  table { font-size: 0.85em; width: 100%; }
  th { background: #2E86AB; color: white; padding: 6px; }
  td { padding: 6px; border-bottom: 1px solid #dee2e6; }
---

# Next Token Prediction
## Building ChatGPT from Scratch (Conceptually)

**Nipun Batra** Â· IIT Gandhinagar

---

# What We'll Learn Today

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      THE JOURNEY TO GPT                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Level 1: The Intuition                                            â”‚
â”‚   â””â”€â”€ What does "predicting the next word" really mean?             â”‚
â”‚                                                                      â”‚
â”‚   Level 2: The Counting Era                                         â”‚
â”‚   â””â”€â”€ Bigrams: Count letter pairs                                   â”‚
â”‚                                                                      â”‚
â”‚   Level 3: Representing Meaning                                     â”‚
â”‚   â””â”€â”€ Embeddings: Words as vectors in space                         â”‚
â”‚                                                                      â”‚
â”‚   Level 4: Learning Patterns                                        â”‚
â”‚   â””â”€â”€ Neural networks for next-token prediction                     â”‚
â”‚                                                                      â”‚
â”‚   Level 5: The Context Problem                                      â”‚
â”‚   â””â”€â”€ Why we need to remember more                                  â”‚
â”‚                                                                      â”‚
â”‚   Level 6: The Revolution                                           â”‚
â”‚   â””â”€â”€ Attention and Transformers                                    â”‚
â”‚                                                                      â”‚
â”‚   Level 7: From Theory to ChatGPT                                   â”‚
â”‚   â””â”€â”€ Scaling up to billions of parameters                          â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# The Journey Ahead

![w:700 center](diagrams/svg/learning_journey.svg)

---

# ğŸ¯ Level 1: The Intuition
## What Are We Really Doing?

---

# The Core Problem

Every language model answers **one simple question**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚   "Given what I have seen so far, what word comes next?"        â”‚
â”‚                                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚   â”‚ The capital of France is ___          â”‚                     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                       â”‚                                         â”‚
â”‚                       â–¼                                         â”‚
â”‚                    "Paris"                                      â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

That's it. **Predict the next word. Repeat until done.**

---

# You Already Know This!

You've been using next-word prediction your whole life:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                      â”‚
â”‚   ğŸ“± Your Phone's Keyboard:                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚  I'm running ___                                 â”‚               â”‚
â”‚   â”‚              [late]  [out]  [away]              â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                      â”‚
â”‚   ğŸ” Google Search:                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚  how to make ___                                 â”‚               â”‚
â”‚   â”‚  â€¢ how to make money                            â”‚               â”‚
â”‚   â”‚  â€¢ how to make pancakes                         â”‚               â”‚
â”‚   â”‚  â€¢ how to make friends                          â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                      â”‚
â”‚   ğŸ“§ Gmail Smart Compose:                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   â”‚  Thanks for the ___                              â”‚               â”‚
â”‚   â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚               â”‚
â”‚   â”‚               â”‚ quick response!   â”‚ â† Suggested â”‚               â”‚
â”‚   â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Let's Play: The Autocomplete Game

<div class="example">

**Round 1:** "The Eiffel Tower is located in ___"

Your brain: **Paris** (very confident!)

**Round 2:** "I want to eat ___"

Your brain: **pizza? pasta? nothing?** (uncertain!)

**Round 3:** "Once upon a ___"

Your brain: **time** (almost certain!)

**Round 4:** "To be or not to ___"

Your brain: **be** (Shakespeare hardcoded in culture!)

</div>

<div class="insight">
Your brain assigns **probabilities** to each possible next word.
Some contexts have obvious answers, others don't!
</div>

---

# The Mathematical View

When you read "The capital of France is ___", your brain computes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                      â”‚
â”‚   P("Paris" | "The capital of France is")    = 0.85  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   â”‚
â”‚   P("the" | "The capital of France is")      = 0.02  â–ˆ              â”‚
â”‚   P("London" | "The capital of France is")   = 0.01  â–‘              â”‚
â”‚   P("beautiful" | "The capital of France is")= 0.01  â–‘              â”‚
â”‚   P("..." | "The capital of France is")      = 0.11  â–ˆâ–ˆ             â”‚
â”‚                                                                      â”‚
â”‚   All probabilities sum to 1.0                                       â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

This is called a PROBABILITY DISTRIBUTION over the vocabulary.
Language models learn to produce these distributions!
```

---

# It's JUST Prediction

You might think ChatGPT "understands" physics or history.
But all it does is predict the next word.

```
Prompt: "F = m"                  Prediction: "a"      â† Newton's Law!
Prompt: "To be or not to"        Prediction: "be"     â† Shakespeare!
Prompt: "E = mc"                 Prediction: "Â²"      â† Einstein!
Prompt: "print('Hello"           Prediction: "')"     â† Python syntax!
Prompt: "2 + 2 ="                Prediction: "4"      â† Math!
Prompt: "The mitochondria is"    Prediction: "the"    â† Biology meme!
```

<div class="insight">
If you predict well enough, you **appear** to understand everything.
The model has compressed patterns from human knowledge into its weights.
</div>

---

# The Shocking Simplicity

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                      â”‚
â”‚                    THE ONE ALGORITHM                                 â”‚
â”‚                                                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚                                                           â”‚      â”‚
â”‚   â”‚   for token in generate(prompt):                          â”‚      â”‚
â”‚   â”‚       probabilities = model(all_tokens_so_far)            â”‚      â”‚
â”‚   â”‚       next_token = sample(probabilities)                  â”‚      â”‚
â”‚   â”‚       output(next_token)                                  â”‚      â”‚
â”‚   â”‚                                                           â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                      â”‚
â”‚   That's literally it. ChatGPT is this loop run millions            â”‚
â”‚   of times with a really good model.                                 â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# The Magic of "Just Prediction"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Q: "What is 17 + 28?"                                           â”‚
â”‚                                                                 â”‚
â”‚ The model has seen THOUSANDS of math problems in training:      â”‚
â”‚   "2 + 2 = 4"                                                   â”‚
â”‚   "15 + 10 = 25"                                                â”‚
â”‚   "17 + 28 = 45"   â† Saw this pattern!                         â”‚
â”‚                                                                 â”‚
â”‚ So when asked "17 + 28 =", it predicts "45"                    â”‚
â”‚ Not because it "knows" math, but because that pattern exists!   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

<div class="warning">
This is why LLMs can make math mistakes â€” they're pattern matching,
not actually computing! Try asking "What is 4738 Ã— 2951?" and you'll see errors.
</div>

---

# Emergent Behaviors

As models get bigger, surprising abilities **emerge**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHAT EMERGES FROM PREDICTION                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Small Model (100M parameters):                                    â”‚
â”‚   â€¢ Complete simple sentences                                        â”‚
â”‚   â€¢ Basic grammar                                                    â”‚
â”‚                                                                      â”‚
â”‚   Medium Model (1B parameters):                                     â”‚
â”‚   â€¢ Answer factual questions                                         â”‚
â”‚   â€¢ Simple reasoning                                                 â”‚
â”‚                                                                      â”‚
â”‚   Large Model (100B+ parameters):                                   â”‚
â”‚   â€¢ Complex reasoning                                                â”‚
â”‚   â€¢ Code generation                                                  â”‚
â”‚   â€¢ Creative writing                                                 â”‚
â”‚   â€¢ Multi-step problem solving                                       â”‚
â”‚   â€¢ "Understanding" context and nuance                               â”‚
â”‚                                                                      â”‚
â”‚   All from the same objective: predict the next token!              â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ”¢ Level 2: The Counting Era
## Bigrams: The Simplest Language Model

---

# The Simplest Possible Model

**Idea:** Just count what letter usually follows each letter.

<div class="example">

**Training data:** Names like `aabid`, `zeel`, `priya`, `nipun`

Count transitions:
- After `a`: saw `a` (1 time), `b` (1 time)
- After `z`: saw `e` (1 time)
- After `e`: saw `e` (1 time), `l` (1 time)
- After `n`: saw `i` (1 time) in "nipun"

</div>

This is called a **Bigram** model (looks at pairs of 2 characters).

---

# Let's Build It Step by Step

```
Training Data: "aabid", "priya", "zeel", "nipun"

Step 1: Add special tokens
        ".aabid.", ".priya.", ".zeel.", ".nipun."
        (. marks beginning and end)

Step 2: Count all pairs
        ".a" appears 2 times   (from aabid, priya doesn't start with 'a')
        "aa" appears 1 time
        "ab" appears 1 time
        "bi" appears 1 time
        "id" appears 1 time
        "d." appears 1 time
        ... and so on

Step 3: Convert counts to probabilities
        P(next = 'a' | current = '.') = Count(".a") / Total pairs starting with "."
        P(next = 'a' | current = '.') = 2 / 4 = 0.50
```

---

# Bigram: The Counting Table

```
        Next Character â†’
      â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
      â”‚  a  â”‚  b  â”‚  e  â”‚  i  â”‚  l  â”‚ ... â”‚
   â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
C  a  â”‚ 0.3 â”‚ 0.2 â”‚ 0.1 â”‚ 0.2 â”‚ 0.1 â”‚ ... â”‚  (probabilities)
u  b  â”‚ 0.1 â”‚ 0.0 â”‚ 0.1 â”‚ 0.5 â”‚ 0.0 â”‚ ... â”‚
r  e  â”‚ 0.2 â”‚ 0.0 â”‚ 0.3 â”‚ 0.1 â”‚ 0.2 â”‚ ... â”‚
r  i  â”‚ 0.4 â”‚ 0.1 â”‚ 0.1 â”‚ 0.0 â”‚ 0.1 â”‚ ... â”‚
   â†“  â”‚ ... â”‚ ... â”‚ ... â”‚ ... â”‚ ... â”‚ ... â”‚
   â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Each row sums to 1.0 (it's a probability distribution!)

To generate: Look up current letter â†’ Sample from that row
```

**This table IS the model.** No neural network needed!

---

# Generating Names with Bigrams

```
Step 1: Start with "." (beginning token)
        Look up row "." â†’ High prob for 'a', 's', 'm'
        Sample â†’ Got 'a'

Step 2: Current = 'a'
        Look up row "a" â†’ Moderate prob for 'a', 'b', 'n'
        Sample â†’ Got 'b'

Step 3: Current = 'b'
        Look up row "b" â†’ High prob for 'i', 'a', 'r'
        Sample â†’ Got 'i'

Step 4: Current = 'i'
        Look up row "i" â†’ High prob for 'd', 'n', 'a'
        Sample â†’ Got 'd'

Step 5: Current = 'd'
        Look up row "d" â†’ High prob for "." (end token)
        Sample â†’ Got "."   (DONE!)

Result: "abid"  â† Looks like a real name!
```

---

# Interactive Example: Generating from Bigrams

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BIGRAM GENERATION DEMO                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Current: .                                                          â”‚
â”‚  Options: a(0.25), m(0.20), s(0.15), j(0.10), r(0.08), ...          â”‚
â”‚  Rolled: 0.18 â†’ Selected 'm'                                        â”‚
â”‚                                                                      â”‚
â”‚  Current: m                                                          â”‚
â”‚  Options: a(0.40), i(0.25), o(0.15), u(0.10), ...                   â”‚
â”‚  Rolled: 0.32 â†’ Selected 'a'                                        â”‚
â”‚                                                                      â”‚
â”‚  Current: a                                                          â”‚
â”‚  Options: n(0.20), r(0.18), l(0.15), y(0.12), .(0.10), ...          â”‚
â”‚  Rolled: 0.45 â†’ Selected 'r'                                        â”‚
â”‚                                                                      â”‚
â”‚  Current: r                                                          â”‚
â”‚  Options: i(0.25), a(0.20), y(0.18), .(0.15), ...                   â”‚
â”‚  Rolled: 0.52 â†’ Selected 'y'                                        â”‚
â”‚                                                                      â”‚
â”‚  Current: y                                                          â”‚
â”‚  Options: .(0.45), a(0.15), i(0.10), ...                            â”‚
â”‚  Rolled: 0.30 â†’ Selected '.'                                        â”‚
â”‚                                                                      â”‚
â”‚  Generated: "mary"  â† A real name!                                   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Why Bigrams Fail: The Memory Problem

<div class="columns">
<div>

**The Problem:**
```
Sentence: "The quick brown
           fox jumps over
           the lazy dog."

Question: After "dog",
          what comes next?

Bigram sees: "dog" â†’ ?
             (forgot everything
              before "dog"!)
```

</div>
<div>

**Context is Lost:**
```
With context:
"The cat sat on the ___"
  â†’ Probably "mat"

Without context:
"the ___"
  â†’ Could be anything!

Bigram only sees 1 char!
```

</div>
</div>

<div class="warning">
Bigrams have **no memory**. They forget everything except the last character!
</div>

---

# A Concrete Example

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE CONTEXT PROBLEM                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Sentence 1: "I love eating pizza with extra cheese"               â”‚
â”‚   Sentence 2: "I love eating pizza with my friends"                 â”‚
â”‚                                                                      â”‚
â”‚   After "with", what comes next?                                    â”‚
â”‚                                                                      â”‚
â”‚   Bigram's view: "h" â†’ ?                                            â”‚
â”‚   (It doesn't even know it's in "with"!)                            â”‚
â”‚                                                                      â”‚
â”‚   A smarter model would know:                                        â”‚
â”‚   - "pizza with" usually followed by toppings or people             â”‚
â”‚   - "eating with" suggests companions                                â”‚
â”‚   - "love eating" suggests food context                              â”‚
â”‚                                                                      â”‚
â”‚   We need to see MORE context!                                       â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# The Curse of Dimensionality

Why not just count longer patterns?

```
1-gram (Unigram):   27 entries           â† Fits in memory
2-gram (Bigram):    27Â² = 729            â† Still fine
3-gram (Trigram):   27Â³ = 19,683         â† OK
4-gram:             27â´ = 531,441        â† Getting big
5-gram:             27âµ = 14,348,907     â† Very big
10-gram:            27Â¹â° â‰ˆ 205 TRILLION  â† Impossible!

For words (50,000 vocabulary):
2-gram:             50,000Â² = 2.5 BILLION
3-gram:             50,000Â³ = 125 TRILLION
```

<div class="insight">
We can't just count longer patterns â€” we need to **generalize**.
This is where neural networks come in!
</div>

---

# Bigrams: Summary

| Aspect | Bigrams |
|--------|---------|
| **What it does** | Counts P(next char \| current char) |
| **Memory** | 1 character only |
| **Size** | 27 Ã— 27 = 729 numbers |
| **Speed** | Instant (just table lookup) |
| **Quality** | Poor (no context) |
| **Training** | Just counting |

**Key insight:** The model is just a lookup table. No learning, no generalization.

---

# ğŸ“ Level 3: Representing Meaning
## Embeddings: Words as Vectors

---

# How Do Computers Read?

Computers only understand numbers. How do we convert letters?

**Option A: One-Hot Encoding**
```
'a' = [1, 0, 0, 0, ..., 0]    (27 dimensions for letters)
'b' = [0, 1, 0, 0, ..., 0]
'c' = [0, 0, 1, 0, ..., 0]
...
'z' = [0, 0, 0, 0, ..., 1]
```

**Problem:** These vectors are **orthogonal** (dot product = 0).
The computer thinks 'a' and 'b' are completely unrelated!

---

# The Problem with One-Hot

```
Distance between letters:

   'a' â—                    'b' â—

   Distance(a, b) = Distance(a, z) = âˆš2

   Every letter is equally far from every other letter!

   But we KNOW:
   - 'a' and 'e' are both vowels (similar!)
   - 'a' and 'x' have nothing in common (different!)
   - 'p' and 'b' look similar (related!)
```

We need a smarter representation where **similar things are close**.

---

# Dense Embeddings: Meaning as Coordinates

**Idea:** Represent each character as a point in space where **similar things are close**.

```
             â–² Dimension 2 ("Common ending?")
             â”‚
        0.9  â”‚    â— e        â— a
             â”‚        â— i              Vowels cluster
        0.5  â”‚              â— o          together!
             â”‚                    â— u
        0.1  â”‚
             â”‚  â— b   â— c   â— d   â— f   â† Consonants cluster
        -0.5 â”‚           â— x   â— z      â† Rare letters
             â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Dimension 1 ("Vowel?")
                  -1       0       1
```

Now `a` and `e` are **mathematically close**!

---

# Word Embeddings: The Famous Example

![w:900 center](diagrams/svg/embedding_space.svg)

---

# The King - Man + Woman = Queen Example

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WORD ARITHMETIC                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   king  = [0.8, 0.3, 0.9, ...]  (royalty, male, power)              â”‚
â”‚   man   = [0.1, 0.3, 0.5, ...]  (person, male, average)             â”‚
â”‚   woman = [0.1, 0.9, 0.5, ...]  (person, female, average)           â”‚
â”‚                                                                      â”‚
â”‚   king - man + woman = ?                                             â”‚
â”‚                                                                      â”‚
â”‚   [0.8, 0.3, 0.9] - [0.1, 0.3, 0.5] + [0.1, 0.9, 0.5]              â”‚
â”‚   = [0.8, 0.9, 0.9]                                                  â”‚
â”‚                                                                      â”‚
â”‚   Nearest word to [0.8, 0.9, 0.9]: "queen"!                         â”‚
â”‚                                                                      â”‚
â”‚   The model learned that:                                            â”‚
â”‚   "The relationship between king and man is the same as             â”‚
â”‚    the relationship between queen and woman"                         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# More Word Analogies

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EMBEDDING ANALOGIES                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   France : Paris :: Japan : ?                                        â”‚
â”‚   â†’ Tokyo                                                            â”‚
â”‚                                                                      â”‚
â”‚   good : better :: bad : ?                                           â”‚
â”‚   â†’ worse                                                            â”‚
â”‚                                                                      â”‚
â”‚   walking : walked :: swimming : ?                                   â”‚
â”‚   â†’ swam                                                             â”‚
â”‚                                                                      â”‚
â”‚   Einstein : physicist :: Picasso : ?                                â”‚
â”‚   â†’ painter                                                          â”‚
â”‚                                                                      â”‚
â”‚   The embeddings capture RELATIONSHIPS automatically!                â”‚
â”‚   No one told the model about capitals or verb tenses!               â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# How Embeddings Are Learned

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  Start: Random vectors for each word                           â”‚
â”‚                                                                 â”‚
â”‚  Training:                                                      â”‚
â”‚    "The cat sat on the mat"                                     â”‚
â”‚                                                                 â”‚
â”‚    "cat" often appears near "sat", "dog", "pet"                â”‚
â”‚    â†’ Push these embeddings closer together                      â”‚
â”‚                                                                 â”‚
â”‚    "cat" rarely appears near "quantum", "fiscal"               â”‚
â”‚    â†’ Push these embeddings apart                                â”‚
â”‚                                                                 â”‚
â”‚  After billions of examples:                                    â”‚
â”‚    Similar words â†’ Similar vectors                              â”‚
â”‚    Related concepts â†’ Close in space                            â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Visualizing Embeddings

```
Real word embeddings projected to 2D (using t-SNE):

                    Countries
                  â— India   â— China
                    â— Japan
               â— France                           Sports
                                               â— football
    Animals                                    â— cricket
  â— dog  â— cat                                â— basketball
     â— tiger
        â— lion                      Colors
                                 â— red  â— blue
                                    â— green
     Food
   â— pizza                         Technology
     â— burger                      â— computer
       â— pasta                       â— phone
                                      â— laptop

Similar things automatically cluster together!
```

---

# Embedding Dimensions

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHAT DO DIMENSIONS MEAN?                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   We use 256-4096 dimensions in practice, but imagine 4:            â”‚
â”‚                                                                      â”‚
â”‚   Dimension 1: "Is it alive?"                                        â”‚
â”‚   Dimension 2: "Is it a person?"                                     â”‚
â”‚   Dimension 3: "Is it concrete vs abstract?"                         â”‚
â”‚   Dimension 4: "Positive or negative sentiment?"                     â”‚
â”‚                                                                      â”‚
â”‚   "dog"    = [0.9, 0.1, 0.8, 0.7]  (alive, not person, concrete, +) â”‚
â”‚   "cat"    = [0.9, 0.1, 0.8, 0.6]  (very similar to dog!)           â”‚
â”‚   "love"   = [0.2, 0.3, -0.8, 0.9] (abstract, positive)             â”‚
â”‚   "hate"   = [0.2, 0.3, -0.8, -0.9](abstract, negative)             â”‚
â”‚   "table"  = [0.0, 0.0, 0.9, 0.0]  (not alive, concrete, neutral)   â”‚
â”‚                                                                      â”‚
â”‚   In reality, dimensions are learned and not so interpretable!       â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ§  Level 4: Learning Patterns
## Neural Networks for Next-Token Prediction

---

# From Counting to Learning

```
BIGRAM (Counting):                NEURAL NETWORK (Learning):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     â”‚           â”‚                     â”‚
â”‚   Count(b|a)        â”‚           â”‚   f(embed(a); Î¸)    â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”‚           â”‚                     â”‚
â”‚   Count(a)          â”‚           â”‚   Î¸ = learned       â”‚
â”‚                     â”‚           â”‚       weights       â”‚
â”‚   (Fixed table)     â”‚           â”‚   (Flexible function)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                   â”‚
        â–¼                                   â–¼
  Only memorizes                  Can GENERALIZE to
  exact patterns                  unseen patterns!
```

---

# The Neural Network Architecture

![w:800 center](diagrams/svg/neural_net_architecture.svg)

---

# The Architecture Unpacked

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MLP LANGUAGE MODEL                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Input: Last 3 characters [a, a, b]                                â”‚
â”‚                                                                      â”‚
â”‚   Step 1: EMBED each character                                       â”‚
â”‚           a â†’ [0.2, 0.5]                                             â”‚
â”‚           a â†’ [0.2, 0.5]                                             â”‚
â”‚           b â†’ [0.8, -0.3]                                            â”‚
â”‚                                                                      â”‚
â”‚   Step 2: CONCATENATE embeddings                                     â”‚
â”‚           [0.2, 0.5, 0.2, 0.5, 0.8, -0.3]  (6 numbers)              â”‚
â”‚                                                                      â”‚
â”‚   Step 3: HIDDEN LAYER (learn patterns)                              â”‚
â”‚           h = ReLU(Wâ‚ Â· concat + bâ‚)                                 â”‚
â”‚           h = [0.7, 0.1, 0.9, 0.3]  (example)                       â”‚
â”‚                                                                      â”‚
â”‚   Step 4: OUTPUT LAYER (predict next char)                           â”‚
â”‚           logits = Wâ‚‚ Â· h + bâ‚‚                                       â”‚
â”‚           probs = softmax(logits)                                    â”‚
â”‚           [P(a)=0.1, P(b)=0.05, ..., P(i)=0.6, ...]                 â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Creating Training Data: The Sliding Window

```
Text: "aabid"

Create (context â†’ target) pairs by sliding a window:

    Position 0:  [., ., .] â†’ 'a'     "What comes first?"
    Position 1:  [., ., a] â†’ 'a'     "After nothing+a?"
    Position 2:  [., a, a] â†’ 'b'     "After a, a?"
    Position 3:  [a, a, b] â†’ 'i'     "After a, a, b?"
    Position 4:  [a, b, i] â†’ 'd'     "After a, b, i?"
    Position 5:  [b, i, d] â†’ '.'     "After b, i, d?"

    â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
    â”‚ . â”‚ a â”‚ a â”‚ â”€â”€â–º Context (input)
    â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
                â””â”€â”€â–º Target: 'b' (what we want to predict)
```

---

# Training: Learning from Mistakes

```
Step 1: Forward Pass
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Input: [a, a, b]      â”‚
        â”‚ â†“                     â”‚
        â”‚ Network predicts:     â”‚
        â”‚ P(i)=0.10 â† Wrong!    â”‚
        â”‚ P(z)=0.30 â† Very wrongâ”‚
        â”‚ P(a)=0.20             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        Actual answer: 'i'

Step 2: Compute Loss
        Loss = -log(P(correct answer))
        Loss = -log(0.10) = 2.3   â† High loss = bad prediction

Step 3: Backpropagation
        Compute gradients: "How should each weight change?"
        Adjust weights to make P(i) higher next time

Step 4: Repeat millions of times
        â†’ Network learns to predict well!
```

---

# The Loss Function: Cross-Entropy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CROSS-ENTROPY LOSS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Loss = -log(P(correct_answer))                                     â”‚
â”‚                                                                      â”‚
â”‚   Examples:                                                          â”‚
â”‚                                                                      â”‚
â”‚   If model is confident and RIGHT:                                   â”‚
â”‚   P(correct) = 0.95  â†’  Loss = -log(0.95) = 0.05  â† Low loss!       â”‚
â”‚                                                                      â”‚
â”‚   If model is uncertain:                                             â”‚
â”‚   P(correct) = 0.50  â†’  Loss = -log(0.50) = 0.69  â† Medium loss     â”‚
â”‚                                                                      â”‚
â”‚   If model is confident and WRONG:                                   â”‚
â”‚   P(correct) = 0.01  â†’  Loss = -log(0.01) = 4.6   â† High loss!      â”‚
â”‚                                                                      â”‚
â”‚   The model gets heavily penalized for confident wrong answers!      â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Gradient Descent: Finding the Best Weights

![w:900 center](diagrams/svg/gradient_descent.svg)

---

# Gradient Descent Intuition

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FINDING THE VALLEY                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Imagine you're blindfolded on a mountain. Goal: reach the lowest  â”‚
â”‚   point (minimum loss).                                              â”‚
â”‚                                                                      â”‚
â”‚   Strategy:                                                          â”‚
â”‚   1. Feel the ground slope with your feet (compute gradient)        â”‚
â”‚   2. Take a step downhill (update weights)                           â”‚
â”‚   3. Repeat until you can't go lower                                 â”‚
â”‚                                                                      â”‚
â”‚                     â•±â•²                                               â”‚
â”‚                    â•±  â•²     Start here                               â”‚
â”‚               â•±â•²  â•±    â•²   â—                                         â”‚
â”‚              â•±  â•²â•±      â•² â†™                                          â”‚
â”‚             â•±            â•²                                           â”‚
â”‚            â•±      â—      â•²   â† Taking steps downhill                 â”‚
â”‚           â•±        â†˜      â•²                                          â”‚
â”‚          â•±          â—      â•²                                         â”‚
â”‚         â•±            â†˜      â•²                                        â”‚
â”‚   â”€â”€â”€â”€â”€â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â•²â”€â”€â”€â”€â”€  â† Minimum (best weights)       â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ”„ Level 5: The Context Problem
## Why Fixed Windows Aren't Enough

---

# The Fatal Flaw

Our neural network has a **fixed context window** (e.g., 3 characters).

```
"Alice picked up the golden key. She walked to the door
 and tried to open it with the ___"

What the human sees:  "golden key" (earlier in story)
What the model sees:  "with the"  (only last 3 words!)

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
Model's window:     â”‚ ... with the ___                â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–²
                          â”‚
    "key" is outside the window! The model forgot it!
```

---

# Why Is This a Problem?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTEXT EXAMPLES                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Example 1: Pronouns                                                â”‚
â”‚   "John gave Mary a book. She thanked ___"                          â”‚
â”‚   Answer: "him" (need to remember "John")                            â”‚
â”‚                                                                      â”‚
â”‚   Example 2: Callbacks                                               â”‚
â”‚   "In Chapter 1, we introduced X. Now, let's explore ___ further."  â”‚
â”‚   Answer: "X" (need to remember earlier topic)                       â”‚
â”‚                                                                      â”‚
â”‚   Example 3: Long dependencies                                       â”‚
â”‚   "The cat, which was sleeping on the mat that grandmother           â”‚
â”‚    made last winter, suddenly ___"                                   â”‚
â”‚   Answer: verb about "cat" (need to skip 15 words!)                  â”‚
â”‚                                                                      â”‚
â”‚   Example 4: Instructions                                            â”‚
â”‚   "I want you to translate to French: Hello"                        â”‚
â”‚   Need to remember "translate to French" while processing "Hello"    â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Attempted Solution: RNNs

**Recurrent Neural Networks** maintain a "memory" that carries forward.

![w:950 center](diagrams/svg/rnn_sequence.svg)

---

# How RNNs Work

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RNN: PASSING THE BATON                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Processing: "I love pizza"                                         â”‚
â”‚                                                                      â”‚
â”‚   Step 1: "I"                                                        â”‚
â”‚           hâ‚ = f("I", hâ‚€)                                            â”‚
â”‚           hâ‚ encodes: "Someone is speaking"                          â”‚
â”‚                                                                      â”‚
â”‚   Step 2: "love"                                                     â”‚
â”‚           hâ‚‚ = f("love", hâ‚)                                         â”‚
â”‚           hâ‚‚ encodes: "Someone loves something"                      â”‚
â”‚                                                                      â”‚
â”‚   Step 3: "pizza"                                                    â”‚
â”‚           hâ‚ƒ = f("pizza", hâ‚‚)                                        â”‚
â”‚           hâ‚ƒ encodes: "Someone loves pizza"                          â”‚
â”‚                                                                      â”‚
â”‚   The hidden state h carries information forward!                    â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Why RNNs Still Fail

**The Telephone Game Problem:**

```
Start: "Alice has a key"
  â”‚
  â–¼ (pass through 50 words)
  â”‚
  â–¼ (memory gets compressed, some info lost)
  â”‚
  â–¼ (pass through 50 more words)
  â”‚
  â–¼ (even more degraded)
  â”‚
End: "She opened the door with the ___"

By now, "key" has been corrupted or forgotten!

This is the "Vanishing Gradient" problem:
Gradients become tiny â†’ Old info can't influence predictions
```

---

# The Vanishing Gradient Problem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SIGNAL DEGRADATION                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Original message: "THE CAT HAS A KEY"                              â”‚
â”‚                                                                      â”‚
â”‚   After 10 steps:   "The cat has a key"        (still clear)         â”‚
â”‚   After 50 steps:   "Something about a cat"    (getting fuzzy)       â”‚
â”‚   After 100 steps:  "Animal? Object?"          (very unclear)        â”‚
â”‚   After 200 steps:  "...???..."                (information lost)    â”‚
â”‚                                                                      â”‚
â”‚   Like whispering a message through 100 people:                      â”‚
â”‚   "The cat has a key" â†’ "The hat has a tree" â†’ "???"                â”‚
â”‚                                                                      â”‚
â”‚   Mathematical cause:                                                â”‚
â”‚   gradient = gradient Ã— weight Ã— weight Ã— ... Ã— weight               â”‚
â”‚   If weight < 1: gradient â†’ 0 (vanishes)                            â”‚
â”‚   If weight > 1: gradient â†’ âˆ (explodes)                            â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# LSTM: A Better RNN

**Long Short-Term Memory** cells have "gates" that control information flow.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LSTM: THE MEMORY CELL                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Three gates control information:                                   â”‚
â”‚                                                                      â”‚
â”‚   ğŸšª FORGET GATE: "Should I forget old stuff?"                       â”‚
â”‚      Example: When starting a new sentence, forget the old one       â”‚
â”‚                                                                      â”‚
â”‚   ğŸšª INPUT GATE: "Should I remember this new thing?"                 â”‚
â”‚      Example: "key" is important, remember it!                       â”‚
â”‚                                                                      â”‚
â”‚   ğŸšª OUTPUT GATE: "What should I output now?"                        â”‚
â”‚      Example: Output information relevant to current prediction      â”‚
â”‚                                                                      â”‚
â”‚   Result: Information can "skip" through time without degradation!   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

But LSTMs are still slow (sequential) and still struggle with very long contexts.

---

# ğŸš€ Level 6: The Revolution
## Attention: "Just Look Back!"

---

# The Brilliant Idea

What if, instead of compressing everything into a hidden state...
We could just **look back** at everything directly?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                 â”‚
â”‚  "Alice picked up the golden key. She walked to the door       â”‚
â”‚   and tried to open it with the ___"                           â”‚
â”‚                                                                 â”‚
â”‚   Fixed Window: Can only see "with the"                        â”‚
â”‚   RNN: Remembers a blurry summary                              â”‚
â”‚   ATTENTION: Can look at ANY word!                             â”‚
â”‚              â†‘                                                  â”‚
â”‚              â””â”€â”€ "Let me check... 'key' was mentioned!"        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Attention: The Searchlight Analogy

```
Reading Methods Compared:

FIXED WINDOW (MLP):
    Reading with tunnel vision
    â”Œâ”€â”€â”€â”
    â”‚â–‘â–‘â–‘â”‚ â† Can only see this tiny part
    â””â”€â”€â”€â”˜

RNN:
    Reading while trying to remember everything
    "I think there was a key... or was it a lock?"

ATTENTION:
    Reading with a highlighter and search engine!
    "Let me search for 'object that opens doors'..."
    Found: "key" at position 7!
    â””â”€â”€ Spotlight on relevant words! ğŸ”¦
```

---

# How Attention Works: The Library Analogy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE LIBRARY OF WORDS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   You're at the library, looking for information.                   â”‚
â”‚                                                                      â”‚
â”‚   YOUR QUESTION (Query): "What opens doors?"                         â”‚
â”‚                                                                      â”‚
â”‚   BOOK LABELS (Keys):    BOOK CONTENTS (Values):                     â”‚
â”‚   â”œâ”€â”€ "key"              â”œâ”€â”€ Info about keys                         â”‚
â”‚   â”œâ”€â”€ "door"             â”œâ”€â”€ Info about doors                        â”‚
â”‚   â”œâ”€â”€ "Alice"            â”œâ”€â”€ Info about Alice                        â”‚
â”‚   â””â”€â”€ "walked"           â””â”€â”€ Info about walking                      â”‚
â”‚                                                                      â”‚
â”‚   You compare your Query to each Key:                                â”‚
â”‚   - Query âˆ™ "key" = HIGH match! (0.8)                               â”‚
â”‚   - Query âˆ™ "door" = Medium match (0.5)                              â”‚
â”‚   - Query âˆ™ "Alice" = Low match (0.1)                                â”‚
â”‚   - Query âˆ™ "walked" = Low match (0.1)                               â”‚
â”‚                                                                      â”‚
â”‚   You read mostly from "key" book, a little from "door" book.       â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# How Attention Works: Q, K, V

Think of it like a **database lookup**:

![w:800 center](diagrams/svg/attention_qkv.svg)

---

# The Math of Attention

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ATTENTION COMPUTATION                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Query (Q):  What am I looking for? (from current position)        â”‚
â”‚   Key (K):    What does each position offer? (from all positions)   â”‚
â”‚   Value (V):  What is the content? (from all positions)             â”‚
â”‚                                                                      â”‚
â”‚   Step 1: Compute similarity scores                                  â”‚
â”‚           scores = Q Â· Káµ€                                            â”‚
â”‚                                                                      â”‚
â”‚   Step 2: Normalize with softmax                                     â”‚
â”‚           attention_weights = softmax(scores / âˆšd)                   â”‚
â”‚                                                                      â”‚
â”‚   Step 3: Weighted sum of values                                     â”‚
â”‚           output = attention_weights Â· V                             â”‚
â”‚                                                                      â”‚
â”‚   Formula:  Attention(Q, K, V) = softmax(QKáµ€ / âˆšd) Â· V              â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Attention Scores Visualized

```
Predicting: "The animal didn't cross the street because it was too ___"
                                                              â”‚
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
            "it" refers to what?

Attention Scores (what "it" is looking at):

    The     animal  didn't  cross   the    street  because  it
    0.02    0.75    0.01    0.02    0.01   0.15    0.02     ---
            ^^^^                           ^^^^
            High attention!                Some attention

The model figures out "it" = "animal" (not "street")!
This is learned automatically from data.
```

---

# Why "Wide" Beats "Narrow"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    COMPARISON                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Predicting the word after "The quick brown fox jumps over         â”‚
â”‚   the lazy dog and then runs to the ___"                            â”‚
â”‚                                                                      â”‚
â”‚   Bigram: sees only "the" â†’ could predict anything!                 â”‚
â”‚                                                                      â”‚
â”‚   3-char window: sees "the " â†’ still very ambiguous                 â”‚
â”‚                                                                      â”‚
â”‚   RNN: has a blurry memory of earlier words                         â”‚
â”‚        "something about a fox... and running?"                       â”‚
â”‚                                                                      â”‚
â”‚   Attention: can directly look at "fox", "runs", "dog"              â”‚
â”‚        Weights: fox(0.3), runs(0.2), lazy(0.1), ...                 â”‚
â”‚        "The fox is running, so it might go to a... den? forest?"   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Self-Attention: Every Word Looks at Every Word

```
Sentence: "The cat sat on the mat"

             The   cat   sat   on   the   mat
           â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
    The    â”‚ 0.5 â”‚ 0.2 â”‚ 0.1 â”‚0.05â”‚ 0.1 â”‚0.05 â”‚
    cat    â”‚ 0.1 â”‚ 0.5 â”‚ 0.2 â”‚0.05â”‚ 0.1 â”‚0.05 â”‚
    sat    â”‚ 0.1 â”‚ 0.3 â”‚ 0.3 â”‚ 0.1â”‚ 0.1 â”‚ 0.1 â”‚
    on     â”‚ 0.1 â”‚ 0.1 â”‚ 0.2 â”‚ 0.3â”‚ 0.2 â”‚ 0.1 â”‚
    the    â”‚ 0.2 â”‚ 0.1 â”‚ 0.1 â”‚ 0.1â”‚ 0.3 â”‚ 0.2 â”‚
    mat    â”‚ 0.1 â”‚ 0.2 â”‚ 0.1 â”‚0.05â”‚ 0.2 â”‚ 0.35â”‚
           â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜

Each row: Where does this word look for context?
Each row sums to 1.0 (softmax)
Computed in parallel (not sequential like RNN)!
```

---

# Multi-Head Attention

**Idea:** Different "heads" can look for different things.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MULTI-HEAD ATTENTION                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Head 1: "What is the subject?"                                     â”‚
â”‚           "The cat sat on the mat"                                   â”‚
â”‚                 ^^^                                                  â”‚
â”‚                                                                      â”‚
â”‚   Head 2: "What is the action?"                                      â”‚
â”‚           "The cat sat on the mat"                                   â”‚
â”‚                     ^^^                                              â”‚
â”‚                                                                      â”‚
â”‚   Head 3: "What is the location?"                                    â”‚
â”‚           "The cat sat on the mat"                                   â”‚
â”‚                              ^^^                                     â”‚
â”‚                                                                      â”‚
â”‚   Head 4: "What came before?"                                        â”‚
â”‚           Looks at previous sentence...                              â”‚
â”‚                                                                      â”‚
â”‚   Combine all heads â†’ Rich understanding!                            â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# The Transformer Architecture

![w:700 center](diagrams/svg/transformer_block.svg)

---

# The Transformer Block

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ONE TRANSFORMER BLOCK                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Input: Sequence of token embeddings                                â”‚
â”‚          [embâ‚, embâ‚‚, embâ‚ƒ, ...]                                    â”‚
â”‚                     â”‚                                                â”‚
â”‚                     â–¼                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚     Multi-Head Self-Attention       â”‚  "Who should I listen to?"â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                     â”‚                                                â”‚
â”‚                     â–¼                                                â”‚
â”‚              Add & Normalize              (Residual connection)      â”‚
â”‚                     â”‚                                                â”‚
â”‚                     â–¼                                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚     Feed-Forward Network (MLP)      â”‚  "Process information"    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                     â”‚                                                â”‚
â”‚                     â–¼                                                â”‚
â”‚              Add & Normalize              (Residual connection)      â”‚
â”‚                     â”‚                                                â”‚
â”‚                     â–¼                                                â”‚
â”‚   Output: Transformed embeddings                                     â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Stacking Transformer Blocks

```
                                    â”‚ Token Embeddings
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Transformer Block 1      â”‚  â† Early processing
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Transformer Block 2      â”‚  â† Build on Block 1
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Transformer Block 3      â”‚  â† More complex patterns
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                                   ...
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Transformer Block 96     â”‚  â† Very abstract
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â–¼
                             Output logits

GPT-4 has ~120 layers! Each layer refines understanding.
```

---

# ğŸŒ Level 7: From Theory to ChatGPT
## Scaling Up

---

# Our Toy Model vs ChatGPT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Feature          â”‚   Our Toy Model  â”‚    ChatGPT       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Vocabulary             â”‚   27 (letters)   â”‚ 100,000 (tokens) â”‚
â”‚ Embedding Size         â”‚   2 dimensions   â”‚ 12,288 dims      â”‚
â”‚ Layers                 â”‚   1 layer        â”‚ 96 layers        â”‚
â”‚ Attention Heads        â”‚   1 head         â”‚ 96 heads         â”‚
â”‚ Parameters             â”‚   ~1,000         â”‚ 175 BILLION      â”‚
â”‚ Training Data          â”‚   1,000 names    â”‚ 500B+ tokens     â”‚
â”‚ Context Window         â”‚   3 chars        â”‚ 128K tokens      â”‚
â”‚ Training Time          â”‚   1 minute       â”‚ Months on 1000s  â”‚
â”‚                        â”‚                  â”‚ of GPUs          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Same core algorithm. Just **much, much bigger**.

---

# Tokenization: Not Characters, Not Words

```
LLMs use "TOKENS" â€” subword units (BPE algorithm):

"unhappiness" â†’ ["un", "happiness"]  or  ["un", "hap", "piness"]

Why?
- Characters: Too slow (many steps to generate a word)
- Words: Too many unique words (millions!)
- Tokens: Best of both worlds (~50,000-100,000 tokens)

Example:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Text: "ChatGPT is amazing!"                                      â”‚
â”‚                                                                 â”‚
â”‚ Tokens: ["Chat", "G", "PT", " is", " amazing", "!"]             â”‚
â”‚         [15496, 38,  2898,  318,    4998,      0]  â† Token IDs  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Note: Spaces are often part of tokens (" is" not "is")
```

---

# How BPE Tokenization Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BYTE PAIR ENCODING (BPE)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Start with character vocabulary: {a, b, c, ..., z, space, ...}    â”‚
â”‚                                                                      â”‚
â”‚   Algorithm:                                                         â”‚
â”‚   1. Count all adjacent pairs in corpus                              â”‚
â”‚   2. Most common pair: "th" (appears 10,000 times)                  â”‚
â”‚   3. Merge: create new token "th"                                    â”‚
â”‚   4. Repeat until vocabulary reaches target size                     â”‚
â”‚                                                                      â”‚
â”‚   After many merges:                                                 â”‚
â”‚   "the" â†’ single token                                               â”‚
â”‚   "ing" â†’ single token                                               â”‚
â”‚   "tion" â†’ single token                                              â”‚
â”‚   "unhappiness" â†’ ["un", "happiness"]                               â”‚
â”‚                                                                      â”‚
â”‚   Common words = 1 token, rare words = multiple tokens               â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Tokenization Quirks

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TOKENIZATION FUN FACTS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Why LLMs struggle with:                                            â”‚
â”‚                                                                      â”‚
â”‚   1. Counting letters:                                               â”‚
â”‚      "strawberry" â†’ ["str", "aw", "berry"]  â† 'r' split across!     â”‚
â”‚      Model doesn't "see" individual letters easily                   â”‚
â”‚                                                                      â”‚
â”‚   2. Non-English languages:                                          â”‚
â”‚      "Hello" â†’ 1 token                                               â”‚
â”‚      "à¤¨à¤®à¤¸à¥à¤¤à¥‡" (Hindi) â†’ 6 tokens  â† Same meaning, 6x tokens!          â”‚
â”‚                                                                      â”‚
â”‚   3. Numbers:                                                        â”‚
â”‚      "1234" might be 1 token                                         â”‚
â”‚      "12345" might be 2 tokens ["1234", "5"]                        â”‚
â”‚      Math becomes harder!                                            â”‚
â”‚                                                                      â”‚
â”‚   4. Code:                                                           â”‚
â”‚      "    " (4 spaces) might be 1 token                             â”‚
â”‚      "   " (3 spaces) might be 3 tokens                              â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Positional Encoding

How does the model know word ORDER?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    POSITIONAL ENCODING                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Problem: Attention is permutation-invariant!                       â”‚
â”‚   "Dog bites man" and "Man bites dog" look the same to attention.   â”‚
â”‚                                                                      â”‚
â”‚   Solution: Add position information to each embedding.              â”‚
â”‚                                                                      â”‚
â”‚   token_embedding("cat")       = [0.5, 0.3, 0.8, ...]               â”‚
â”‚   position_encoding(pos=3)     = [0.1, -0.2, 0.4, ...]              â”‚
â”‚   final_embedding              = [0.6, 0.1, 1.2, ...]               â”‚
â”‚                                                                      â”‚
â”‚   Now "cat" at position 3 is different from "cat" at position 10!   â”‚
â”‚                                                                      â”‚
â”‚   Original paper uses sinusoidal patterns:                           â”‚
â”‚   PE(pos, 2i) = sin(pos / 10000^(2i/d))                             â”‚
â”‚   PE(pos, 2i+1) = cos(pos / 10000^(2i/d))                           â”‚
â”‚                                                                      â”‚
â”‚   Modern models: Learn position embeddings!                          â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Temperature: The Creativity Knob

When sampling the next token, we can adjust **temperature**:

```
Probabilities for next word after "I love to eat":

                Low Temp (0.1)      High Temp (2.0)
                (Conservative)      (Creative)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   pizza     â”‚     0.80        â”‚     0.25        â”‚
â”‚   pasta     â”‚     0.15        â”‚     0.20        â”‚
â”‚   shoes     â”‚     0.01        â”‚     0.15        â”‚
â”‚   clouds    â”‚     0.001       â”‚     0.12        â”‚
â”‚   dreams    â”‚     0.0001      â”‚     0.10        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Low temp â†’ Always picks "pizza" (boring but safe)
High temp â†’ Might pick "clouds" (creative but weird)
```

---

# Temperature: The Math

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HOW TEMPERATURE WORKS                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   softmax(logits / temperature)                                      â”‚
â”‚                                                                      â”‚
â”‚   Example logits: [2.0, 1.0, 0.5, 0.1]                              â”‚
â”‚                                                                      â”‚
â”‚   Temperature = 1.0 (normal):                                        â”‚
â”‚   probs = [0.43, 0.26, 0.19, 0.12]                                  â”‚
â”‚                                                                      â”‚
â”‚   Temperature = 0.1 (cold/greedy):                                   â”‚
â”‚   probs = [0.99, 0.01, 0.00, 0.00]  â† Almost deterministic          â”‚
â”‚                                                                      â”‚
â”‚   Temperature = 2.0 (hot/random):                                    â”‚
â”‚   probs = [0.32, 0.27, 0.22, 0.19]  â† More uniform                  â”‚
â”‚                                                                      â”‚
â”‚   Temperature = 0 (greedy):                                          â”‚
â”‚   Always pick highest probability (argmax)                           â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Top-k and Top-p Sampling

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OTHER SAMPLING METHODS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   TOP-K SAMPLING:                                                    â”‚
â”‚   Only consider the top K most likely tokens.                        â”‚
â”‚                                                                      â”‚
â”‚   All tokens: pizza(0.4), pasta(0.3), shoes(0.1), clouds(0.05)...   â”‚
â”‚   Top-3:      pizza(0.53), pasta(0.40), shoes(0.07)                 â”‚
â”‚                                                                      â”‚
â”‚   Problem: K is fixed. Sometimes 3 options make sense, sometimes 10.â”‚
â”‚                                                                      â”‚
â”‚   TOP-P (NUCLEUS) SAMPLING:                                          â”‚
â”‚   Include tokens until cumulative probability > P.                   â”‚
â”‚                                                                      â”‚
â”‚   P = 0.9:                                                           â”‚
â”‚   Include pizza(0.4) + pasta(0.3) + shoes(0.1) = 0.8 < 0.9          â”‚
â”‚   Include clouds(0.05) â†’ 0.85 < 0.9                                 â”‚
â”‚   Include dreams(0.04) â†’ 0.89 < 0.9                                 â”‚
â”‚   Include hope(0.02) â†’ 0.91 > 0.9 â†’ Stop!                           â”‚
â”‚                                                                      â”‚
â”‚   Adaptive: Narrow when confident, wide when uncertain!              â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# The Sampling Tree

Because we sample probabilistically, each generation is different!

![w:800 center](diagrams/svg/sampling_tree.svg)

---

# Training at Scale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GPT-3 TRAINING                                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Data:                                                              â”‚
â”‚   â€¢ Common Crawl (web pages): 410B tokens                           â”‚
â”‚   â€¢ Books: 67B tokens                                                â”‚
â”‚   â€¢ Wikipedia: 3B tokens                                             â”‚
â”‚   â€¢ Total: ~500B tokens                                              â”‚
â”‚                                                                      â”‚
â”‚   Compute:                                                           â”‚
â”‚   â€¢ 10,000 GPUs                                                      â”‚
â”‚   â€¢ Training time: ~1 month                                          â”‚
â”‚   â€¢ Cost: ~$4.6 million (just electricity!)                         â”‚
â”‚                                                                      â”‚
â”‚   Model:                                                             â”‚
â”‚   â€¢ 175 billion parameters                                          â”‚
â”‚   â€¢ 96 layers                                                        â”‚
â”‚   â€¢ 96 attention heads                                               â”‚
â”‚   â€¢ 12,288 embedding dimensions                                      â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# The Complete Recipe

![w:650 center](diagrams/svg/llm_recipe.svg)

---

# From GPT to ChatGPT

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PRE-TRAINING â†’ FINE-TUNING                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   Step 1: PRE-TRAINING (GPT)                                        â”‚
â”‚   â€¢ Train on internet text (next token prediction)                  â”‚
â”‚   â€¢ Model learns language patterns, facts, reasoning                 â”‚
â”‚   â€¢ But it's just an autocomplete engine!                           â”‚
â”‚                                                                      â”‚
â”‚   Step 2: SUPERVISED FINE-TUNING (SFT)                              â”‚
â”‚   â€¢ Train on (instruction, response) pairs                          â”‚
â”‚   â€¢ Humans write example responses                                   â”‚
â”‚   â€¢ Model learns to follow instructions                              â”‚
â”‚                                                                      â”‚
â”‚   Step 3: RLHF (Reinforcement Learning from Human Feedback)         â”‚
â”‚   â€¢ Humans rank model responses                                      â”‚
â”‚   â€¢ Train a reward model on these rankings                           â”‚
â”‚   â€¢ Use RL to optimize for human preferences                         â”‚
â”‚   â€¢ Makes responses helpful, harmless, honest                        â”‚
â”‚                                                                      â”‚
â”‚   Result: ChatGPT = GPT + SFT + RLHF                                â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Summary: The Full Stack

```
Layer 0: THE TASK
         Predict P(next_token | all_previous_tokens)

Layer 1: REPRESENTATION
         Tokens â†’ Embeddings (meaning as vectors)

Layer 2: CONTEXT
         Self-Attention (look at relevant past tokens)

Layer 3: COMPUTATION
         Feed-Forward layers (process information)

Layer 4: STACKING
         Repeat attention+FFN 96 times for depth

Layer 5: TRAINING
         Next token prediction on internet-scale data

Layer 6: ALIGNMENT
         Instruction tuning + RLHF for helpfulness
```

---

# Key Takeaways

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE 5 BIG IDEAS                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚   1. PREDICTION IS ALL YOU NEED                                      â”‚
â”‚      Just predicting the next token gives emergent abilities         â”‚
â”‚                                                                      â”‚
â”‚   2. EMBEDDINGS CAPTURE MEANING                                      â”‚
â”‚      Similar words â†’ Similar vectors                                 â”‚
â”‚                                                                      â”‚
â”‚   3. ATTENTION ENABLES LONG-RANGE CONTEXT                            â”‚
â”‚      Every token can look at every other token                       â”‚
â”‚                                                                      â”‚
â”‚   4. SCALE MATTERS                                                   â”‚
â”‚      Bigger models + more data = better capabilities                 â”‚
â”‚                                                                      â”‚
â”‚   5. ALIGNMENT IS CRUCIAL                                            â”‚
â”‚      Raw prediction â†’ helpful assistant through RLHF                 â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Resources to Learn More

<div class="columns">
<div>

**Videos:**
1. **Andrej Karpathy** - "Neural Networks: Zero to Hero"
   - Builds GPT from scratch
2. **3Blue1Brown** - "Attention in Transformers"
   - Beautiful animations

</div>
<div>

**Code & Blogs:**
1. **NanoGPT** - Karpathy's GitHub
   - Full GPT in ~300 lines
2. **Jay Alammar** - "The Illustrated Transformer"
   - Best visualizations
3. **HuggingFace Course**
   - Practical transformer tutorials

</div>
</div>

---

# What's Next?

<div class="columns">
<div>

**In the Labs:**
- Lab 4: Build bigram & neural LM
- Lab 5: Deploy with Gradio
- Generate names, explore temperature

</div>
<div>

**Beyond:**
- Fine-tune a real LLM
- Build RAG applications
- Explore multimodal models

</div>
</div>

<div class="insight">
The same simple idea â€” predicting the next token â€” powers everything
from autocomplete to ChatGPT to Claude. Now you understand how!
</div>

---

# Thank You!

**"The best way to predict the future is to create it."**

The same simple idea â€” predicting the next token â€” powers everything
from autocomplete to ChatGPT to Claude.

## Questions?

---
